{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad \n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from autograd.core import primitive\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 10\n",
    "dx = 1. / nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, psy, d_psy):\n",
    "    '''\n",
    "        d(psy)/dx = f(x, psy)\n",
    "        This is f() function on the right\n",
    "    '''\n",
    "    return B(x) - psy * A(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(W, x):\n",
    "    a1 = sigmoid(np.dot(x, W[0]))\n",
    "    return np.dot(a1, W[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_neural_network_dx(W, x, k=1):\n",
    "    return np.dot(np.dot(W[1].T, W[0].T**k), sigmoid_grad(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(W, x, A, a, B, b):\n",
    "    loss_sum = 0.\n",
    "    for xi in x:\n",
    "        net_out = neural_network(W, xi)[0][0]\n",
    "        psy_t = (b*A - a*B)/(b - a) + ((B - A)/(b - a) * xi) + (xi - a)*(xi  -zb)*net_out\n",
    "        d_net_out = d_neural_network_dx(W, xi)[0][0]\n",
    "        d_psy_t = (B - A)/(b - a) + (xi - b)*net_out + (xi - a)*net_out + (x - a)*(x - b)*d_neural_network_dx\n",
    "        d2_psy_t = 2*net_out + (x - a)*d_neural_network_dx + (x - b)*d_neural_network_dx + (x - a)*(x - b)*d_neural_network_dx**2\n",
    "        func = f(xi, psy_t, d_psy_t)       \n",
    "        err_sqr = (d2_psy_t - func)**2\n",
    "\n",
    "        loss_sum += err_sqr\n",
    "    return loss_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_space = np.linspace(0, 1, nx)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loss_function() missing 4 required positional arguments: 'A', 'a', 'B', and 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-98cee937d790>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mloss_grad\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#print (loss_grad[0].shape, W[0].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\autograd\\wrap_util.py\u001b[0m in \u001b[0;36mnary_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0munary_operator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munary_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mnary_op_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnary_op_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnary_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnary_operator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\autograd\\differential_operators.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(fun, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0marguments\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0minstead\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     should be scalar-valued. The gradient has the same type as the argument.\"\"\"\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mvjp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_vjp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         raise TypeError(\"Grad only applies to real scalar-output functions. \"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\autograd\\core.py\u001b[0m in \u001b[0;36mmake_vjp\u001b[1;34m(fun, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_vjp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mstart_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVJPNode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_root\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mend_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_node\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mend_node\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mvspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\autograd\\tracer.py\u001b[0m in \u001b[0;36mtrace\u001b[1;34m(start_node, fun, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace_stack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mstart_box\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_node\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mend_box\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_box\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_box\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mend_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trace\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstart_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mend_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\autograd\\wrap_util.py\u001b[0m in \u001b[0;36munary_f\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0msubargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubvals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msubargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margnum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: loss_function() missing 4 required positional arguments: 'A', 'a', 'B', and 'b'"
     ]
    }
   ],
   "source": [
    "W = [npr.randn(1, 10), npr.randn(10, 1)]\n",
    "lmb = 0.001\n",
    "\n",
    "#x = np.array(1)\n",
    "#print (neural_network(W, x))\n",
    "#print (d_neural_network_dx(W, x))\n",
    "\n",
    "for i in range(1000):\n",
    "    loss_grad =  grad(loss_function)(W, x_space)\n",
    "    \n",
    "    #print (loss_grad[0].shape, W[0].shape)\n",
    "    #print (loss_grad[1].shape, W[1].shape)\n",
    "    #Descenso de gradiente \n",
    "    W[0] = W[0] - lmb * loss_grad[0]\n",
    "    W[1] = W[1] - lmb * loss_grad[1]\n",
    "    \n",
    "    #print (loss_function(W, x_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loss_function() missing 4 required positional arguments: 'A', 'a', 'B', and 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-5e71df7245a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: loss_function() missing 4 required positional arguments: 'A', 'a', 'B', and 'b'"
     ]
    }
   ],
   "source": [
    "print(loss_function(W, x_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.83736762, -0.05034919,  0.70179358,  1.00925586,  0.93224945,\n",
      "        -1.25040107, -0.29125768, -0.89388984, -0.12162807, -0.62153698]]), array([[-0.0321308 ],\n",
      "       [ 1.83948826],\n",
      "       [-1.0496137 ],\n",
      "       [-1.25707127],\n",
      "       [-1.61826518],\n",
      "       [-1.53729098],\n",
      "       [ 0.08192983],\n",
      "       [ 1.1355332 ],\n",
      "       [-0.23336909],\n",
      "       [-0.39988042]])]\n"
     ]
    }
   ],
   "source": [
    "print (W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [1 + xi * neural_network(W, xi)[0][0] for xi in x_space] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad, elementwise_grad\n",
    "import autograd.numpy.random as npr\n",
    "from autograd.misc.optimizers import adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_params(scale, layer_sizes, rs=npr.RandomState(0)):\n",
    "    \"\"\"Build a list of (weights, biases) tuples, one for each layer.\"\"\"\n",
    "    return [(rs.randn(insize, outsize) * scale,   # weight matrix\n",
    "             rs.randn(outsize) * scale)           # bias vector\n",
    "            for insize, outsize in zip(layer_sizes[:-1], layer_sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    \"see https://arxiv.org/pdf/1710.05941.pdf\"\n",
    "    return x / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(params, inputs):\n",
    "    \"Neural network functions\"\n",
    "    for W, b in params:\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        inputs = swish(outputs)    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our initial guess of params:\n",
    "params = init_random_params(0.1, layer_sizes=[1, 8, 1])\n",
    "\n",
    "# Derivatives\n",
    "fp = elementwise_grad(f, 1)\n",
    "fpp = elementwise_grad(fp, 1)\n",
    "fppp = elementwise_grad(fpp, 1)\n",
    "\n",
    "eta = np.linspace(0, 6).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function we seek to minimize\n",
    "def objective(params, step):\n",
    "    # These should all be zero at the solution\n",
    "    # f''' + 0.5 f'' f = 0\n",
    "    zeq = fppp(params, eta) + 0.5 * f(params, eta) * fpp(params, eta) \n",
    "    bc0 = f(params, 0.0)  # equal to zero at solution\n",
    "    bc1 = fp(params, 0.0)  # equal to zero at solution\n",
    "    bc2 = fp(params, 6.0) - 1.0 # this is the one at \"infinity\"\n",
    "    return np.mean(zeq**2) + bc0**2 + bc1**2 + bc2**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(params, step, g):\n",
    "    if step % 1000 == 0:\n",
    "        print(\"Iteration {0:3d} objective {1}\".format(step,\n",
    "                                                      objective(params, step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = adam(grad(objective), params,\n",
    "              step_size=0.001, num_iters=10000, callback=callback) \n",
    "\n",
    "print('f(0) = {}'.format(f(params, 0.0)))\n",
    "print('fp(0) = {}'.format(fp(params, 0.0)))\n",
    "print('fp(6) = {}'.format(fp(params, 6.0)))\n",
    "print('fpp(0) = {}'.format(fpp(params, 0.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   0 objective [[1.11472535]]\n",
      "Iteration 1000 objective [[0.00049768]]\n",
      "Iteration 2000 objective [[0.0004579]]\n",
      "Iteration 3000 objective [[0.00041697]]\n",
      "Iteration 4000 objective [[0.00037408]]\n",
      "Iteration 5000 objective [[0.00033705]]\n",
      "Iteration 6000 objective [[0.00031016]]\n"
     ]
    }
   ],
   "source": [
    "plt.plot(eta, f(params, eta))\n",
    "plt.xlabel('$\\eta$')\n",
    "plt.ylabel('$f(\\eta)$')\n",
    "plt.xlim([0, 6])\n",
    "plt.ylim([0, 4.5])\n",
    "plt.savefig('nn-blasius.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
