{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+import tensorflow as tf\n",
    "#import tensorflow_probability as tfp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from plotting import newfig, savefig\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "import uuid\n",
    "\n",
    "@tf.RegisterGradient(\"HeavisideGrad\")\n",
    "def _heaviside_grad(unused_op: tf.Operation, grad: tf.Tensor):\n",
    "    return tf.maximum(0.0, 1.0-tf.abs(unused_op.inputs[0])) * grad\n",
    "\n",
    "def heaviside(x: tf.Tensor, g: tf.Graph = tf.get_default_graph()):\n",
    "    custom_grads = {\n",
    "        \"Identity\": \"HeavisideGrad\"\n",
    "    }\n",
    "    with g.gradient_override_map(custom_grads):\n",
    "        i = tf.identity(x, name=\"identity_\" + str(uuid.uuid1()))\n",
    "        ge = tf.greater_equal(x, 0, name=\"ge_\" + str(uuid.uuid1()))\n",
    "        # tf.stop_gradient is needed to exclude tf.to_float from derivative\n",
    "        step_func = i + tf.stop_gradient(tf.to_float(ge) - i)\n",
    "        return step_func\n",
    "\n",
    "class HiddenPathways:\n",
    "    # Initialize the class\n",
    "    def __init__(self, t, S, layers):\n",
    "        \n",
    "        self.D = S.shape[1]\n",
    "        self.npar_intake = 3\n",
    "        \n",
    "        self.t_min = t.min(0)\n",
    "        self.t_max = t.max(0)\n",
    "        \n",
    "        self.S_mean = S.mean(0)\n",
    "        self.S_std = S.std(0)\n",
    "        \n",
    "        # data on concentrations\n",
    "        self.t = t\n",
    "        self.S = S\n",
    "\n",
    "        # layers\n",
    "        self.layers = layers\n",
    "        \n",
    "        # initialize NN\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "#        self.k = tf.Variable(1.0/120.0, dtype=tf.float32, trainable=False)\n",
    "#        self.Rm = tf.Variable(209.0, dtype=tf.float32, trainable=False)\n",
    "        self.Vg = tf.Variable(10.0, dtype=tf.float32, trainable=False)\n",
    "#        self.C1 = tf.Variable(300.0, dtype=tf.float32, trainable=False)\n",
    "#        self.a1 = tf.Variable(6.6, dtype=tf.float32, trainable=False)\n",
    "#        self.Ub = tf.Variable(72.0, dtype=tf.float32, trainable=False)\n",
    "#        self.C2 = tf.Variable(144.0, dtype=tf.float32, trainable=False)\n",
    "#        self.U0 = tf.Variable(4.0, dtype=tf.float32, trainable=False)\n",
    "#        self.Um = tf.Variable(90.0, dtype=tf.float32, trainable=False)\n",
    "#        self.C3 = tf.Variable(100.0, dtype=tf.float32, trainable=False)\n",
    "#        self.C4 = tf.Variable(80.0, dtype=tf.float32, trainable=False)\n",
    "#        self.Vi = tf.Variable(11.0, dtype=tf.float32, trainable=False)\n",
    "#        self.E = tf.Variable(0.2, dtype=tf.float32, trainable=False)\n",
    "#        self.ti = tf.Variable(100.0, dtype=tf.float32, trainable=False)\n",
    "#        self.beta = tf.Variable(1.772, dtype=tf.float32, trainable=False)\n",
    "#        self.Rg = tf.Variable(180.0, dtype=tf.float32, trainable=False)\n",
    "#        self.alpha = tf.Variable(7.5, dtype=tf.float32, trainable=False)\n",
    "#        self.Vp = tf.Variable(3.0, dtype=tf.float32, trainable=False)\n",
    "#        self.C5 = tf.Variable(26.0, dtype=tf.float32, trainable=False)\n",
    "#        self.tp = tf.Variable(6.0, dtype=tf.float32, trainable=False)\n",
    "#        self.td = tf.Variable(12.0, dtype=tf.float32, trainable=False)\n",
    "\n",
    "        self.logk = tf.Variable(-8.0, dtype=tf.float32, trainable=True)\n",
    "        self.logRm = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "#        self.logVg = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logC1 = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.loga1 = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logUb = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logC2 = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logU0 = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logUm = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logC3 = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logC4 = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logVi = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logE = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logti = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logbeta = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logRg = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logalpha = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logVp = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logC5 = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logtp = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logtd = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logmq = tf.Variable(tf.random.uniform([self.npar_intake,], minval=0.0, maxval=7.0, dtype=tf.float32),\n",
    "                                 dtype=tf.float32, trainable=True)\n",
    "        self.mt = tf.Variable(tf.random.uniform([self.npar_intake,], minval=-1.0, maxval=1.0, dtype=tf.float32),\n",
    "                              dtype=tf.float32, trainable=True,\n",
    "                              constraint=lambda x: tf.clip_by_value(x, -tf.ones(x.shape), tf.ones(x.shape)))\n",
    "#        self.start = tf.constant([-8.] + \n",
    "#                                 list(np.random.uniform(low=0.0, high=1800.0, size=self.npar_intake)) + \n",
    "#                                 list(np.random.uniform(low=0.0, high=7.0, size=self.npar_intake)),\n",
    "#                                 dtype=tf.float32)\n",
    "        \n",
    "        self.var_list_eqns = [self.logk, self.logRm, self.logC1, self.loga1, self.logUb, \n",
    "                              self.logC2, self.logU0, self.logUm, self.logC3, self.logC4, \n",
    "                              self.logE, self.logti, self.logbeta, self.logRg, self.logalpha, \n",
    "                              self.logC5, self.logtp, self.logtd, self.logVi, self.logVp,\n",
    "                              self.logmq, self.mt]\n",
    "  \n",
    "        self.k = tf.exp(self.logk)\n",
    "        self.Rm = tf.exp(self.logRm)\n",
    "#        self.Vg = tf.exp(self.logVg)\n",
    "        self.C1 = tf.exp(self.logC1)\n",
    "        self.a1 = tf.exp(self.loga1)\n",
    "        self.Ub = tf.exp(self.logUb)\n",
    "        self.C2 = tf.exp(self.logC2)\n",
    "        self.U0 = tf.exp(self.logU0)\n",
    "        self.Um = tf.exp(self.logUm)\n",
    "        self.C3 = tf.exp(self.logC3)\n",
    "        self.C4 = tf.exp(self.logC4)\n",
    "        self.Vi = tf.exp(self.logVi)\n",
    "        self.E = tf.exp(self.logE)\n",
    "        self.ti = tf.exp(self.logti)\n",
    "        self.beta = tf.exp(self.logbeta)\n",
    "        self.Rg = tf.exp(self.logRg)\n",
    "        self.alpha = tf.exp(self.logalpha)\n",
    "        self.Vp = tf.exp(self.logVp)\n",
    "        self.C5 = tf.exp(self.logC5)\n",
    "        self.tp = tf.exp(self.logtp)\n",
    "        self.td = tf.exp(self.logtd)\n",
    "        self.mq = tf.exp(self.logmq)\n",
    "        \n",
    "        # tf placeholders and graph\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        \n",
    "        # placeholders for data\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "        self.t_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.S_tf = tf.placeholder(tf.float32, shape=[None, self.D])\n",
    "        self.start = tf.placeholder(tf.float32, shape=[None,])\n",
    "        \n",
    "        # placeholders for forward differentian\n",
    "        self.dummy_tf = tf.placeholder(tf.float32, shape=(None, self.D)) # dummy variable for fwd_gradients (D outputs)\n",
    " \n",
    "        # physics informed neural networks\n",
    "        (self.S_pred,\n",
    "         self.E_pred,\n",
    "         self.IG_pred, self.IG_exp) = self.net_HiddenPathways(self.t_tf)\n",
    "\n",
    "        # loss\n",
    "        self.loss_data = tf.reduce_mean(tf.square((self.S_tf[:,2:3] - self.S_pred[:,2:3])/self.S_std[2]))\n",
    "        self.loss_eqns = tf.reduce_mean(tf.square(self.E_pred/self.S_std[0:self.E_pred.shape[1]]))\n",
    "        self.loss_auxl = tf.reduce_mean(tf.square((self.S_tf[-1,:]-self.S_pred[-1,:])/self.S_std[:]))\n",
    "        self.loss = 0.99*self.loss_data + 0.01*self.loss_eqns + 0.01*self.loss_auxl\n",
    "        \n",
    "        # optimizers\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "        self.optimizer_para = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "\n",
    "        self.train_op = self.optimizer.minimize(self.loss, var_list = self.weights + self.biases)\n",
    "        self.trainpara_op = self.optimizer_para.minimize(self.loss, var_list = self.var_list_eqns)\n",
    "#        self.trainintake_op = tfp.optimizer.nelder_mead_minimize(self.loss_fcn, initial_vertex=self.start,\n",
    "#                                                                 max_iterations=5000)\n",
    "        self.weightnorm, _ = tf.clip_by_global_norm(self.weights, 1.0)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "    def loss_fcn(self, x):\n",
    "#        H = 2.0*(self.t - self.t_min)/(self.t_max - self.t_min) - 1.0\n",
    "        H = self.t\n",
    "        k = tf.exp(x[0])\n",
    "        mt = x[1:(self.npar_intake+1)] * tf.ones([H.shape[0], self.npar_intake], dtype=tf.float32)\n",
    "        mq = tf.exp(x[(self.npar_intake+1):]) * tf.ones([H.shape[0], self.npar_intake], dtype=tf.float32)\n",
    "        intake = k * mq * heaviside(H-mt) * tf.exp(k*(mt-H))#*(self.t_max-self.t_min)/2.0)\n",
    "        IG_exp = tf.reduce_sum(intake, axis=1, keepdims=True)\n",
    "        \n",
    "        loss_intake = tf.reduce_sum(tf.square((self.IG_pred - IG_exp)/tf.math.reduce_std(self.IG_pred)))\n",
    "        return loss_intake\n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2.0/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        H = X\n",
    "        for l in range(0, num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.matmul(H, W) + b\n",
    "            H = H*tf.sigmoid(H)\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.matmul(H, W) + b\n",
    "        return Y\n",
    "    \n",
    "    def fwd_gradients(self, U, x):\n",
    "        g = tf.gradients(U, x, grad_ys=self.dummy_tf)[0]\n",
    "        return tf.gradients(g, self.dummy_tf)[0]\n",
    "    \n",
    "    def net_HiddenPathways(self, t):\n",
    "        H = 2.0*(t - self.t_min)/(self.t_max - self.t_min) - 1.0\n",
    "        S_tilde = self.neural_net(H, self.weights, self.biases)\n",
    "        S = self.S[0,:] + self.S_std*(H + 1.0)*S_tilde\n",
    "\n",
    "        mq = self.mq * tf.ones([tf.shape(H)[0], self.mq.shape[0]], dtype=tf.float32)\n",
    "        mt = self.mt * tf.ones([tf.shape(H)[0], self.mt.shape[0]], dtype=tf.float32)\n",
    "        intake = self.k * mq * heaviside(H-mt) * tf.exp(self.k*(mt-H)*(self.t_max-self.t_min)/2.0)\n",
    "        IG_exp = tf.reduce_sum(intake, axis=1, keepdims=True)\n",
    "\n",
    "        IG = S[:,6:7]        \n",
    "        kappa = 1.0/self.Vi + 1.0/(self.E*self.ti)\n",
    "        f1 = self.Rm * tf.sigmoid(S[:,2:3]/(self.Vg*self.C1) - self.a1)\n",
    "        f2 = self.Ub * (1.0 - tf.exp(-S[:,2:3]/(self.Vg*self.C2)))\n",
    "        safe_log = tf.where(S[:,1:2] <= 0.0, tf.ones_like(S[:,1:2]), S[:,1:2])\n",
    "        f3 = (self.U0 + self.Um*tf.sigmoid(self.beta*tf.log(kappa*safe_log/self.C4))) / (self.Vg*self.C3)\n",
    "        f4 = self.Rg * tf.sigmoid(-self.alpha*(S[:,5:6]/(self.Vp*self.C5)-1.0))\n",
    "                               \n",
    "        F0 = f1 - self.E*(S[:,0:1]/self.Vp-S[:,1:2]/self.Vi) - S[:,0:1]/self.tp\n",
    "        F1 = self.E*(S[:,0:1]/self.Vp-S[:,1:2]/self.Vi) - S[:,1:2]/self.ti\n",
    "        F2 = f4 + IG - f2 - f3*S[:,2:3]\n",
    "        F3 = (S[:,0:1] - S[:,3:4]) / self.td\n",
    "        F4 = (S[:,3:4] - S[:,4:5]) / self.td\n",
    "        F5 = (S[:,4:5] - S[:,5:6]) / self.td\n",
    "        \n",
    "        F = tf.concat([F0, F1, F2, F3, F4, F5], 1)\n",
    "\n",
    "        S_t = self.fwd_gradients(S, t)\n",
    "        \n",
    "        E = S_t[:,:-1] - F\n",
    "#        E = S_t - F\n",
    "        return S, E, IG, IG_exp\n",
    "    \n",
    "    def train(self, num_epochs, batch_size, learning_rate):\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            N = self.t.shape[0]\n",
    "            perm = np.concatenate( (np.array([0]), np.random.permutation(np.arange(1,N)),\n",
    "                                    np.array([N])) )\n",
    "            \n",
    "            start_time = time.time()\n",
    "            for it in range(0, N, batch_size):\n",
    "                idx = perm[np.arange(it, it+batch_size)]\n",
    "                (t_batch,\n",
    "                 S_batch) = (self.t[idx,:],\n",
    "                             self.S[idx,:])\n",
    "    \n",
    "                tf_dict = {self.t_tf: t_batch, self.S_tf: S_batch,\n",
    "                           self.dummy_tf: np.ones((batch_size, self.D)),\n",
    "                           self.learning_rate: learning_rate}\n",
    "                \n",
    "#                self.weights = self.sess.run(self.weightnorm)\n",
    "                self.sess.run([self.train_op,\n",
    "                               self.trainpara_op], tf_dict)\n",
    "                \n",
    "                # Print\n",
    "                if it % batch_size == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    [loss_data_value,\n",
    "                     loss_eqns_value,\n",
    "                     loss_auxl_value,\n",
    "                     learning_rate_value] = self.sess.run([self.loss_data,\n",
    "                                                           self.loss_eqns,\n",
    "                                                           self.loss_auxl,\n",
    "                                                           self.learning_rate], tf_dict)\n",
    "                    print('Epoch: %d, It: %d, Loss Data: %.3e, Loss Eqns: %.3e, Loss Auxl: %.3e, Time: %.3f, Learning Rate: %.1e'\n",
    "                          %(epoch, it/batch_size, loss_data_value, loss_eqns_value, loss_auxl_value, elapsed, learning_rate_value))\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "    def train_aux(self, init):\n",
    "        start = np.array(init)\n",
    "        tf_dict = {self.t_tf: self.t, self.S_tf: self.S,\n",
    "                   self.dummy_tf: np.ones((self.t.shape[0], self.D)),\n",
    "                   self.start: start}\n",
    "        \n",
    "        results = self.sess.run([self.trainintake_op], tf_dict)\n",
    "        \n",
    "        params = results[0].position\n",
    "        if results[0].converged:\n",
    "            print('Optimization for intake parameters converged!')\n",
    "        else:\n",
    "            print('Optimization for intake parameters did not converge!')\n",
    "\n",
    "        params[0] = np.exp(params[0])\n",
    "#        params[1:(self.npar_intake+1)] = (params[1:(self.npar_intake+1)]+1.0) * \\\n",
    "#                                         (self.t_max-self.t_min)/2.0 + self.t_min\n",
    "        params[(self.npar_intake+1):] = np.exp(params[(self.npar_intake+1):])\n",
    "        return params\n",
    "\n",
    "    def predict(self, t_star):\n",
    "        \n",
    "        tf_dict = {self.t_tf: t_star,\n",
    "                   self.dummy_tf: np.ones((t_star.shape[0], self.D))}\n",
    "        \n",
    "        S_star, IG = self.sess.run([self.S_pred, self.IG_pred], tf_dict)\n",
    "        S_star = np.append(S_star[:,:], IG[:], axis=1)\n",
    "        return S_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    layers = [1] + 5*[7*30] + [7]\n",
    "#    layers = [1] + 5*[6*30] + [6]\n",
    "\n",
    "    meal_t = [300., 650., 1100., 2000.]\n",
    "    meal_q = [60e3, 40e3, 50e3, 100e3]\n",
    "    Vg2 = 10.0*10.0\n",
    "    \n",
    "    def intake(tn, k):\n",
    "        def s(mjtj):\n",
    "            return k*mjtj[1]*np.heaviside(tn-mjtj[0], 0.5)*np.exp(k*(mjtj[0]-tn))\n",
    "        IG = np.array([s(mjtj) for mjtj in list(zip(meal_t, meal_q))]).sum()\n",
    "        return IG\n",
    "    \n",
    "    # function that returns dx/dt\n",
    "    def f(x, t): # x is 6 x 1\n",
    "        k = 1./120.\n",
    "        Rm = 209.\n",
    "        Vg = 10.\n",
    "        C1 = 300.\n",
    "        a1 = 6.6\n",
    "        Ub = 72.\n",
    "        C2 = 144.\n",
    "        U0 = 4.\n",
    "        Um = 90.\n",
    "        C3 = 100.\n",
    "        C4 = 80.\n",
    "        Vi = 11.\n",
    "        E = 0.2\n",
    "        ti = 100.\n",
    "        beta = 1.772\n",
    "        Rg = 180.\n",
    "        alpha = 7.5\n",
    "        Vp = 3.\n",
    "        C5 = 26.\n",
    "        tp = 6.\n",
    "        td = 12.\n",
    "        \n",
    "        kappa = 1.0/Vi + 1.0/E/ti\n",
    "        f1 = Rm / (1.0 + np.exp(-x[2]/Vg/C1 + a1))\n",
    "        f2 = Ub * (1.0 - np.exp(-x[2]/Vg/C2))\n",
    "        f3 = (U0 + Um/(1.0+np.exp(-beta*np.log(kappa*x[1]/C4)))) / Vg / C3\n",
    "        f4 = Rg / (1.0 + np.exp(alpha*(x[5]/Vp/C5-1.0)))\n",
    "        \n",
    "        x0 = f1 - E*(x[0]/Vp-x[1]/Vi) - x[0]/tp\n",
    "        x1 = E*(x[0]/Vp-x[1]/Vi) - x[1]/ti\n",
    "        x2 = f4 + intake(t, k) - f2 - f3*x[2]\n",
    "        x3 = (x[0] - x[3]) / td\n",
    "        x4 = (x[3] - x[4]) / td\n",
    "        x5 = (x[4] - x[5]) / td\n",
    "        \n",
    "        X = np.array([x0, x1, x2, x3, x4, x5])\n",
    "        return X\n",
    "    \n",
    "    def plotting(t_star, S_star, S_pred, perm, forecast=False):\n",
    "        sns.set()\n",
    "    \n",
    "        fig, ax = newfig(2.0, 0.7)\n",
    "        gs0 = gridspec.GridSpec(1, 1)\n",
    "        gs0.update(top=0.9, bottom=0.15, left=0.1, right=0.95, hspace=0.3, wspace=0.3)\n",
    "        ax = plt.subplot(gs0[0:1, 0:1])\n",
    "        ax.plot(t_star,S_star[:,2],'C1',linewidth=2,label='input data')\n",
    "        ax.scatter(t_star[perm],S_star[perm,2],marker='o',s=40,label='sampled input')\n",
    "        ax.set_xlabel('$t\\ (min)$', fontsize=18)\n",
    "        ax.set_ylabel('$G\\ (mg/dl) $', fontsize=18)\n",
    "        ax.legend(fontsize='large')\n",
    "        \n",
    "        ####################################    \n",
    "        fig, ax = newfig(1.8, 0.75)\n",
    "        gs1 = gridspec.GridSpec(1, 2)\n",
    "        gs1.update(top=0.85, bottom=0.15, left=0.1, right=0.95, hspace=0.3, wspace=0.3)\n",
    "        ax = plt.subplot(gs1[0:1, 0:1])\n",
    "        ax.plot(t_star,S_star[:,0]*Vg2,'C1',linewidth=2,label='exact')\n",
    "        ax.plot(t_star,S_pred[:,0]*Vg2,'g-.',linewidth=3,label='learned')\n",
    "        ax.set_xlabel('$t\\ (min)$', fontsize=18)\n",
    "        ax.set_ylabel('$I_p\\ (\\mu U/ml)$', fontsize=18)\n",
    "        ax.legend(fontsize='large')\n",
    "        \n",
    "        ax = plt.subplot(gs1[0:1, 1:2])\n",
    "        ax.plot(t_star,S_star[:,1]*Vg2,'C1',linewidth=2)\n",
    "        ax.plot(t_star,S_pred[:,1]*Vg2,'g-.',linewidth=3)\n",
    "        ax.set_xlabel('$t\\ (min)$', fontsize=18)\n",
    "        ax.set_ylabel('$I_i\\ (\\mu U/ml)$', fontsize=18)\n",
    "    \n",
    "        fig, ax = newfig(1.8, 0.75)\n",
    "        gs2 = gridspec.GridSpec(1, 2)\n",
    "        gs2.update(top=0.85, bottom=0.15, left=0.1, right=0.95, hspace=0.3, wspace=0.3)\n",
    "        ax = plt.subplot(gs2[0:1, 0:1])\n",
    "        if not forecast:\n",
    "            ax.scatter(t_star[perm],S_star[perm,2],marker='o',c='C1',s=30)\n",
    "        else:\n",
    "            ax.plot(t_star,S_star[:,2],'C1',linewidth=2)\n",
    "        ax.plot(t_star,S_pred[:,2],'g-.',linewidth=3)\n",
    "        ax.set_xlabel('$t\\ (min)$', fontsize=18)\n",
    "        ax.set_ylabel('$G\\ (mg/dl)$', fontsize=18)\n",
    "        \n",
    "        ax = plt.subplot(gs2[0:1, 1:2])\n",
    "        ax.plot(t_star,S_star[:,6]*Vg2,'C1',linewidth=2)\n",
    "        ax.plot(t_star,S_pred[:,6]*Vg2,'g-.',linewidth=3)\n",
    "        ax.set_xlabel('$t\\ (min)$', fontsize=18)\n",
    "        ax.set_ylabel('$I_G\\ (mg/min)$', fontsize=18)\n",
    "\n",
    "    # time points\n",
    "    t_star = np.arange(0, 3000, 1.0)\n",
    "\n",
    "    k = 1./120.\n",
    "    Vp = 3.0\n",
    "    Vi = 11.0\n",
    "    S0 = 12.0*Vp\n",
    "    S1 = 4.0*Vi\n",
    "    S2 = 110.0*Vg2\n",
    "    S3 = 0.0\n",
    "    S4 = 0.0\n",
    "    S5 = 0.0\n",
    "    \n",
    "    # initial condition\n",
    "    x0 = np.array([S0, S1, S2, S3, S4, S5]).flatten()\n",
    "\n",
    "    # solve ODE\n",
    "    S_star = odeint(f, x0, t_star)\n",
    "    S_star /= Vg2\n",
    "    IG_star = np.array([intake(t, k) for t in t_star]) / Vg2\n",
    "\n",
    "    t_train = t_star[:,None]\n",
    "    S_train = np.append(S_star[:,:], IG_star[:,None], axis=1)\n",
    "#    S_train = S_star\n",
    "    N_train = t_train.shape[0]\n",
    "    N_perm = np.int32(N_train)\n",
    "    perm = np.concatenate( (np.array([0]), np.random.randint(1, high=N_train-1, size=N_perm-2),\n",
    "                            np.array([N_train-1])) )\n",
    "\n",
    "    model = HiddenPathways(t_train[perm], S_train[perm,:], layers)\n",
    "\n",
    "    model.train(num_epochs = 20000, batch_size = perm.shape[0], learning_rate = 1e-3)\n",
    "    model.train(num_epochs = 40000, batch_size = perm.shape[0], learning_rate = 1e-4)\n",
    "    model.train(num_epochs = 40000, batch_size = perm.shape[0], learning_rate = 1e-5)\n",
    "    model.train(num_epochs = 20000, batch_size = perm.shape[0], learning_rate = 1e-6)\n",
    "\n",
    "    # NN prediction\n",
    "    S_pred = model.predict(t_star[:,None])\n",
    "    plotting(t_star, S_train, S_pred, perm)\n",
    "\n",
    "\n",
    "    #################################################\n",
    "    # Prediction based on inferred parameters\n",
    "    init = list(np.random.uniform(low=-7.0, high=0.0, size=1)) + \\\n",
    "           list(np.random.uniform(low=0.0, high=1800.0, size=3)) + \\\n",
    "           list(np.random.uniform(low=0.0, high=7., size=3))\n",
    "    params = model.train_aux(init)\n",
    "    \n",
    "    nparams = len(params)\n",
    "    k = params[0]\n",
    "    meal_t = list(params[1:(nparams//2+1)]) + [2000.]\n",
    "    meal_q = list(params[(nparams//2+1):]*Vg2) + [100e3]\n",
    "    Rm = model.sess.run(model.Rm)*Vg2\n",
    "    Vg = model.sess.run(model.Vg)\n",
    "    C1 = model.sess.run(model.C1)*Vg2\n",
    "    a1 = model.sess.run(model.a1)\n",
    "    Ub = model.sess.run(model.Ub)*Vg2\n",
    "    C2 = model.sess.run(model.C2)*Vg2\n",
    "    U0 = model.sess.run(model.U0)*Vg2\n",
    "    Um = model.sess.run(model.Um)*Vg2\n",
    "    C3 = model.sess.run(model.C3)*Vg2\n",
    "    C4 = model.sess.run(model.C4)*Vg2\n",
    "    Vi = model.sess.run(model.Vi)\n",
    "    E = model.sess.run(model.E)\n",
    "    ti = model.sess.run(model.ti)\n",
    "    beta = model.sess.run(model.beta)\n",
    "    Rg = model.sess.run(model.Rg)*Vg2\n",
    "    alpha = model.sess.run(model.alpha)\n",
    "    Vp = model.sess.run(model.Vp)\n",
    "    C5 = model.sess.run(model.C5)*Vg2\n",
    "    tp = model.sess.run(model.tp)\n",
    "    td = model.sess.run(model.td)\n",
    "    \n",
    "    # function that returns dx/dt\n",
    "    def f_pred(x, t): # x is 6 x 1\n",
    "        kappa = 1.0/Vi + 1.0/E/ti\n",
    "        f1 = Rm / (1.0 + np.exp(-x[2]/Vg/C1 + a1))\n",
    "        f2 = Ub * (1.0 - np.exp(-x[2]/Vg/C2))\n",
    "        f3 = (U0 + Um/(1.0+np.exp(-beta*np.log(kappa*x[1]/C4)))) / Vg / C3\n",
    "        f4 = Rg / (1.0 + np.exp(alpha*(x[5]/Vp/C5-1.0)))\n",
    "        \n",
    "        x0 = f1 - E*(x[0]/Vp-x[1]/Vi) - x[0]/tp\n",
    "        x1 = E*(x[0]/Vp-x[1]/Vi) - x[1]/ti\n",
    "        x2 = f4 + intake(t, k) - f2 - f3*x[2]\n",
    "        x3 = (x[0] - x[3]) / td\n",
    "        x4 = (x[3] - x[4]) / td\n",
    "        x5 = (x[4] - x[5]) / td\n",
    "        \n",
    "        X = np.array([x0, x1, x2, x3, x4, x5])\n",
    "        return X\n",
    "\n",
    "    S0 = 12.0*Vp\n",
    "    S1 = 4.0*Vi\n",
    "    S2 = 110.0*Vg2\n",
    "    S3 = 0.0\n",
    "    S4 = 0.0\n",
    "    S5 = 0.0\n",
    "    x0 = np.array([S0, S1, S2, S3, S4, S5]).flatten()\n",
    "    S_pred = odeint(f_pred, x0, t_star)\n",
    "    S_pred /= Vg2\n",
    "    IG_pred = np.array([intake(t, k) for t in t_star]) / Vg2\n",
    "    S_pred = np.append(S_pred[:,:], IG_pred[:,None], axis=1)\n",
    "    plotting(t_star, S_train, S_pred, perm, forecast=True)\n",
    "\n",
    "    #################################################\n",
    "\n",
    "\n",
    "#    print('k = %.6f' % ( model.sess.run(model.k) ) )\n",
    "    print('Rm = %.6f' % ( model.sess.run(model.Rm)*Vg2 ) )\n",
    "    print('Vg = %.6f' % ( model.sess.run(model.Vg) ) )\n",
    "    print('C1 = %.6f' % ( model.sess.run(model.C1)*Vg2 ) )\n",
    "    print('a1 = %.6f' % ( model.sess.run(model.a1) ) )\n",
    "    print('Ub = %.6f' % ( model.sess.run(model.Ub)*Vg2 ) )\n",
    "    print('C2 = %.6f' % ( model.sess.run(model.C2)*Vg2 ) )\n",
    "    print('U0 = %.6f' % ( model.sess.run(model.U0)*Vg2 ) )\n",
    "    print('Um = %.6f' % ( model.sess.run(model.Um)*Vg2 ) )\n",
    "    print('C3 = %.6f' % ( model.sess.run(model.C3)*Vg2 ) )\n",
    "    print('C4 = %.6f' % ( model.sess.run(model.C4)*Vg2 ) )\n",
    "    print('Vi = %.6f' % ( model.sess.run(model.Vi) ) )\n",
    "    print('E = %.6f' % ( model.sess.run(model.E) ) )\n",
    "    print('ti = %.6f' % ( model.sess.run(model.ti) ) )\n",
    "    print('beta = %.6f' % ( model.sess.run(model.beta) ) )\n",
    "    print('Rg = %.6f' % ( model.sess.run(model.Rg)*Vg2 ) )\n",
    "    print('alpha = %.6f' % ( model.sess.run(model.alpha) ) )\n",
    "    print('Vp = %.6f' % ( model.sess.run(model.Vp) ) )\n",
    "    print('C5 = %.6f' % ( model.sess.run(model.C5)*Vg2 ) )\n",
    "    print('tp = %.6f' % ( model.sess.run(model.tp) ) )\n",
    "    print('td = %.6f' % ( model.sess.run(model.td) ) )\n",
    "#    print('mq = %.6f' % ( model.sess.run(model.mq)*Vg2 ) )\n",
    "#    print('mt = %.6f' % ( model.sess.run(model.mt) ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
