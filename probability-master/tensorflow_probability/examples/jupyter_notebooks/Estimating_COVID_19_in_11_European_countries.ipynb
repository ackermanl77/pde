{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9QUsWYVvWq0-"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors.\n",
    "##### Copyright 2020 Imperial College London.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEwCccuiYViy"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the MIT License { display-mode: \"form\" }\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-f7FVRYZFeA"
   },
   "source": [
    "# Modeling COVID-19 spread in Europe and the effect of interventions\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/probability/examples/Estimating_COVID_19_in_11_European_countries\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Estimating_COVID_19_in_11_European_countries.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Estimating_COVID_19_in_11_European_countries.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/probability/tensorflow_probability/examples/jupyter_notebooks/Estimating_COVID_19_in_11_European_countries.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZuINNbybkM5Q"
   },
   "source": [
    "To slow the spread of COVID-19 in early 2020, European countries adopted non-pharmaceutical interventions such as closure of non-essential businesses, isolation of individual cases, travel bans, and other measures to encourage social distancing. The Imperial College COVID-19 Response Team analyzed the effectiveness of these measures in their paper [\"Estimating the number of infections and the impact of non-pharmaceutical interventions on COVID-19 in 11 European countries\"](https://doi.org/10.25561/77731), using a Bayesian hierarchical model combined with a mechanistic epidemiological model.\n",
    "\n",
    "This Colab contains a TensorFlow Probability (TFP) implementation of that analysis, organized as follows:\n",
    "- \"Model setup\" defines the epidemiological model for disease transmission and resulting deaths, the Bayesian prior distribution over model parameters, and the distribution of number of deaths conditional on parameter values.\n",
    "- \"Data preprocessing\" loads in data on the timing and type of interventions in each country, counts of deaths over time, and estimated fatality rates for those infected.\n",
    "- \"Model inference\" builds a Bayesian hierarchical model and runs Hamiltonian Monte Carlo (HMC) to sample from the posterior distribution over parameters.\n",
    "- \"Results\" shows posterior predictive distributions for quantities of interest such as forecasted deaths, and counterfactual deaths in the absence of interventions.\n",
    "\n",
    "The paper found evidence that countries had managed to reduce the number of new infections transmitted by each infected person ($R_t$), but that credible intervals contained $R_t=1$ (the value above which the epidemic continues to spread) and that it was premature to draw strong conclusions on the effectiveness of interventions. The Stan code for the paper is in the authors' [Github](https://github.com/ImperialCollegeLondon/covid19model) repository, and this Colab reproduces [Version 2](https://github.com/ImperialCollegeLondon/covid19model/commit/25ef22e68fbbbd2fcbdf8ea2bde8e26a8a440bf6#diff-445347d4d98a197511cb1c4e17e55c33).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-GJ_sFxaFRv"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q git+git://github.com/arviz-devs/arviz.git\n",
    "!pip3 install -q tf-nightly tfp-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "ejlBkPjkuP43"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability.python.internal import prefer_static as ps\n",
    "\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "# Globally Enable XLA.\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "\n",
    "try:\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass\n",
    "\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "\n",
    "DTYPE = np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ID_szqDSfKks"
   },
   "source": [
    "## 1  Model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bE7GpQye5064"
   },
   "source": [
    "### 1.1  Mechanistic model for infections and deaths\n",
    "The infection model simulates the number of infections in each country over time. Input data are the timing and type of interventions, population size, and initial cases. Parameters control the effectiveness of interventions and the rate of disease transmission. The model for the expected number of deaths applies a fatality rate to the predicted infections.\n",
    "\n",
    "The infection model performs a convolution of previous daily infections with the serial interval distributution (the distribution over the number of days between becoming infected and infecting someone else). At each time step, the number of new infections at time $t$, $n_t$, is calculated as\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=0}^{t-1} n_i \\mu_t \text{p} (\\text{caught from someone infected at } i | \\text{newly infected at } t)\n",
    "\\end{equation}\n",
    "where $\\mu_t=R_t$ and the conditional probability is stored in `conv_serial_interval`, defined below.\n",
    "\n",
    "The model for expected deaths performs a convolution of daily infections and the distribution of days between infection and death. That is, expected deaths on day $t$ is calculated as\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=0}^{t-1} n_i\\text{p(death on day $t$|infection on day $i$)}\n",
    "\\end{equation}\n",
    "where the conditional probability is stored in `conv_fatality_rate`, defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hkXt4nsbFlb"
   },
   "outputs": [],
   "source": [
    "from tensorflow_probability.python.mcmc.internal import util as mcmc_util\n",
    "\n",
    "def predict_infections(\n",
    "    intervention_indicators, population, initial_cases, mu, alpha_hier,\n",
    "    conv_serial_interval, initial_days, total_days):\n",
    "  \"\"\"Predict the number of infections by forward-simulation.\n",
    "\n",
    "  Args:\n",
    "    intervention_indicators: Binary array of shape\n",
    "      `[num_countries, total_days, num_interventions]`, in which `1` indicates\n",
    "      the intervention is active in that country at that time and `0` indicates\n",
    "      otherwise.\n",
    "    population: Vector of length `num_countries`. Population of each country.\n",
    "    initial_cases: Array of shape `[batch_size, num_countries]`. Number of cases\n",
    "      in each country at the start of the simulation.\n",
    "    mu: Array of shape `[batch_size, num_countries]`. Initial reproduction rate\n",
    "      (R_0) by country.\n",
    "    alpha_hier: Array of shape `[batch_size, num_interventions]` representing\n",
    "      the effectiveness of interventions.\n",
    "    conv_serial_interval: Array of shape\n",
    "      `[total_days - initial_days, total_days]` output from\n",
    "      `make_conv_serial_interval`. Convolution kernel for serial interval\n",
    "      distribution.\n",
    "    initial_days: Integer, number of sequential days to seed infections after\n",
    "      the 10th death in a country. (N0 in the authors' Stan code.)\n",
    "    total_days: Integer, number of days of observed data plus days to forecast.\n",
    "      (N2 in the authors' Stan code.)\n",
    "  Returns:\n",
    "    predicted_infections: Array of shape\n",
    "      `[total_days, batch_size, num_countries]`. (Batched) predicted number of\n",
    "      infections over time and by country.\n",
    "  \"\"\"\n",
    "  alpha = alpha_hier - tf.cast(np.log(1.05) / 6.0, DTYPE)\n",
    "\n",
    "  # Multiply the effectiveness of each intervention in each country (alpha)\n",
    "  # by the indicator variable for whether the intervention was active and sum\n",
    "  # over interventions, yielding an array of shape\n",
    "  # [total_days, batch_size, num_countries] that represents the total effectiveness of\n",
    "  # all interventions in each country on each day (for a batch of data).\n",
    "  linear_prediction = tf.einsum(\n",
    "      'ijk,...k->j...i', intervention_indicators, alpha)\n",
    "\n",
    "  # Adjust the reproduction rate per country downward, according to the\n",
    "  # effectiveness of the interventions.\n",
    "  rt = mu * tf.exp(-linear_prediction, name='reproduction_rate')\n",
    "\n",
    "  # Initialize storage array for daily infections and seed it with initial\n",
    "  # cases.\n",
    "  daily_infections = tf.TensorArray(\n",
    "      dtype=DTYPE, size=total_days, element_shape=initial_cases.shape)\n",
    "  for i in range(initial_days):\n",
    "    daily_infections = daily_infections.write(i, initial_cases)\n",
    "\n",
    "  # Initialize cumulative cases.\n",
    "  init_cumulative_infections = initial_cases * initial_days\n",
    "\n",
    "  # Simulate forward for total_days days.\n",
    "  cond = lambda i, *_: i < total_days\n",
    "  def body(i, prev_daily_infections, prev_cumulative_infections):\n",
    "    # The probability distribution over days j that someone infected on day i\n",
    "    # caught the virus from someone infected on day j.\n",
    "    p_infected_on_day = tf.gather(\n",
    "        conv_serial_interval, i - initial_days, axis=0)\n",
    "\n",
    "    # Multiply p_infected_on_day by the number previous infections each day and\n",
    "    # by mu, and sum to obtain new infections on day i. Mu is adjusted by\n",
    "    # the fraction of the population already infected, so that the population\n",
    "    # size is the upper limit on the number of infections.\n",
    "    prev_daily_infections_array = prev_daily_infections.stack()\n",
    "    to_sum = prev_daily_infections_array * mcmc_util.left_justified_expand_dims_like(\n",
    "        p_infected_on_day, prev_daily_infections_array)\n",
    "    convolution = tf.reduce_sum(to_sum, axis=0)\n",
    "    rt_adj = (\n",
    "        (population - prev_cumulative_infections) / population\n",
    "        ) * tf.gather(rt, i)\n",
    "    new_infections = rt_adj * convolution\n",
    "\n",
    "    # Update the prediction array and the cumulative number of infections.\n",
    "    daily_infections = prev_daily_infections.write(i, new_infections)\n",
    "    cumulative_infections = prev_cumulative_infections + new_infections\n",
    "    return i + 1, daily_infections, cumulative_infections\n",
    "\n",
    "  _, daily_infections_final, last_cumm_sum = tf.while_loop(\n",
    "      cond, body,\n",
    "      (initial_days, daily_infections, init_cumulative_infections),\n",
    "      maximum_iterations=(total_days - initial_days))\n",
    "  return daily_infections_final.stack()\n",
    "\n",
    "def predict_deaths(predicted_infections, ifr_noise, conv_fatality_rate):\n",
    "  \"\"\"Expected number of reported deaths by country, by day.\n",
    "\n",
    "  Args:\n",
    "    predicted_infections: Array of shape\n",
    "      `[total_days, batch_size, num_countries]` output from\n",
    "      `predict_infections`.\n",
    "    ifr_noise: Array of shape `[batch_size, num_countries]`. Noise in Infection\n",
    "      Fatality Rate (IFR).\n",
    "    conv_fatality_rate: Array of shape\n",
    "      `[total_days - 1, total_days, num_countries]`. Convolutional kernel for\n",
    "      calculating fatalities, output from `make_conv_fatality_rate`.\n",
    "  Returns:\n",
    "    predicted_deaths: Array of shape `[total_days, batch_size, num_countries]`.\n",
    "      (Batched) predicted number of deaths over time and by country.\n",
    "  \"\"\"\n",
    "  # Multiply the number of infections on day j by the probability of death\n",
    "  # on day i given infection on day j, and sum over j. This yields the expected\n",
    "  result_remainder = tf.einsum(\n",
    "      'i...j,kij->k...j', predicted_infections, conv_fatality_rate) * ifr_noise\n",
    "\n",
    "  # Concatenate the result with a vector of zeros so that the first day is\n",
    "  # included.\n",
    "  result_temp = 1e-15 * predicted_infections[:1]\n",
    "  return tf.concat([result_temp, result_remainder], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3S5l06DA5h9J"
   },
   "source": [
    "### 1.2  Prior over parameter values\n",
    "\n",
    "Here we define the joint prior distribution over the model parameters. Many of the parameter values are assumed to be independent, such that the prior can be expressed as:\n",
    "\n",
    "$\\text p(\\tau, y, \\psi, \\kappa, \\mu, \\alpha) = \\text p(\\tau)\\text p(y|\\tau)\\text p(\\psi)\\text p(\\kappa)\\text p(\\mu|\\kappa)\\text p(\\alpha)\\text p(\\epsilon)$\n",
    "\n",
    "in which:\n",
    "\n",
    "- $\\tau$ is the shared rate parameter of the Exponential distribution over the number of initial cases per country, $y = y_1, ... y_{num\\_countries}$.\n",
    "- $\\psi$ is a parameter in the Negative Binomial distribution for number of deaths.\n",
    "- $\\kappa$ is the shared scale parameter of the HalfNormal distribution over the initial reproduction number in each country, $\\mu = \\mu_1, ..., \\mu_{num\\_countries}$ (indicating the number of additional cases transmitted by each infected person).\n",
    "- $\\alpha = \\alpha_1, ..., \\alpha_6$ is the effectiveness of each of the six interventions.\n",
    "- $\\epsilon$ (called `ifr_noise` in the code, after the authors' Stan code) is noise in the Infection Fatality Rate (IFR).\n",
    "\n",
    "We express this model as a TFP JointDistribution, a type of TFP distribution that enables expression of probabilistic graphical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0-1ZiGvfd6J"
   },
   "outputs": [],
   "source": [
    "def make_jd_prior(num_countries, num_interventions):\n",
    "  return tfd.JointDistributionSequentialAutoBatched([\n",
    "      # Rate parameter for the distribution of initial cases (tau).\n",
    "      tfd.Exponential(rate=tf.cast(0.03, DTYPE)),\n",
    "\n",
    "      # Initial cases for each country.\n",
    "      lambda tau: tfd.Sample(\n",
    "          tfd.Exponential(rate=tf.cast(1, DTYPE) / tau),\n",
    "          sample_shape=num_countries),\n",
    "\n",
    "      # Parameter in Negative Binomial model for deaths (psi).\n",
    "      tfd.HalfNormal(scale=tf.cast(5, DTYPE)),\n",
    "\n",
    "      # Parameter in the distribution over the initial reproduction number, R_0\n",
    "      # (kappa).\n",
    "      tfd.HalfNormal(scale=tf.cast(0.5, DTYPE)),\n",
    "\n",
    "      # Initial reproduction number, R_0, for each country (mu).\n",
    "      lambda kappa: tfd.Sample(\n",
    "          tfd.TruncatedNormal(loc=3.28, scale=kappa, low=1e-5, high=1e5),\n",
    "          sample_shape=num_countries),\n",
    "\n",
    "      # Impact of interventions (alpha; shared for all countries).\n",
    "      tfd.Sample(\n",
    "          tfd.Gamma(tf.cast(0.1667, DTYPE), 1), sample_shape=num_interventions),\n",
    "\n",
    "      # Multiplicative noise in Infection Fatality Rate.\n",
    "      tfd.Sample(\n",
    "          tfd.TruncatedNormal(\n",
    "              loc=tf.cast(1., DTYPE), scale=0.1, low=1e-5, high=1e5),\n",
    "              sample_shape=num_countries)\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vVAH9kE76GBw"
   },
   "source": [
    "### 1.3  Likelihood of observed deaths conditional on parameter values\n",
    "\n",
    "The likelihood model expresses $p(\\text{deaths} | \\tau, y, \\psi, \\kappa, \\mu, \\alpha, \\epsilon)$. It applies the models for the number of infections and expected deaths conditional on parameters, and assumes actual deaths follow a Negative Binomial distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kGVJspH3A-B"
   },
   "outputs": [],
   "source": [
    "def make_likelihood_fn(\n",
    "    intervention_indicators, population, deaths,\n",
    "    infection_fatality_rate, initial_days, total_days):\n",
    "\n",
    "  # Create a mask for the initial days of simulated data, as they are not\n",
    "  # counted in the likelihood.\n",
    "  observed_deaths = tf.constant(deaths.T[np.newaxis, ...], dtype=DTYPE)\n",
    "  mask_temp = deaths != -1\n",
    "  mask_temp[:, :START_DAYS] = False\n",
    "  observed_deaths_mask = tf.constant(mask_temp.T[np.newaxis, ...])\n",
    "\n",
    "  conv_serial_interval = make_conv_serial_interval(initial_days, total_days)\n",
    "  conv_fatality_rate = make_conv_fatality_rate(\n",
    "      infection_fatality_rate, total_days)\n",
    "\n",
    "  def likelihood_fn(tau, initial_cases, psi, kappa, mu, alpha_hier, ifr_noise):\n",
    "    # Run models for infections and expected deaths\n",
    "    predicted_infections = predict_infections(\n",
    "        intervention_indicators, population, initial_cases, mu, alpha_hier,\n",
    "        conv_serial_interval, initial_days, total_days)\n",
    "    e_deaths_all_countries = predict_deaths(\n",
    "        predicted_infections, ifr_noise, conv_fatality_rate)\n",
    "\n",
    "    # Construct the Negative Binomial distribution for deaths by country.\n",
    "    mu_m = tf.transpose(e_deaths_all_countries, [1, 0, 2])\n",
    "    psi_m = psi[..., tf.newaxis, tf.newaxis]\n",
    "    probs = tf.clip_by_value(mu_m / (mu_m + psi_m), 1e-9, 1.)\n",
    "    likelihood_elementwise = tfd.NegativeBinomial(\n",
    "        total_count=psi_m, probs=probs).log_prob(observed_deaths)\n",
    "    return tf.reduce_sum(\n",
    "        tf.where(observed_deaths_mask,\n",
    "                likelihood_elementwise,\n",
    "                tf.zeros_like(likelihood_elementwise)),\n",
    "        axis=[-2, -1])\n",
    "\n",
    "  return likelihood_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XP_-WlR0e8bx"
   },
   "source": [
    "### 1.4  Probability of death given infection\n",
    "\n",
    "This section computes the distribution of deaths on the days following infection. It assumes the time from infection to death is the sum of two Gamma-variate quantities, representing the time from infection to disease onset and the time from onset to death. The time-to-death distribution is combined with Infection Fatality Rate data from [Verity et al. (2020)](https://www.medrxiv.org/content/10.1101/2020.03.09.20033357v1) to compute the probability of death on days following infection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XxwE2G-OlSX6"
   },
   "outputs": [],
   "source": [
    "def daily_fatality_probability(infection_fatality_rate, total_days):\n",
    "  \"\"\"Computes the probability of death `d` days after infection.\"\"\"\n",
    "\n",
    "  # Convert from alternative Gamma parametrization and construct distributions\n",
    "  # for number of days from infection to onset and onset to death.\n",
    "  concentration1 = tf.cast((1. / 0.86)**2, DTYPE)\n",
    "  rate1 = concentration1 / 5.1\n",
    "  concentration2 = tf.cast((1. / 0.45)**2, DTYPE)\n",
    "  rate2 = concentration2 / 18.8\n",
    "  infection_to_onset = tfd.Gamma(concentration=concentration1, rate=rate1)\n",
    "  onset_to_death = tfd.Gamma(concentration=concentration2, rate=rate2)\n",
    "\n",
    "  # Create empirical distribution for number of days from infection to death.\n",
    "  inf_to_death_dist = tfd.Empirical(\n",
    "      infection_to_onset.sample([5e6]) + onset_to_death.sample([5e6]))\n",
    "\n",
    "  # Subtract the CDF value at day i from the value at day i + 1 to compute the\n",
    "  # probability of death on day i given infection on day 0, and given that\n",
    "  # death (not recovery) is the outcome.\n",
    "  times = np.arange(total_days + 1., dtype=DTYPE) + 0.5\n",
    "  cdf = inf_to_death_dist.cdf(times).numpy()\n",
    "  f_before_ifr = cdf[1:] - cdf[:-1]\n",
    "  # Explicitly set the zeroth value to the empirical cdf at time 1.5, to include\n",
    "  # the mass between time 0 and time .5.\n",
    "  f_before_ifr[0] = cdf[1]\n",
    "\n",
    "  # Multiply the daily fatality rates conditional on infection and eventual\n",
    "  # death (f_before_ifr) by the infection fatality rates (probability of death\n",
    "  # given intection) to obtain the probability of death on day i conditional\n",
    "  # on infection on day 0.\n",
    "  return infection_fatality_rate[..., np.newaxis] * f_before_ifr\n",
    "\n",
    "def make_conv_fatality_rate(infection_fatality_rate, total_days):\n",
    "  \"\"\"Computes the probability of death on day `i` given infection on day `j`.\"\"\"\n",
    "  p_fatal_all_countries = daily_fatality_probability(\n",
    "      infection_fatality_rate, total_days)\n",
    "\n",
    "  # Use the probability of death d days after infection in each country\n",
    "  # to build an array of shape [total_days - 1, total_days, num_countries],\n",
    "  # where the element [i, j, c] is the probability of death on day i+1 given\n",
    "  # infection on day j in country c.\n",
    "  conv_fatality_rate = np.zeros(\n",
    "      [total_days - 1, total_days, p_fatal_all_countries.shape[0]])\n",
    "  for n in range(1, total_days):\n",
    "    conv_fatality_rate[n - 1, 0:n, :] = (\n",
    "        p_fatal_all_countries[:, n - 1::-1]).T\n",
    "  return tf.constant(conv_fatality_rate, dtype=DTYPE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vDbnDEdDQP9r"
   },
   "source": [
    "### 1.5  Serial Interval\n",
    "\n",
    "The serial interval is the time between successive cases in a chain of disease transmission, and is assumed to be Gamma distributed. We use the serial interval distribution to compute the probability that a person infected on day $i$ caught the virus from a person previously infected on day $j$ (the `conv_serial_interval` argument to `predict_infections`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tp8eY3ZTQZYK"
   },
   "outputs": [],
   "source": [
    "def make_conv_serial_interval(initial_days, total_days):\n",
    "  \"\"\"Construct the convolutional kernel for infection timing.\"\"\"\n",
    "\n",
    "  g = tfd.Gamma(tf.cast(1. / (0.62**2), DTYPE), 1./(6.5*0.62**2))\n",
    "  g_cdf = g.cdf(np.arange(total_days, dtype=DTYPE))\n",
    "\n",
    "  # Approximate the probability mass function for the number of days between\n",
    "  # successive infections.\n",
    "  serial_interval = g_cdf[1:] - g_cdf[:-1]\n",
    "\n",
    "  # `conv_serial_interval` is an array of shape\n",
    "  # [total_days - initial_days, total_days] in which entry [i, j] contains the\n",
    "  # probability that an individual infected on day i + initial_days caught the\n",
    "  # virus from someone infected on day j.\n",
    "  conv_serial_interval = np.zeros([total_days - initial_days, total_days])\n",
    "  for n in range(initial_days, total_days):\n",
    "    conv_serial_interval[n - initial_days, 0:n] = serial_interval[n - 1::-1]\n",
    "  return tf.constant(conv_serial_interval, dtype=DTYPE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hb9gU263VYR6"
   },
   "source": [
    "## 2  Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLBD4oJ_ELg0"
   },
   "outputs": [],
   "source": [
    "COUNTRIES = [\n",
    "    'Austria',\n",
    "    'Belgium',\n",
    "    'Denmark',\n",
    "    'France',\n",
    "    'Germany',\n",
    "    'Italy',\n",
    "    'Norway',\n",
    "    'Spain',\n",
    "    'Sweden',\n",
    "    'Switzerland',\n",
    "    'United_Kingdom'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "X94vDnTVHwMH"
   },
   "outputs": [],
   "source": [
    "#@title 2.1  Fetch and preprocess interventions data\n",
    "raw_interventions = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/ImperialCollegeLondon/covid19model/master/data/interventions.csv')\n",
    "\n",
    "raw_interventions['Date effective'] = pd.to_datetime(\n",
    "    raw_interventions['Date effective'], dayfirst=True)\n",
    "interventions = raw_interventions.pivot(index='Country', columns='Type', values='Date effective')\n",
    "\n",
    "# If any interventions happened after the lockdown, use the date of the lockdown.\n",
    "for col in interventions.columns:\n",
    "  idx = interventions[col] > interventions['Lockdown']\n",
    "  interventions.loc[idx, col] = interventions[idx]['Lockdown']\n",
    "\n",
    "num_countries = len(COUNTRIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "0MKgEhJLYG_X"
   },
   "outputs": [],
   "source": [
    "#@title 2.2  Fetch case/death data and join to interventions\n",
    "# Load the case data\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/ImperialCollegeLondon/covid19model/master/data/COVID-19-up-to-date.csv')\n",
    "# You can also use the dataset directly from european cdc (where the ICL model fetch their data from)\n",
    "# data = pd.read_csv('https://opendata.ecdc.europa.eu/covid19/casedistribution/csv')\n",
    "\n",
    "data['country'] = data['countriesAndTerritories']\n",
    "data = data[['dateRep', 'cases', 'deaths', 'country']]\n",
    "data = data[data['country'].isin(COUNTRIES)]\n",
    "data['dateRep'] = pd.to_datetime(data['dateRep'], format='%d/%m/%Y')\n",
    "\n",
    "# Add 0/1 features for whether or not each intevention was in place.\n",
    "data = data.join(interventions, on='country', how='outer')\n",
    "for col in interventions.columns:\n",
    "  data[col] = (data['dateRep'] >= data[col]).astype(int)\n",
    "\n",
    "# Add \"any_intevention\" 0/1 feature.\n",
    "any_intervention_list = ['Schools + Universities',\n",
    "                         'Self-isolating if ill',\n",
    "                         'Public events',\n",
    "                         'Lockdown',\n",
    "                         'Social distancing encouraged']\n",
    "data['any_intervention'] = (\n",
    "    data[any_intervention_list].apply(np.sum, 'columns') > 0).astype(int)\n",
    "\n",
    "# Index by country and date.\n",
    "data = data.sort_values(by=['country', 'dateRep'])\n",
    "data = data.set_index(['country', 'dateRep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "ZMMF91DBCkM_"
   },
   "outputs": [],
   "source": [
    "#@title 2.3  Fetch and process Infected Fatality Ratio and population data\n",
    "infected_fatality_ratio = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/ImperialCollegeLondon/covid19model/master/data/popt_ifr.csv')\n",
    "\n",
    "infected_fatality_ratio = infected_fatality_ratio.replace(to_replace='United Kingdom', value='United_Kingdom')\n",
    "infected_fatality_ratio['Country'] = infected_fatality_ratio.iloc[:, 1]\n",
    "infected_fatality_ratio = infected_fatality_ratio[infected_fatality_ratio['Country'].isin(COUNTRIES)]\n",
    "infected_fatality_ratio = infected_fatality_ratio[\n",
    "  ['Country', 'popt', 'ifr']].set_index('Country')\n",
    "infected_fatality_ratio = infected_fatality_ratio.sort_index()\n",
    "infection_fatality_rate = infected_fatality_ratio['ifr'].to_numpy()\n",
    "population_value = infected_fatality_ratio['popt'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cXttSm6cpkY4"
   },
   "source": [
    "### 2.4  Preprocess country-specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "So18BBjOaXgO"
   },
   "outputs": [],
   "source": [
    "# Model up to 75 days of data for each country, starting 30 days before the\n",
    "# tenth cumulative death.\n",
    "START_DAYS = 30\n",
    "MAX_DAYS = 102\n",
    "COVARIATE_COLUMNS = any_intervention_list + ['any_intervention']\n",
    "\n",
    "# Initialize an array for number of deaths.\n",
    "deaths = -np.ones((num_countries, MAX_DAYS), dtype=DTYPE)\n",
    "\n",
    "# Assuming every intervention is still inplace in the unobserved future\n",
    "num_interventions = len(COVARIATE_COLUMNS)\n",
    "intervention_indicators = np.ones((num_countries, MAX_DAYS, num_interventions))\n",
    "\n",
    "first_days = {}\n",
    "for i, c in enumerate(COUNTRIES):\n",
    "  c_data = data.loc[c]\n",
    "\n",
    "  # Include data only after 10th death in a country.\n",
    "  mask = c_data['deaths'].cumsum() >= 10\n",
    "\n",
    "  # Get the date that the epidemic starts in a country.\n",
    "  first_day = c_data.index[mask][0] - pd.to_timedelta(START_DAYS, 'days')\n",
    "  c_data = c_data.truncate(before=first_day)\n",
    "\n",
    "  # Truncate the data after 28 March 2020 for comparison with Flaxman et al.\n",
    "  c_data = c_data.truncate(after='2020-03-28')\n",
    "\n",
    "  c_data = c_data.iloc[:MAX_DAYS]\n",
    "  days_of_data = c_data.shape[0]\n",
    "  deaths[i, :days_of_data] = c_data['deaths']\n",
    "  intervention_indicators[i, :days_of_data] = c_data[\n",
    "    COVARIATE_COLUMNS].to_numpy()\n",
    "  first_days[c] = first_day\n",
    "\n",
    "# Number of sequential days to seed infections after the 10th death in a\n",
    "# country. (N0 in authors' Stan code.)\n",
    "INITIAL_DAYS = 6\n",
    "\n",
    "# Number of days of observed data plus days to forecast. (N2 in authors' Stan\n",
    "# code.)\n",
    "TOTAL_DAYS = deaths.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SSaXn30U7mSp"
   },
   "source": [
    "## 3  Model inference\n",
    "\n",
    "Flaxman et al. (2020) used [Stan](https://mc-stan.org/) to sample from the parameter posterior with Hamiltonian Monte Carlo (HMC) and the No-U-Turn Sampler (NUTS).\n",
    "\n",
    "Here, we apply HMC with dual-averaging step size adaptation.  We use a pilot run of HMC for preconditioniting and initialization.\n",
    "\n",
    "Inference runs in a few minutes on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Sv8jaiIHTmG"
   },
   "source": [
    "### 3.1  Build prior and likelihood for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NhRHwH-VDR3s"
   },
   "outputs": [],
   "source": [
    "jd_prior = make_jd_prior(num_countries, num_interventions)\n",
    "likelihood_fn = make_likelihood_fn(\n",
    "    intervention_indicators, population_value, deaths,\n",
    "    infection_fatality_rate, INITIAL_DAYS, TOTAL_DAYS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6uBvjDDns57"
   },
   "source": [
    "### 3.2  Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y5lmcEpPoQWL"
   },
   "outputs": [],
   "source": [
    "def get_bijectors_from_samples(samples, unconstraining_bijectors, batch_axes):\n",
    "  \"\"\"Fit bijectors to the samples of a distribution.\n",
    "\n",
    "  This fits a diagonal covariance multivariate Gaussian transformed by the\n",
    "  `unconstraining_bijectors` to the provided samples. The resultant\n",
    "  transformation can be used to precondition MCMC and other inference methods.\n",
    "  \"\"\"\n",
    "  state_std = [    \n",
    "      tf.math.reduce_std(bij.inverse(x), axis=batch_axes)\n",
    "      for x, bij in zip(samples, unconstraining_bijectors)\n",
    "  ]\n",
    "  state_mu = [\n",
    "      tf.math.reduce_mean(bij.inverse(x), axis=batch_axes)\n",
    "      for x, bij in zip(samples, unconstraining_bijectors)\n",
    "  ]\n",
    "  return [tfb.Chain([cb, tfb.Shift(sh), tfb.Scale(sc)])\n",
    "          for cb, sh, sc in zip(unconstraining_bijectors, state_mu, state_std)]\n",
    "\n",
    "def generate_init_state_and_bijectors_from_prior(nchain, unconstraining_bijectors):\n",
    "  \"\"\"Creates an initial MCMC state, and bijectors from the prior.\"\"\"\n",
    "  prior_samples = jd_prior.sample(4096)\n",
    "\n",
    "  bijectors = get_bijectors_from_samples(\n",
    "      prior_samples, unconstraining_bijectors, batch_axes=0)\n",
    "  \n",
    "  init_state = [\n",
    "    bij(tf.zeros([nchain] + list(s), DTYPE))\n",
    "    for s, bij in zip(jd_prior.event_shape, bijectors)\n",
    "  ]\n",
    "  \n",
    "  return init_state, bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XRaBt7z3pb4g"
   },
   "outputs": [],
   "source": [
    "@tf.function(autograph=False, experimental_compile=True)\n",
    "def sample_hmc(\n",
    "    init_state,\n",
    "    step_size,\n",
    "    target_log_prob_fn,\n",
    "    unconstraining_bijectors,\n",
    "    num_steps=500,\n",
    "    burnin=50,\n",
    "    num_leapfrog_steps=10):\n",
    "\n",
    "    def trace_fn(_, pkr):\n",
    "        return {\n",
    "            'target_log_prob': pkr.inner_results.inner_results.accepted_results.target_log_prob,\n",
    "            'diverging': ~(pkr.inner_results.inner_results.log_accept_ratio > -1000.),\n",
    "            'is_accepted': pkr.inner_results.inner_results.is_accepted,\n",
    "            'step_size': [tf.exp(s) for s in pkr.log_averaging_step],\n",
    "        }\n",
    "    \n",
    "    hmc = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn,\n",
    "        step_size=step_size,\n",
    "        num_leapfrog_steps=num_leapfrog_steps)\n",
    "\n",
    "    hmc = tfp.mcmc.TransformedTransitionKernel(\n",
    "        inner_kernel=hmc,\n",
    "        bijector=unconstraining_bijectors)\n",
    "  \n",
    "    hmc = tfp.mcmc.DualAveragingStepSizeAdaptation(\n",
    "        hmc,\n",
    "        num_adaptation_steps=int(burnin * 0.8),\n",
    "        target_accept_prob=0.8,\n",
    "        decay_rate=0.5)\n",
    "\n",
    "    # Sampling from the chain.\n",
    "    return tfp.mcmc.sample_chain(\n",
    "        num_results=burnin + num_steps,\n",
    "        current_state=init_state,\n",
    "        kernel=hmc,\n",
    "        trace_fn=trace_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HbPc3HdXhfC"
   },
   "source": [
    "### 3.3  Define event space bijectors\n",
    "\n",
    "HMC is most efficient when sampling from an isotropic multivariate Gaussian distribution ([Mangoubi & Smith (2017)](http://arxiv.org/abs/1708.07114)), so the first step is to precondition the target density to look as much like that as possible.\n",
    "\n",
    "First and foremost, we transform constrained (e.g., non-negative) variables to an unconstrained space, which HMC requires. Additionally, we employ the SinhArcsinh bijector to manipulate the heaviness of the tails of the transformed target density; we want these to fall off roughly as $e^{-x^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gwdFrLSDXj-D"
   },
   "outputs": [],
   "source": [
    "unconstraining_bijectors = [\n",
    "    tfb.Chain([tfb.Scale(tf.constant(1 / 0.03, DTYPE)), tfb.Softplus(),\n",
    "                tfb.SinhArcsinh(tailweight=tf.constant(1.85, DTYPE))]), # tau\n",
    "    tfb.Chain([tfb.Scale(tf.constant(1 / 0.03, DTYPE)), tfb.Softplus(),\n",
    "                tfb.SinhArcsinh(tailweight=tf.constant(1.85, DTYPE))]), # initial_cases\n",
    "    tfb.Softplus(), # psi\n",
    "    tfb.Softplus(), # kappa\n",
    "    tfb.Softplus(), # mu\n",
    "    tfb.Chain([tfb.Scale(tf.constant(0.4, DTYPE)), tfb.Softplus(),\n",
    "                tfb.SinhArcsinh(skewness=tf.constant(-0.2, DTYPE), tailweight=tf.constant(2., DTYPE))]), # alpha\n",
    "    tfb.Softplus(), # ifr_noise\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0Q6pztLqJOc"
   },
   "source": [
    "### 3.4  HMC pilot run\n",
    "\n",
    "We first run HMC preconditioned by the prior, initialized from 0's in the transformed space. We don't use the prior samples to initialize the chain as in practice those often result in stuck chains due to poor numerics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "height": 51
    },
    "colab_type": "code",
    "id": "d85pyn6uqOWQ",
    "outputId": "0bceaf37-3e15-40d3-b209-36b0e9b259b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x7f0f5c093158> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x7f0f5c093158> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:Note that RandomUniformInt inside pfor op may not give same output as inside a sequential loop.\n",
      "WARNING:tensorflow:Note that RandomUniformInt inside pfor op may not give same output as inside a sequential loop.\n",
      "WARNING:tensorflow:Note that RandomStandardNormal inside pfor op may not give same output as inside a sequential loop.\n",
      "WARNING:tensorflow:Note that RandomStandardNormal inside pfor op may not give same output as inside a sequential loop.\n",
      "WARNING:tensorflow:Note that RandomUniformInt inside pfor op may not give same output as inside a sequential loop.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessParameterizedTruncatedNormal\n",
      "WARNING:tensorflow:Note that RandomUniformInt inside pfor op may not give same output as inside a sequential loop.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0f181d9c80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0f181d9c80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0f1816f7b8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0f1816f7b8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function WhileV2.__call__.<locals>.while_fn at 0x7f0efc7b56a8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function WhileV2.__call__.<locals>.while_fn at 0x7f0efc7b56a8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0efc722048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0efc722048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0efc6cbbf8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0efc6cbbf8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function WhileV2.__call__.<locals>.while_fn at 0x7f0efc501048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function WhileV2.__call__.<locals>.while_fn at 0x7f0efc501048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0efc526950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0efc526950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0efc526bf8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0efc526bf8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function WhileV2.__call__.<locals>.while_fn at 0x7f0efc3d4268> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function WhileV2.__call__.<locals>.while_fn at 0x7f0efc3d4268> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0efc386c80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0efc386c80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0efc33a730> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f0efc33a730> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:Note that RandomUniformInt inside pfor op may not give same output as inside a sequential loop.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessParameterizedTruncatedNormal\n",
      "WARNING:tensorflow:From /home/david/.local/lib/python3.7/site-packages/tensorflow_probability/python/mcmc/kernel.py:104: calling HamiltonianMonteCarlo.__init__ (from tensorflow_probability.python.mcmc.hmc) with step_size_update_fn is deprecated and will be removed after 2019-05-22.\n",
      "Instructions for updating:\n",
      "The `step_size_update_fn` argument is deprecated. Use `tfp.mcmc.SimpleStepSizeAdaptation` instead.\n",
      "WARNING:tensorflow:From /home/david/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:507: calling HamiltonianMonteCarlo.__init__ (from tensorflow_probability.python.mcmc.hmc) with seed is deprecated and will be removed after 2020-09-20.\n",
      "Instructions for updating:\n",
      "The `seed` argument is deprecated (but will work until removed). Pass seed to `tfp.mcmc.sample_chain` instead.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x7f0efc770730> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x7f0efc770730> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x7f0ed85f8158> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x7f0ed85f8158> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x7f0e9403c1e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x7f0e9403c1e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x7f0e78566e18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x7f0e78566e18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "libdevice not found at ./libdevice.10.bc [Op:__inference_sample_hmc_43655]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m           \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m           \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    844\u001b[0m               *args, **kwds)\n\u001b[1;32m    845\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: libdevice not found at ./libdevice.10.bc [Op:__inference_sample_hmc_43655]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nchain = 32\n",
    "\n",
    "target_log_prob_fn = lambda *x: jd_prior.log_prob(*x) + likelihood_fn(*x)\n",
    "init_state, bijectors = generate_init_state_and_bijectors_from_prior(nchain, unconstraining_bijectors)\n",
    "\n",
    "# Each chain gets its own step size.\n",
    "step_size = [tf.fill([nchain] + [1] * (len(s.shape) - 1), tf.constant(0.01, DTYPE)) for s in init_state]\n",
    "\n",
    "burnin = 200\n",
    "num_steps = 100\n",
    "\n",
    "pilot_samples, pilot_sampler_stat = sample_hmc(\n",
    "    init_state,\n",
    "    step_size,\n",
    "    target_log_prob_fn,\n",
    "    bijectors,\n",
    "    num_steps=num_steps,\n",
    "    burnin=burnin,\n",
    "    num_leapfrog_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9op0XHQUp9A"
   },
   "source": [
    "### 3.5  Visualize pilot samples\n",
    "\n",
    "We're looking for stuck chains and eyeballing convergence. We can do formal diagnostics here, but that's not super necessary given that it's just a pilot run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02CO9XzXUp9B"
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "az.style.use('arviz-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSj2dSx0Up9G"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pilot_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9911fe023e8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m pilot_with_warmup = {k: np.swapaxes(v.numpy(), 1, 0)\n\u001b[0;32m----> 4\u001b[0;31m                      for k, v in zip(var_name, pilot_samples)}\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pilot_samples' is not defined"
     ]
    }
   ],
   "source": [
    "var_name = ['tau', 'initial_cases', 'psi', 'kappa', 'mu', 'alpha', 'ifr_noise']\n",
    "\n",
    "pilot_with_warmup = {k: np.swapaxes(v.numpy(), 1, 0)\n",
    "                     for k, v in zip(var_name, pilot_samples)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2iwpCn-Dw3HL"
   },
   "source": [
    "We observe divergences during warmup, primarily because dual averaging step size adaptation uses a very aggressive search for the optimal step size. Once the adaptation turns off, divergences disappear as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 601
    },
    "colab_type": "code",
    "id": "9bcDvqtUUp9K",
    "outputId": "24840e79-f40b-431b-e925-8b985cfe3d5a"
   },
   "outputs": [],
   "source": [
    "az_trace = az.from_dict(posterior=pilot_with_warmup,\n",
    "                        sample_stats={'diverging': np.swapaxes(pilot_sampler_stat['diverging'].numpy(), 0, 1)})\n",
    "az.plot_trace(az_trace, combined=True, compact=True, figsize=(12, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 309
    },
    "colab_type": "code",
    "id": "m13oI0eWwuIZ",
    "outputId": "5221721b-8d7a-4734-daba-71fde9b26430"
   },
   "outputs": [],
   "source": [
    "plt.plot(pilot_sampler_stat['step_size'][0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0AGhIfRgUcTK"
   },
   "source": [
    "### 3.6  Run HMC\n",
    "\n",
    "In principle we could use the pilot samples for final analysis (if we ran it for longer to get convergence), but it's a little more efficient to start another HMC run, this time preconditioned and initialized by pilot samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 51
    },
    "colab_type": "code",
    "id": "QUhTuoNnUdpI",
    "outputId": "545dfd11-19d5-4115-b303-3b4df92d9017"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "burnin = 50\n",
    "num_steps = 200\n",
    "\n",
    "bijectors = get_bijectors_from_samples([s[burnin:] for s in pilot_samples],\n",
    "                                       unconstraining_bijectors=unconstraining_bijectors,\n",
    "                                       batch_axes=(0, 1))\n",
    "\n",
    "samples, sampler_stat = sample_hmc(\n",
    "    [s[-1] for s in pilot_samples],\n",
    "    [s[-1] for s in pilot_sampler_stat['step_size']],\n",
    "    target_log_prob_fn,\n",
    "    bijectors,\n",
    "    num_steps=num_steps,\n",
    "    burnin=burnin,\n",
    "    num_leapfrog_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 309
    },
    "colab_type": "code",
    "id": "xMZ8lI356gK6",
    "outputId": "634bb856-298a-459e-a011-910409d726c0"
   },
   "outputs": [],
   "source": [
    "plt.plot(sampler_stat['step_size'][0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aMo9kWFrsn5F"
   },
   "source": [
    "### 3.7  Visualize samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjsw6jeVu0V1"
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "az.style.use('arviz-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iHrohSqdsvwD"
   },
   "outputs": [],
   "source": [
    "var_name = ['tau', 'initial_cases', 'psi', 'kappa', 'mu', 'alpha', 'ifr_noise']\n",
    "\n",
    "posterior = {k: np.swapaxes(v.numpy()[burnin:], 1, 0)\n",
    "             for k, v in zip(var_name, samples)}\n",
    "posterior_with_warmup = {k: np.swapaxes(v.numpy(), 1, 0)\n",
    "             for k, v in zip(var_name, samples)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DkJTPUn6dA2h"
   },
   "source": [
    "Compute the summary of the chains. We're looking for high ESS and r_hat close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "id": "d4ZKbnkWu1Al",
    "outputId": "77e57b59-40f7-4e4d-9e10-294f9af0bea7"
   },
   "outputs": [],
   "source": [
    "az.summary(posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 601
    },
    "colab_type": "code",
    "id": "_WLCcnpus3yv",
    "outputId": "e6162617-d8f4-496b-9091-fca6344ffc12"
   },
   "outputs": [],
   "source": [
    "az_trace = az.from_dict(posterior=posterior_with_warmup,\n",
    "                        sample_stats={'diverging': np.swapaxes(sampler_stat['diverging'].numpy(), 0, 1)})\n",
    "az.plot_trace(az_trace, combined=True, compact=True, figsize=(12, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Rvbs-e9TYCD"
   },
   "source": [
    "It is instructive to look at the auto-correlation functions across all the dimensions. We're looking for functions which go down quickly, but not so much that they go into the negative (which is indicative of HMC hitting a resonance, which is bad for ergodicity and can introduce bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "id": "bwnv1Ld4R7x0",
    "outputId": "fb0e7eeb-05ed-40e3-972c-af460f1f975c"
   },
   "outputs": [],
   "source": [
    "with az.rc_context(rc={'plot.max_subplots': None}):\n",
    "  az.plot_autocorr(posterior, combined=True, figsize=(12, 16), textsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8w2AgPVgkXS"
   },
   "source": [
    "## 4  Results\n",
    "\n",
    "The following plots analyze the posterior predictive distributions over $R_t$, number of deaths, and number of infections, similar to the analysis in Flaxman et al. (2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MS4MsGuxsw_9"
   },
   "outputs": [],
   "source": [
    "total_num_samples = np.prod(posterior['mu'].shape[:2])\n",
    "\n",
    "# Calculate R_t given parameter estimates.\n",
    "def rt_samples_batched(mu, intervention_indicators, alpha):\n",
    "  linear_prediction = tf.reduce_sum(\n",
    "      intervention_indicators * alpha[..., np.newaxis, np.newaxis, :], axis=-1)\n",
    "  rt_hat = mu[..., tf.newaxis] * tf.exp(-linear_prediction, name='rt')\n",
    "  return rt_hat\n",
    "\n",
    "alpha_hat = tf.convert_to_tensor(\n",
    "    posterior['alpha'].reshape(total_num_samples, posterior['alpha'].shape[-1]))\n",
    "mu_hat = tf.convert_to_tensor(\n",
    "    posterior['mu'].reshape(total_num_samples, num_countries))\n",
    "rt_hat = rt_samples_batched(mu_hat, intervention_indicators, alpha_hat)\n",
    "sampled_initial_cases = posterior['initial_cases'].reshape(\n",
    "    total_num_samples, num_countries)\n",
    "sampled_ifr_noise = posterior['ifr_noise'].reshape(\n",
    "    total_num_samples, num_countries)\n",
    "psi_hat = posterior['psi'].reshape([total_num_samples])\n",
    "\n",
    "conv_serial_interval = make_conv_serial_interval(INITIAL_DAYS, TOTAL_DAYS)\n",
    "conv_fatality_rate = make_conv_fatality_rate(infection_fatality_rate, TOTAL_DAYS)\n",
    "pred_hat = predict_infections(\n",
    "    intervention_indicators, population_value, sampled_initial_cases, mu_hat,\n",
    "    alpha_hat, conv_serial_interval, INITIAL_DAYS, TOTAL_DAYS)\n",
    "expected_deaths = predict_deaths(pred_hat, sampled_ifr_noise, conv_fatality_rate)\n",
    "\n",
    "psi_m = psi_hat[np.newaxis, ..., np.newaxis]\n",
    "probs = tf.clip_by_value(expected_deaths / (expected_deaths + psi_m), 1e-9, 1.)\n",
    "predicted_deaths = tfd.NegativeBinomial(\n",
    "    total_count=psi_m, probs=probs).sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rbX8kgID1zU"
   },
   "outputs": [],
   "source": [
    "# Predict counterfactual infections/deaths in the absence of interventions\n",
    "no_intervention_infections = predict_infections(\n",
    "    intervention_indicators,\n",
    "    population_value,\n",
    "    sampled_initial_cases,\n",
    "    mu_hat,\n",
    "    tf.zeros_like(alpha_hat),\n",
    "    conv_serial_interval,\n",
    "    INITIAL_DAYS, TOTAL_DAYS)\n",
    "\n",
    "no_intervention_expected_deaths = predict_deaths(\n",
    "    no_intervention_infections, sampled_ifr_noise, conv_fatality_rate)\n",
    "probs = tf.clip_by_value(\n",
    "    no_intervention_expected_deaths / (no_intervention_expected_deaths + psi_m),\n",
    "    1e-9, 1.)\n",
    "no_intervention_predicted_deaths = tfd.NegativeBinomial(\n",
    "    total_count=psi_m, probs=probs).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eo6jR2Is7A80"
   },
   "source": [
    "### 4.1  Effectiveness of interventions\n",
    "\n",
    "Similar to Figure 4 of Flaxman et al. (2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 375
    },
    "colab_type": "code",
    "id": "JZM3qIxyMIP4",
    "outputId": "90c41eca-ba87-4b26-f87d-dce674227995"
   },
   "outputs": [],
   "source": [
    "def intervention_effectiveness(alpha):\n",
    "\n",
    "  alpha_adj = 1. - np.exp(-alpha + np.log(1.05) / 6.)\n",
    "  alpha_adj_first = (\n",
    "      1. - np.exp(-alpha - alpha[..., -1:] + np.log(1.05) / 6.))\n",
    "\n",
    "  fig, ax = plt.subplots(1, 1, figsize=[12, 6])\n",
    "  intervention_perm = [2, 1, 3, 4, 0]\n",
    "  percentile_vals = [2.5, 97.5]\n",
    "  jitter = .2\n",
    "\n",
    "  for ind in range(5):\n",
    "    first_low, first_high = tfp.stats.percentile(\n",
    "        alpha_adj_first[..., ind], percentile_vals)\n",
    "    low, high = tfp.stats.percentile(\n",
    "        alpha_adj[..., ind], percentile_vals)\n",
    "\n",
    "    p_ind = intervention_perm[ind]\n",
    "    ax.hlines(p_ind, low, high, label='Later Intervention', colors='g')\n",
    "    ax.scatter(alpha_adj[..., ind].mean(), p_ind, color='g')\n",
    "    ax.hlines(p_ind + jitter, first_low, first_high,\n",
    "              label='First Intervention', colors='r')\n",
    "    ax.scatter(alpha_adj_first[..., ind].mean(), p_ind + jitter, color='r')\n",
    "\n",
    "    if ind == 0:\n",
    "      plt.legend(loc='lower right')\n",
    "  ax.set_yticks(range(5))\n",
    "  ax.set_yticklabels(\n",
    "      [any_intervention_list[intervention_perm.index(p)] for p in range(5)])\n",
    "  ax.set_xlim([-0.01, 1.])\n",
    "  r = fig.patch\n",
    "  r.set_facecolor('white') \n",
    "\n",
    "intervention_effectiveness(alpha_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WMUxoLEZEayT"
   },
   "source": [
    "### 4.2  Infections, deaths, and R_t by country\n",
    "Similar to Figure 2 of Flaxman et al. (2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "id": "_SRoP26uEayX",
    "outputId": "fae38d24-81e4-48b1-dd43-1d51127818bb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "plot_quantile = True #@param {type:\"boolean\"}\n",
    "forecast_days = 0 #@param {type:\"slider\", min:0, max:75, step:1}\n",
    "\n",
    "fig, ax = plt.subplots(11, 3, figsize=(15, 40))\n",
    "\n",
    "for ind, country in enumerate(COUNTRIES):\n",
    "  num_days = (pd.to_datetime('2020-03-28') - first_days[country]).days + forecast_days\n",
    "  dates = [(first_days[country] + i*pd.to_timedelta(1, 'days')).strftime('%m-%d') for i in range(num_days)]\n",
    "  plot_dates = [dates[i] for i in range(0, num_days, 7)]\n",
    "\n",
    "  # Plot daily number of infections\n",
    "  infections = pred_hat[:, :, ind]\n",
    "  posterior_quantile = np.percentile(infections, [2.5, 25, 50, 75, 97.5], axis=-1)\n",
    "  ax[ind, 0].plot(\n",
    "      dates, posterior_quantile[2, :num_days],\n",
    "      color='b', label='posterior median', lw=2)\n",
    "  if plot_quantile:\n",
    "    ax[ind, 0].fill_between(\n",
    "        dates, posterior_quantile[1, :num_days], posterior_quantile[3, :num_days],\n",
    "        color='b', label='50% quantile', alpha=.4)\n",
    "    ax[ind, 0].fill_between(\n",
    "        dates, posterior_quantile[0, :num_days], posterior_quantile[4, :num_days],\n",
    "        color='b', label='95% quantile', alpha=.2)\n",
    "\n",
    "  ax[ind, 0].set_xticks(plot_dates)\n",
    "  ax[ind, 0].xaxis.set_tick_params(rotation=45)\n",
    "  ax[ind, 0].set_ylabel('Daily number of infections', fontsize='large')\n",
    "  ax[ind, 0].set_xlabel('Day', fontsize='large')\n",
    "\n",
    "  # Plot deaths\n",
    "  ax[ind, 1].set_title(country)\n",
    "\n",
    "  samples = predicted_deaths[:, :, ind]\n",
    "  posterior_quantile = np.percentile(samples, [2.5, 25, 50, 75, 97.5], axis=-1)\n",
    "  ax[ind, 1].plot(\n",
    "      range(num_days), posterior_quantile[2, :num_days],\n",
    "      color='b', label='Posterior median', lw=2)\n",
    "  if plot_quantile:\n",
    "    ax[ind, 1].fill_between(\n",
    "        range(num_days), posterior_quantile[1, :num_days], posterior_quantile[3, :num_days],\n",
    "        color='b', label='50% quantile', alpha=.4)\n",
    "    ax[ind, 1].fill_between(\n",
    "        range(num_days), posterior_quantile[0, :num_days], posterior_quantile[4, :num_days],\n",
    "        color='b', label='95% quantile', alpha=.2)\n",
    "\n",
    "  observed = deaths[ind, :]\n",
    "  observed[observed == -1] = np.nan\n",
    "  ax[ind, 1].plot(\n",
    "      dates, observed[:num_days],\n",
    "      '--o', color='k', markersize=3,\n",
    "      label='Observed deaths', alpha=.8)\n",
    "  ax[ind, 1].set_xticks(plot_dates)\n",
    "  ax[ind, 1].xaxis.set_tick_params(rotation=45)\n",
    "  ax[ind, 1].set_title(country)\n",
    "  ax[ind, 1].set_xlabel('Day', fontsize='large')\n",
    "  ax[ind, 1].set_ylabel('Deaths', fontsize='large')\n",
    "\n",
    "  # Plot R_t\n",
    "  samples = np.transpose(rt_hat[:, ind, :])\n",
    "  posterior_quantile = np.percentile(samples, [2.5, 25, 50, 75, 97.5], axis=-1)\n",
    "  l1 = ax[ind, 2].plot(\n",
    "      dates, posterior_quantile[2, :num_days],\n",
    "      color='g', label='Posterior median', lw=2)\n",
    "  l2 = ax[ind, 2].fill_between(\n",
    "      dates, posterior_quantile[1, :num_days], posterior_quantile[3, :num_days],\n",
    "      color='g', label='50% quantile', alpha=.4)\n",
    "  if plot_quantile:\n",
    "    l3 = ax[ind, 2].fill_between(\n",
    "        dates, posterior_quantile[0, :num_days], posterior_quantile[4, :num_days],\n",
    "        color='g', label='95% quantile', alpha=.2)\n",
    "\n",
    "  l4 = ax[ind, 2].hlines(1., dates[0], dates[-1], linestyle='--', label='R == 1')\n",
    "  ax[ind, 2].set_xlabel('Day', fontsize='large')\n",
    "  ax[ind, 2].set_ylabel('R_t', fontsize='large')\n",
    "  ax[ind, 2].set_xticks(plot_dates)\n",
    "  ax[ind, 2].xaxis.set_tick_params(rotation=45)\n",
    "\n",
    "fontsize = 'medium'\n",
    "ax[0, 0].legend(loc='upper left', fontsize=fontsize)\n",
    "ax[0, 1].legend(loc='upper left', fontsize=fontsize)\n",
    "ax[0, 2].legend(\n",
    "  bbox_to_anchor=(1., 1.),\n",
    "  loc='upper right',\n",
    "  borderaxespad=0.,\n",
    "  fontsize=fontsize)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lyi5k2tHEayl"
   },
   "source": [
    "### 4.3  Daily number of predicted/forecasted deaths with and without interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "id": "mvo9QcOGEayn",
    "outputId": "d38da704-28fd-4188-9db3-121334c60ca0"
   },
   "outputs": [],
   "source": [
    "plot_quantile = True #@param {type:\"boolean\"}\n",
    "forecast_days = 0 #@param {type:\"slider\", min:0, max:30, step:1}\n",
    "\n",
    "fig, ax = plt.subplots(4, 3, figsize=(15, 16))\n",
    "ax = ax.flatten()\n",
    "fig.delaxes(ax[-1])\n",
    "for country_index, country in enumerate(COUNTRIES):\n",
    "  num_days = (pd.to_datetime('2020-03-28') - first_days[country]).days + forecast_days\n",
    "  dates = [(first_days[country] + i*pd.to_timedelta(1, 'days')).strftime('%m-%d') for i in range(num_days)]\n",
    "  plot_dates = [dates[i] for i in range(0, num_days, 7)]\n",
    "\n",
    "  ax[country_index].set_title(country)\n",
    "\n",
    "  quantile_vals = [.025, .25, .5, .75, .975]\n",
    "  samples = predicted_deaths[:, :, country_index].numpy()\n",
    "  quantiles = []\n",
    "\n",
    "  psi_m = psi_hat[np.newaxis, ..., np.newaxis]\n",
    "  probs = tf.clip_by_value(expected_deaths / (expected_deaths + psi_m), 1e-9, 1.)\n",
    "  predicted_deaths_dist = tfd.NegativeBinomial(\n",
    "    total_count=psi_m, probs=probs)\n",
    "\n",
    "  posterior_quantile = np.percentile(samples, [2.5, 25, 50, 75, 97.5], axis=-1)\n",
    "  ax[country_index].plot(\n",
    "      dates, posterior_quantile[2, :num_days],\n",
    "      color='b', label='Posterior median', lw=2)\n",
    "  if plot_quantile:\n",
    "    ax[country_index].fill_between(\n",
    "        dates, posterior_quantile[1, :num_days], posterior_quantile[3, :num_days],\n",
    "        color='b', label='50% quantile', alpha=.4)\n",
    "\n",
    "  samples_counterfact = no_intervention_predicted_deaths[:, :, country_index]\n",
    "  posterior_quantile = np.percentile(samples_counterfact, [2.5, 25, 50, 75, 97.5], axis=-1)\n",
    "  ax[country_index].plot(\n",
    "      dates, posterior_quantile[2, :num_days],\n",
    "      color='r', label='Posterior median', lw=2)\n",
    "  if plot_quantile:\n",
    "    ax[country_index].fill_between(\n",
    "        dates, posterior_quantile[1, :num_days], posterior_quantile[3, :num_days],\n",
    "        color='r', label='50% quantile, no intervention', alpha=.4)\n",
    "\n",
    "  observed = deaths[country_index, :]\n",
    "  observed[observed == -1] = np.nan\n",
    "  ax[country_index].plot(\n",
    "      dates, observed[:num_days],\n",
    "      '--o', color='k', markersize=3,\n",
    "      label='Observed deaths', alpha=.8)\n",
    "  ax[country_index].set_xticks(plot_dates)\n",
    "  ax[country_index].xaxis.set_tick_params(rotation=45)\n",
    "  ax[country_index].set_title(country)\n",
    "  ax[country_index].set_xlabel('Day', fontsize='large')\n",
    "  ax[country_index].set_ylabel('Deaths', fontsize='large')\n",
    "  ax[0].legend(loc='upper left')\n",
    "plt.tight_layout(pad=1.0);"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Estimating COVID-19 in 11 European countries",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
