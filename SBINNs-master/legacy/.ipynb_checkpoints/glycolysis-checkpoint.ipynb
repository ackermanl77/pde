{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "from plotting import newfig, savefig\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time\n",
    "\n",
    "from utilities import neural_net, fwd_gradients,\\\n",
    "                      tf_session, mean_squared_error, relative_error\n",
    "\n",
    "class HiddenPathways(object):\n",
    "    # Initialize the class\n",
    "    def __init__(self, t_data, S_data, t_eqns, layers):\n",
    "        \n",
    "        self.D = S_data.shape[1]\n",
    "        \n",
    "        self.t_min = t_data.min(0)\n",
    "        self.t_max = t_data.max(0)\n",
    "        \n",
    "        self.S_scale = tf.Variable(S_data.std(0), dtype=tf.float32, trainable=False)\n",
    "        \n",
    "        # data on all the species (only some are used as input)\n",
    "        self.t_data, self.S_data = t_data, S_data\n",
    "        self.t_eqns = t_eqns\n",
    "                \n",
    "        # layers\n",
    "        self.layers = layers\n",
    "        \n",
    "#        self.J0 = tf.Variable(2.5, dtype=tf.float32, trainable=False)\n",
    "#        self.k1 = tf.Variable(100.0, dtype=tf.float32, trainable=False)\n",
    "#        self.k2 = tf.Variable(6.0, dtype=tf.float32, trainable=False)\n",
    "#        self.k3 = tf.Variable(16.0, dtype=tf.float32, trainable=False)\n",
    "#        self.k4 = tf.Variable(100.0, dtype=tf.float32, trainable=False)\n",
    "#        self.k5 = tf.Variable(1.28, dtype=tf.float32, trainable=False)\n",
    "#        self.k6 = tf.Variable(12.0, dtype=tf.float32, trainable=False)\n",
    "#        self.k = tf.Variable(1.8, dtype=tf.float32, trainable=False)\n",
    "#        self.kappa = tf.Variable(13.0, dtype=tf.float32, trainable=False)\n",
    "#        self.q = tf.Variable(4.0, dtype=tf.float32, trainable=False)\n",
    "#        self.K1 = tf.Variable(0.52, dtype=tf.float32, trainable=False)\n",
    "#        self.psi = tf.Variable(0.1, dtype=tf.float32, trainable=False)\n",
    "#        self.N = tf.Variable(1.0, dtype=tf.float32, trainable=False)\n",
    "#        self.A = tf.Variable(4.0, dtype=tf.float32, trainable=False)\n",
    "\n",
    "        self.logJ0 = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logk1 = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logk2 = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logk3 = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logk4 = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logk5 = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logk6 = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logk = tf.Variable(1.0, dtype=tf.float32, trainable=True)\n",
    "        self.logkappa = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logq = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logK1 = tf.Variable(1.0, dtype=tf.float32, trainable=True)\n",
    "        self.logpsi = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logN = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        self.logA = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "        \n",
    "        self.var_list_eqns = [self.logJ0, self.logk1, self.logk2, self.logk3, self.logk4,\n",
    "                              self.logk5, self.logk6, self.logk, self.logkappa,\n",
    "                              self.logq, self.logK1, self.logpsi, self.logN, self.logA]\n",
    "  \n",
    "        self.J0 = tf.exp(self.logJ0)\n",
    "        self.k1 = tf.exp(self.logk1)\n",
    "        self.k2 = tf.exp(self.logk2)\n",
    "        self.k3 = tf.exp(self.logk3)\n",
    "        self.k4 = tf.exp(self.logk4)\n",
    "        self.k5 = tf.exp(self.logk5)\n",
    "        self.k6 = tf.exp(self.logk6)\n",
    "        self.k = tf.exp(self.logk)\n",
    "        self.kappa = tf.exp(self.logkappa)\n",
    "        self.q = tf.exp(self.logq)\n",
    "        self.K1 = tf.exp(self.logK1)\n",
    "        self.psi = tf.exp(self.logpsi)\n",
    "        self.N = tf.exp(self.logN)\n",
    "        self.A = tf.exp(self.logA)\n",
    "\n",
    "        # placeholders for data\n",
    "        self.t_data_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.S_data_tf = tf.placeholder(tf.float32, shape=[None, self.D])\n",
    "        self.t_eqns_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "        # physics uninformed neural networks\n",
    "        self.net_sysbio = neural_net(layers=self.layers)\n",
    "        \n",
    "        self.H_data = 2.0*(self.t_data_tf - self.t_min)/(self.t_max - self.t_min) - 1.0\n",
    "        self.S_data_pred = self.S_data[0,:] + self.S_scale*(self.H_data+1.0)*self.net_sysbio(self.H_data)\n",
    "\n",
    "        # physics informed neural networks\n",
    "        self.H_eqns = 2.0*(self.t_eqns_tf - self.t_min)/(self.t_max - self.t_min) - 1.0\n",
    "        self.S_eqns_pred = self.S_data[0,:] + self.S_scale*(self.H_eqns+1.0)*self.net_sysbio(self.H_eqns)\n",
    "\n",
    "        self.E_eqns_pred = self.SysODE(self.S_eqns_pred, self.t_eqns_tf)\n",
    "\n",
    "#        self.S_scale = 0.9*self.S_scale + 0.1*tf.math.reduce_std(self.S_eqns_pred, 0)\n",
    "#        scale_list = tf.unstack(self.S_scale)\n",
    "#        scale_list[4:6] = self.S_data.std(0)[4:6]\n",
    "#        self.S_scale = tf.stack(scale_list)\n",
    "        \n",
    "        # loss\n",
    "        self.loss_data = mean_squared_error(self.S_data_tf[:,4:6]/self.S_scale[4:6], self.S_data_pred[:,4:6]/self.S_scale[4:6])\n",
    "        self.loss_eqns = mean_squared_error(0.0, self.E_eqns_pred/self.S_scale)\n",
    "        self.loss_auxl = mean_squared_error(self.S_data_tf[-1,:]/self.S_scale[:], self.S_data_pred[-1,:]/self.S_scale[:])\n",
    "        self.loss = 0.95*self.loss_data + 0.05*self.loss_eqns + 0.05*self.loss_auxl\n",
    "        \n",
    "        # optimizers\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.optimizer_para = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "        self.train_op = self.optimizer.minimize(self.loss,\n",
    "                                                var_list=[self.net_sysbio.weights,\n",
    "                                                          self.net_sysbio.biases,\n",
    "                                                          self.net_sysbio.gammas])\n",
    "        self.trainpara_op = self.optimizer_para.minimize(self.loss,\n",
    "                                                         var_list=self.var_list_eqns)\n",
    "        self.sess = tf_session()\n",
    "\n",
    "    def SysODE(self, S, t):\n",
    "        F1 = self.J0 - (self.k1*S[:,0:1]*S[:,5:6])/(1+(S[:,5:6]/self.K1)**self.q)\n",
    "        F2 = 2*(self.k1*S[:,0:1]*S[:,5:6])/(1+(S[:,5:6]/self.K1)**self.q) - self.k2*S[:,1:2]*(self.N-S[:,4:5]) - self.k6*S[:,1:2]*S[:,4:5]\n",
    "        F3 = self.k2*S[:,1:2]*(self.N-S[:,4:5]) - self.k3*S[:,2:3]*(self.A-S[:,5:6])\n",
    "        F4 = self.k3*S[:,2:3]*(self.A-S[:,5:6]) - self.k4*S[:,3:4]*S[:,4:5] - self.kappa*(S[:,3:4]-S[:,6:7])\n",
    "        F5 = self.k2*S[:,1:2]*(self.N-S[:,4:5]) - self.k4*S[:,3:4]*S[:,4:5] - self.k6*S[:,1:2]*S[:,4:5]\n",
    "        F6 = -2*(self.k1*S[:,0:1]*S[:,5:6])/(1+(S[:,5:6]/self.K1)**self.q) + 2*self.k3*S[:,2:3]*(self.A-S[:,5:6]) - self.k5*S[:,5:6]\n",
    "        F7 = self.psi*self.kappa*(S[:,3:4]-S[:,6:7]) - self.k*S[:,6:7]\n",
    "        \n",
    "        F = tf.concat([F1, F2, F3, F4, F5, F6, F7], 1)\n",
    "\n",
    "        S_t = fwd_gradients(S, t)\n",
    "        \n",
    "        E = S_t - F\n",
    "        return E\n",
    "    \n",
    "    def train(self, num_epochs, batch_size, learning_rate):\n",
    "\n",
    "        N_data = self.t_data.shape[0]\n",
    "        N_eqns = self.t_eqns.shape[0]\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            for it in range(N_eqns//batch_size):\n",
    "                idx_data = np.concatenate([np.array([0]),\n",
    "                                           np.random.choice(np.arange(1, N_data-1), min(batch_size, N_data)-2),\n",
    "                                           np.array([N_data-1])])\n",
    "                idx_eqns = np.random.choice(N_eqns, batch_size)\n",
    "\n",
    "                t_data_batch, S_data_batch = self.t_data[idx_data,:], self.S_data[idx_data,:]\n",
    "                t_eqns_batch = self.t_eqns[idx_eqns,:]\n",
    "    \n",
    "                tf_dict = {self.t_data_tf: t_data_batch,\n",
    "                           self.S_data_tf: S_data_batch,\n",
    "                           self.t_eqns_tf: t_eqns_batch,\n",
    "                           self.learning_rate: learning_rate}\n",
    "                \n",
    "                self.sess.run([self.train_op, self.trainpara_op], tf_dict)\n",
    "                \n",
    "                # Print\n",
    "                if it % 10 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    [loss_data_value,\n",
    "                     loss_eqns_value,\n",
    "                     loss_auxl_value,\n",
    "                     learning_rate_value] = self.sess.run([self.loss_data,\n",
    "                                                           self.loss_eqns,\n",
    "                                                           self.loss_auxl,\n",
    "                                                           self.learning_rate], tf_dict)\n",
    "                    print('Epoch: %d, It: %d, Loss Data: %.3e, Loss Eqns: %.3e, Loss Aux: %.3e, Time: %.3f, Learning Rate: %.1e'\n",
    "                          %(epoch, it, loss_data_value, loss_eqns_value, loss_auxl_value, elapsed, learning_rate_value))\n",
    "                    start_time = time.time()\n",
    "\n",
    "    def predict(self, t_star):\n",
    "        \n",
    "        tf_dict = {self.t_eqns_tf: t_star}\n",
    "        \n",
    "        S_star, S_scale = self.sess.run([self.S_eqns_pred, self.S_scale], tf_dict)\n",
    "        return S_star, S_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/david/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n",
      "Epoch: 0, It: 0, Loss Data: 3.713e+00, Loss Eqns: 2.801e+02, Loss Aux: 4.458e+00, Time: 1.758, Learning Rate: 1.0e-03\n",
      "Epoch: 1, It: 0, Loss Data: 3.800e+00, Loss Eqns: 2.540e+02, Loss Aux: 3.792e+00, Time: 0.098, Learning Rate: 1.0e-03\n",
      "Epoch: 2, It: 0, Loss Data: 3.530e+00, Loss Eqns: 2.173e+02, Loss Aux: 3.285e+00, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 3, It: 0, Loss Data: 3.240e+00, Loss Eqns: 2.438e+02, Loss Aux: 6.749e+00, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 4, It: 0, Loss Data: 3.172e+00, Loss Eqns: 2.170e+02, Loss Aux: 4.163e+00, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 5, It: 0, Loss Data: 2.815e+00, Loss Eqns: 1.978e+02, Loss Aux: 2.836e+00, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 6, It: 0, Loss Data: 2.980e+00, Loss Eqns: 2.035e+02, Loss Aux: 2.492e+00, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 7, It: 0, Loss Data: 3.230e+00, Loss Eqns: 1.968e+02, Loss Aux: 2.367e+00, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 8, It: 0, Loss Data: 2.911e+00, Loss Eqns: 1.899e+02, Loss Aux: 2.253e+00, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 9, It: 0, Loss Data: 2.963e+00, Loss Eqns: 1.802e+02, Loss Aux: 2.198e+00, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 10, It: 0, Loss Data: 2.634e+00, Loss Eqns: 1.591e+02, Loss Aux: 2.346e+00, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 11, It: 0, Loss Data: 2.465e+00, Loss Eqns: 1.509e+02, Loss Aux: 2.766e+00, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 12, It: 0, Loss Data: 2.396e+00, Loss Eqns: 1.404e+02, Loss Aux: 3.085e+00, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 13, It: 0, Loss Data: 2.228e+00, Loss Eqns: 1.282e+02, Loss Aux: 2.952e+00, Time: 0.112, Learning Rate: 1.0e-03\n",
      "Epoch: 14, It: 0, Loss Data: 2.268e+00, Loss Eqns: 1.093e+02, Loss Aux: 2.602e+00, Time: 0.092, Learning Rate: 1.0e-03\n",
      "Epoch: 15, It: 0, Loss Data: 2.180e+00, Loss Eqns: 9.958e+01, Loss Aux: 2.279e+00, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 16, It: 0, Loss Data: 1.960e+00, Loss Eqns: 8.371e+01, Loss Aux: 2.042e+00, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 17, It: 0, Loss Data: 1.967e+00, Loss Eqns: 6.869e+01, Loss Aux: 1.841e+00, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 18, It: 0, Loss Data: 1.811e+00, Loss Eqns: 4.522e+01, Loss Aux: 1.652e+00, Time: 0.093, Learning Rate: 1.0e-03\n",
      "Epoch: 19, It: 0, Loss Data: 1.732e+00, Loss Eqns: 3.193e+01, Loss Aux: 1.552e+00, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 20, It: 0, Loss Data: 1.586e+00, Loss Eqns: 3.504e+01, Loss Aux: 1.510e+00, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 21, It: 0, Loss Data: 1.360e+00, Loss Eqns: 3.686e+01, Loss Aux: 1.348e+00, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 22, It: 0, Loss Data: 1.289e+00, Loss Eqns: 2.703e+01, Loss Aux: 1.250e+00, Time: 0.068, Learning Rate: 1.0e-03\n",
      "Epoch: 23, It: 0, Loss Data: 1.244e+00, Loss Eqns: 2.095e+01, Loss Aux: 1.185e+00, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 24, It: 0, Loss Data: 1.288e+00, Loss Eqns: 2.326e+01, Loss Aux: 1.133e+00, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 25, It: 0, Loss Data: 1.357e+00, Loss Eqns: 2.698e+01, Loss Aux: 1.106e+00, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 26, It: 0, Loss Data: 1.423e+00, Loss Eqns: 2.560e+01, Loss Aux: 1.156e+00, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 27, It: 0, Loss Data: 1.293e+00, Loss Eqns: 2.488e+01, Loss Aux: 1.143e+00, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 28, It: 0, Loss Data: 1.164e+00, Loss Eqns: 2.023e+01, Loss Aux: 1.061e+00, Time: 0.080, Learning Rate: 1.0e-03\n",
      "Epoch: 29, It: 0, Loss Data: 1.154e+00, Loss Eqns: 1.736e+01, Loss Aux: 1.001e+00, Time: 0.097, Learning Rate: 1.0e-03\n",
      "Epoch: 30, It: 0, Loss Data: 1.019e+00, Loss Eqns: 1.507e+01, Loss Aux: 9.475e-01, Time: 0.091, Learning Rate: 1.0e-03\n",
      "Epoch: 31, It: 0, Loss Data: 1.067e+00, Loss Eqns: 2.000e+01, Loss Aux: 8.889e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 32, It: 0, Loss Data: 1.209e+00, Loss Eqns: 2.006e+01, Loss Aux: 8.295e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 33, It: 0, Loss Data: 1.090e+00, Loss Eqns: 1.632e+01, Loss Aux: 8.286e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 34, It: 0, Loss Data: 1.081e+00, Loss Eqns: 1.514e+01, Loss Aux: 9.197e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 35, It: 0, Loss Data: 1.051e+00, Loss Eqns: 1.357e+01, Loss Aux: 1.046e+00, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 36, It: 0, Loss Data: 1.140e+00, Loss Eqns: 1.578e+01, Loss Aux: 1.092e+00, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 37, It: 0, Loss Data: 1.194e+00, Loss Eqns: 1.487e+01, Loss Aux: 1.066e+00, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 38, It: 0, Loss Data: 1.141e+00, Loss Eqns: 1.286e+01, Loss Aux: 1.033e+00, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 39, It: 0, Loss Data: 1.114e+00, Loss Eqns: 1.320e+01, Loss Aux: 1.013e+00, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 40, It: 0, Loss Data: 1.122e+00, Loss Eqns: 1.193e+01, Loss Aux: 9.670e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 41, It: 0, Loss Data: 1.066e+00, Loss Eqns: 1.357e+01, Loss Aux: 8.842e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 42, It: 0, Loss Data: 1.155e+00, Loss Eqns: 1.410e+01, Loss Aux: 7.807e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 43, It: 0, Loss Data: 1.062e+00, Loss Eqns: 1.299e+01, Loss Aux: 7.164e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 44, It: 0, Loss Data: 1.060e+00, Loss Eqns: 1.144e+01, Loss Aux: 7.023e-01, Time: 0.073, Learning Rate: 1.0e-03\n",
      "Epoch: 45, It: 0, Loss Data: 1.123e+00, Loss Eqns: 1.141e+01, Loss Aux: 7.218e-01, Time: 0.064, Learning Rate: 1.0e-03\n",
      "Epoch: 46, It: 0, Loss Data: 1.192e+00, Loss Eqns: 1.090e+01, Loss Aux: 7.404e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 47, It: 0, Loss Data: 1.107e+00, Loss Eqns: 1.232e+01, Loss Aux: 7.253e-01, Time: 0.069, Learning Rate: 1.0e-03\n",
      "Epoch: 48, It: 0, Loss Data: 1.056e+00, Loss Eqns: 1.138e+01, Loss Aux: 6.775e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 49, It: 0, Loss Data: 1.112e+00, Loss Eqns: 1.163e+01, Loss Aux: 6.347e-01, Time: 0.064, Learning Rate: 1.0e-03\n",
      "Epoch: 50, It: 0, Loss Data: 1.145e+00, Loss Eqns: 1.250e+01, Loss Aux: 6.222e-01, Time: 0.067, Learning Rate: 1.0e-03\n",
      "Epoch: 51, It: 0, Loss Data: 1.120e+00, Loss Eqns: 1.198e+01, Loss Aux: 6.410e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 52, It: 0, Loss Data: 1.054e+00, Loss Eqns: 1.182e+01, Loss Aux: 6.855e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 53, It: 0, Loss Data: 1.128e+00, Loss Eqns: 1.159e+01, Loss Aux: 7.253e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 54, It: 0, Loss Data: 9.734e-01, Loss Eqns: 1.054e+01, Loss Aux: 7.339e-01, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 55, It: 0, Loss Data: 1.100e+00, Loss Eqns: 9.749e+00, Loss Aux: 7.068e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 56, It: 0, Loss Data: 1.108e+00, Loss Eqns: 1.029e+01, Loss Aux: 6.700e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 57, It: 0, Loss Data: 1.098e+00, Loss Eqns: 1.132e+01, Loss Aux: 6.498e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 58, It: 0, Loss Data: 1.035e+00, Loss Eqns: 1.123e+01, Loss Aux: 6.521e-01, Time: 0.081, Learning Rate: 1.0e-03\n",
      "Epoch: 59, It: 0, Loss Data: 1.073e+00, Loss Eqns: 9.867e+00, Loss Aux: 6.472e-01, Time: 0.061, Learning Rate: 1.0e-03\n",
      "Epoch: 60, It: 0, Loss Data: 1.122e+00, Loss Eqns: 1.030e+01, Loss Aux: 6.171e-01, Time: 0.067, Learning Rate: 1.0e-03\n",
      "Epoch: 61, It: 0, Loss Data: 1.064e+00, Loss Eqns: 1.115e+01, Loss Aux: 5.745e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 62, It: 0, Loss Data: 1.116e+00, Loss Eqns: 9.160e+00, Loss Aux: 5.412e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 63, It: 0, Loss Data: 1.066e+00, Loss Eqns: 1.009e+01, Loss Aux: 5.375e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 64, It: 0, Loss Data: 1.085e+00, Loss Eqns: 8.291e+00, Loss Aux: 5.325e-01, Time: 0.088, Learning Rate: 1.0e-03\n",
      "Epoch: 65, It: 0, Loss Data: 1.120e+00, Loss Eqns: 9.463e+00, Loss Aux: 5.138e-01, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 66, It: 0, Loss Data: 1.066e+00, Loss Eqns: 9.932e+00, Loss Aux: 5.129e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 67, It: 0, Loss Data: 1.088e+00, Loss Eqns: 1.039e+01, Loss Aux: 5.535e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 68, It: 0, Loss Data: 1.070e+00, Loss Eqns: 8.573e+00, Loss Aux: 6.253e-01, Time: 0.064, Learning Rate: 1.0e-03\n",
      "Epoch: 69, It: 0, Loss Data: 1.111e+00, Loss Eqns: 8.709e+00, Loss Aux: 6.664e-01, Time: 0.070, Learning Rate: 1.0e-03\n",
      "Epoch: 70, It: 0, Loss Data: 9.672e-01, Loss Eqns: 8.752e+00, Loss Aux: 6.416e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 71, It: 0, Loss Data: 1.015e+00, Loss Eqns: 8.706e+00, Loss Aux: 5.622e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 72, It: 0, Loss Data: 1.068e+00, Loss Eqns: 8.437e+00, Loss Aux: 4.857e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 73, It: 0, Loss Data: 9.937e-01, Loss Eqns: 9.011e+00, Loss Aux: 4.595e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 74, It: 0, Loss Data: 1.011e+00, Loss Eqns: 9.290e+00, Loss Aux: 4.705e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 75, It: 0, Loss Data: 1.004e+00, Loss Eqns: 8.198e+00, Loss Aux: 4.599e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 76, It: 0, Loss Data: 1.080e+00, Loss Eqns: 9.286e+00, Loss Aux: 4.157e-01, Time: 0.079, Learning Rate: 1.0e-03\n",
      "Epoch: 77, It: 0, Loss Data: 9.870e-01, Loss Eqns: 8.801e+00, Loss Aux: 4.204e-01, Time: 0.093, Learning Rate: 1.0e-03\n",
      "Epoch: 78, It: 0, Loss Data: 1.018e+00, Loss Eqns: 7.772e+00, Loss Aux: 4.951e-01, Time: 0.101, Learning Rate: 1.0e-03\n",
      "Epoch: 79, It: 0, Loss Data: 1.004e+00, Loss Eqns: 8.522e+00, Loss Aux: 5.629e-01, Time: 0.107, Learning Rate: 1.0e-03\n",
      "Epoch: 80, It: 0, Loss Data: 1.069e+00, Loss Eqns: 8.004e+00, Loss Aux: 4.957e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 81, It: 0, Loss Data: 9.579e-01, Loss Eqns: 9.004e+00, Loss Aux: 3.982e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 82, It: 0, Loss Data: 9.882e-01, Loss Eqns: 8.641e+00, Loss Aux: 3.734e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 83, It: 0, Loss Data: 1.126e+00, Loss Eqns: 8.222e+00, Loss Aux: 4.121e-01, Time: 0.090, Learning Rate: 1.0e-03\n",
      "Epoch: 84, It: 0, Loss Data: 1.083e+00, Loss Eqns: 8.058e+00, Loss Aux: 4.520e-01, Time: 0.100, Learning Rate: 1.0e-03\n",
      "Epoch: 85, It: 0, Loss Data: 1.065e+00, Loss Eqns: 8.513e+00, Loss Aux: 4.247e-01, Time: 0.096, Learning Rate: 1.0e-03\n",
      "Epoch: 86, It: 0, Loss Data: 1.042e+00, Loss Eqns: 8.193e+00, Loss Aux: 3.816e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 87, It: 0, Loss Data: 9.849e-01, Loss Eqns: 8.085e+00, Loss Aux: 3.727e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 88, It: 0, Loss Data: 1.015e+00, Loss Eqns: 8.033e+00, Loss Aux: 3.582e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 89, It: 0, Loss Data: 1.041e+00, Loss Eqns: 7.471e+00, Loss Aux: 3.716e-01, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 90, It: 0, Loss Data: 9.490e-01, Loss Eqns: 8.957e+00, Loss Aux: 4.217e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 91, It: 0, Loss Data: 1.042e+00, Loss Eqns: 7.032e+00, Loss Aux: 4.064e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 92, It: 0, Loss Data: 9.180e-01, Loss Eqns: 7.562e+00, Loss Aux: 4.091e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 93, It: 0, Loss Data: 1.025e+00, Loss Eqns: 8.075e+00, Loss Aux: 4.388e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 94, It: 0, Loss Data: 1.037e+00, Loss Eqns: 7.426e+00, Loss Aux: 4.715e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 95, It: 0, Loss Data: 1.054e+00, Loss Eqns: 7.479e+00, Loss Aux: 4.068e-01, Time: 0.073, Learning Rate: 1.0e-03\n",
      "Epoch: 96, It: 0, Loss Data: 1.046e+00, Loss Eqns: 7.998e+00, Loss Aux: 3.583e-01, Time: 0.091, Learning Rate: 1.0e-03\n",
      "Epoch: 97, It: 0, Loss Data: 1.042e+00, Loss Eqns: 7.559e+00, Loss Aux: 3.849e-01, Time: 0.081, Learning Rate: 1.0e-03\n",
      "Epoch: 98, It: 0, Loss Data: 1.094e+00, Loss Eqns: 8.280e+00, Loss Aux: 4.030e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 99, It: 0, Loss Data: 9.708e-01, Loss Eqns: 7.680e+00, Loss Aux: 3.005e-01, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 100, It: 0, Loss Data: 9.865e-01, Loss Eqns: 6.674e+00, Loss Aux: 2.472e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 101, It: 0, Loss Data: 1.104e+00, Loss Eqns: 7.049e+00, Loss Aux: 2.729e-01, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 102, It: 0, Loss Data: 9.776e-01, Loss Eqns: 7.715e+00, Loss Aux: 3.255e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 103, It: 0, Loss Data: 9.381e-01, Loss Eqns: 7.391e+00, Loss Aux: 3.358e-01, Time: 0.078, Learning Rate: 1.0e-03\n",
      "Epoch: 104, It: 0, Loss Data: 1.089e+00, Loss Eqns: 6.449e+00, Loss Aux: 3.586e-01, Time: 0.083, Learning Rate: 1.0e-03\n",
      "Epoch: 105, It: 0, Loss Data: 9.967e-01, Loss Eqns: 7.704e+00, Loss Aux: 4.463e-01, Time: 0.063, Learning Rate: 1.0e-03\n",
      "Epoch: 106, It: 0, Loss Data: 1.080e+00, Loss Eqns: 6.013e+00, Loss Aux: 3.290e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 107, It: 0, Loss Data: 1.006e+00, Loss Eqns: 7.889e+00, Loss Aux: 3.423e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 108, It: 0, Loss Data: 9.426e-01, Loss Eqns: 7.652e+00, Loss Aux: 3.577e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 109, It: 0, Loss Data: 1.024e+00, Loss Eqns: 7.068e+00, Loss Aux: 2.242e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 110, It: 0, Loss Data: 9.779e-01, Loss Eqns: 6.794e+00, Loss Aux: 2.680e-01, Time: 0.069, Learning Rate: 1.0e-03\n",
      "Epoch: 111, It: 0, Loss Data: 1.090e+00, Loss Eqns: 6.375e+00, Loss Aux: 2.848e-01, Time: 0.083, Learning Rate: 1.0e-03\n",
      "Epoch: 112, It: 0, Loss Data: 9.722e-01, Loss Eqns: 6.818e+00, Loss Aux: 3.267e-01, Time: 0.080, Learning Rate: 1.0e-03\n",
      "Epoch: 113, It: 0, Loss Data: 1.076e+00, Loss Eqns: 6.136e+00, Loss Aux: 2.989e-01, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 114, It: 0, Loss Data: 9.934e-01, Loss Eqns: 6.744e+00, Loss Aux: 2.971e-01, Time: 0.061, Learning Rate: 1.0e-03\n",
      "Epoch: 115, It: 0, Loss Data: 1.001e+00, Loss Eqns: 6.603e+00, Loss Aux: 3.287e-01, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 116, It: 0, Loss Data: 1.040e+00, Loss Eqns: 6.632e+00, Loss Aux: 2.641e-01, Time: 0.087, Learning Rate: 1.0e-03\n",
      "Epoch: 117, It: 0, Loss Data: 1.055e+00, Loss Eqns: 6.593e+00, Loss Aux: 2.514e-01, Time: 0.082, Learning Rate: 1.0e-03\n",
      "Epoch: 118, It: 0, Loss Data: 1.005e+00, Loss Eqns: 6.086e+00, Loss Aux: 3.470e-01, Time: 0.090, Learning Rate: 1.0e-03\n",
      "Epoch: 119, It: 0, Loss Data: 9.364e-01, Loss Eqns: 7.288e+00, Loss Aux: 3.049e-01, Time: 0.091, Learning Rate: 1.0e-03\n",
      "Epoch: 120, It: 0, Loss Data: 1.023e+00, Loss Eqns: 5.991e+00, Loss Aux: 3.600e-01, Time: 0.082, Learning Rate: 1.0e-03\n",
      "Epoch: 121, It: 0, Loss Data: 1.080e+00, Loss Eqns: 5.732e+00, Loss Aux: 3.911e-01, Time: 0.090, Learning Rate: 1.0e-03\n",
      "Epoch: 122, It: 0, Loss Data: 9.522e-01, Loss Eqns: 6.353e+00, Loss Aux: 3.520e-01, Time: 0.088, Learning Rate: 1.0e-03\n",
      "Epoch: 123, It: 0, Loss Data: 1.017e+00, Loss Eqns: 6.718e+00, Loss Aux: 2.872e-01, Time: 0.082, Learning Rate: 1.0e-03\n",
      "Epoch: 124, It: 0, Loss Data: 1.060e+00, Loss Eqns: 6.093e+00, Loss Aux: 2.825e-01, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 125, It: 0, Loss Data: 1.048e+00, Loss Eqns: 6.353e+00, Loss Aux: 1.988e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 126, It: 0, Loss Data: 1.024e+00, Loss Eqns: 5.846e+00, Loss Aux: 2.600e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 127, It: 0, Loss Data: 1.015e+00, Loss Eqns: 7.318e+00, Loss Aux: 3.151e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 128, It: 0, Loss Data: 9.437e-01, Loss Eqns: 5.602e+00, Loss Aux: 4.232e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 129, It: 0, Loss Data: 1.077e+00, Loss Eqns: 6.792e+00, Loss Aux: 2.780e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 130, It: 0, Loss Data: 1.001e+00, Loss Eqns: 6.336e+00, Loss Aux: 2.780e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 131, It: 0, Loss Data: 9.788e-01, Loss Eqns: 6.820e+00, Loss Aux: 2.428e-01, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 132, It: 0, Loss Data: 1.048e+00, Loss Eqns: 5.906e+00, Loss Aux: 1.522e-01, Time: 0.084, Learning Rate: 1.0e-03\n",
      "Epoch: 133, It: 0, Loss Data: 1.056e+00, Loss Eqns: 7.121e+00, Loss Aux: 3.485e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 134, It: 0, Loss Data: 9.551e-01, Loss Eqns: 6.836e+00, Loss Aux: 3.403e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 135, It: 0, Loss Data: 1.016e+00, Loss Eqns: 5.771e+00, Loss Aux: 4.810e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 136, It: 0, Loss Data: 1.019e+00, Loss Eqns: 5.559e+00, Loss Aux: 4.884e-01, Time: 0.061, Learning Rate: 1.0e-03\n",
      "Epoch: 137, It: 0, Loss Data: 1.014e+00, Loss Eqns: 5.680e+00, Loss Aux: 1.992e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 138, It: 0, Loss Data: 9.389e-01, Loss Eqns: 6.817e+00, Loss Aux: 1.261e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 139, It: 0, Loss Data: 1.011e+00, Loss Eqns: 6.328e+00, Loss Aux: 2.383e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 140, It: 0, Loss Data: 9.537e-01, Loss Eqns: 6.331e+00, Loss Aux: 2.398e-01, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 141, It: 0, Loss Data: 1.044e+00, Loss Eqns: 5.615e+00, Loss Aux: 5.019e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 142, It: 0, Loss Data: 9.424e-01, Loss Eqns: 6.802e+00, Loss Aux: 6.781e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 143, It: 0, Loss Data: 9.962e-01, Loss Eqns: 6.932e+00, Loss Aux: 3.007e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 144, It: 0, Loss Data: 9.736e-01, Loss Eqns: 5.914e+00, Loss Aux: 2.091e-01, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 145, It: 0, Loss Data: 9.585e-01, Loss Eqns: 5.731e+00, Loss Aux: 1.915e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 146, It: 0, Loss Data: 1.095e+00, Loss Eqns: 6.307e+00, Loss Aux: 1.741e-01, Time: 0.067, Learning Rate: 1.0e-03\n",
      "Epoch: 147, It: 0, Loss Data: 1.040e+00, Loss Eqns: 5.637e+00, Loss Aux: 3.452e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 148, It: 0, Loss Data: 9.915e-01, Loss Eqns: 5.427e+00, Loss Aux: 5.136e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 149, It: 0, Loss Data: 1.049e+00, Loss Eqns: 5.232e+00, Loss Aux: 3.189e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 150, It: 0, Loss Data: 9.930e-01, Loss Eqns: 5.425e+00, Loss Aux: 1.710e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 151, It: 0, Loss Data: 1.032e+00, Loss Eqns: 5.419e+00, Loss Aux: 1.859e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 152, It: 0, Loss Data: 9.497e-01, Loss Eqns: 5.289e+00, Loss Aux: 2.388e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 153, It: 0, Loss Data: 9.466e-01, Loss Eqns: 5.521e+00, Loss Aux: 1.922e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 154, It: 0, Loss Data: 9.045e-01, Loss Eqns: 4.872e+00, Loss Aux: 3.158e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 155, It: 0, Loss Data: 9.917e-01, Loss Eqns: 5.797e+00, Loss Aux: 3.834e-01, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 156, It: 0, Loss Data: 1.013e+00, Loss Eqns: 5.068e+00, Loss Aux: 2.084e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 157, It: 0, Loss Data: 1.007e+00, Loss Eqns: 5.402e+00, Loss Aux: 1.762e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 158, It: 0, Loss Data: 1.081e+00, Loss Eqns: 5.408e+00, Loss Aux: 3.045e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 159, It: 0, Loss Data: 9.524e-01, Loss Eqns: 5.417e+00, Loss Aux: 3.264e-01, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 160, It: 0, Loss Data: 1.072e+00, Loss Eqns: 5.900e+00, Loss Aux: 1.674e-01, Time: 0.064, Learning Rate: 1.0e-03\n",
      "Epoch: 161, It: 0, Loss Data: 1.003e+00, Loss Eqns: 5.394e+00, Loss Aux: 1.845e-01, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 162, It: 0, Loss Data: 9.177e-01, Loss Eqns: 5.858e+00, Loss Aux: 3.356e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 163, It: 0, Loss Data: 9.383e-01, Loss Eqns: 5.026e+00, Loss Aux: 2.754e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 164, It: 0, Loss Data: 9.855e-01, Loss Eqns: 5.240e+00, Loss Aux: 1.285e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 165, It: 0, Loss Data: 9.409e-01, Loss Eqns: 5.323e+00, Loss Aux: 2.745e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 166, It: 0, Loss Data: 9.564e-01, Loss Eqns: 5.027e+00, Loss Aux: 7.031e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 167, It: 0, Loss Data: 1.021e+00, Loss Eqns: 5.094e+00, Loss Aux: 4.872e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 168, It: 0, Loss Data: 9.907e-01, Loss Eqns: 5.573e+00, Loss Aux: 1.475e-01, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 169, It: 0, Loss Data: 9.426e-01, Loss Eqns: 5.084e+00, Loss Aux: 1.269e-01, Time: 0.076, Learning Rate: 1.0e-03\n",
      "Epoch: 170, It: 0, Loss Data: 1.043e+00, Loss Eqns: 4.309e+00, Loss Aux: 2.202e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 171, It: 0, Loss Data: 9.664e-01, Loss Eqns: 5.585e+00, Loss Aux: 3.685e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 172, It: 0, Loss Data: 9.327e-01, Loss Eqns: 5.176e+00, Loss Aux: 3.730e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 173, It: 0, Loss Data: 9.902e-01, Loss Eqns: 4.826e+00, Loss Aux: 2.506e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 174, It: 0, Loss Data: 9.667e-01, Loss Eqns: 4.769e+00, Loss Aux: 1.594e-01, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 175, It: 0, Loss Data: 9.515e-01, Loss Eqns: 5.617e+00, Loss Aux: 1.646e-01, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 176, It: 0, Loss Data: 9.753e-01, Loss Eqns: 4.845e+00, Loss Aux: 2.234e-01, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 177, It: 0, Loss Data: 9.195e-01, Loss Eqns: 4.510e+00, Loss Aux: 3.393e-01, Time: 0.068, Learning Rate: 1.0e-03\n",
      "Epoch: 178, It: 0, Loss Data: 9.524e-01, Loss Eqns: 4.744e+00, Loss Aux: 3.904e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 179, It: 0, Loss Data: 1.037e+00, Loss Eqns: 4.089e+00, Loss Aux: 3.866e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 180, It: 0, Loss Data: 9.624e-01, Loss Eqns: 4.498e+00, Loss Aux: 2.816e-01, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 181, It: 0, Loss Data: 1.010e+00, Loss Eqns: 4.608e+00, Loss Aux: 1.870e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 182, It: 0, Loss Data: 9.289e-01, Loss Eqns: 4.483e+00, Loss Aux: 1.501e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 183, It: 0, Loss Data: 9.426e-01, Loss Eqns: 5.467e+00, Loss Aux: 1.473e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 184, It: 0, Loss Data: 9.505e-01, Loss Eqns: 6.101e+00, Loss Aux: 2.773e-01, Time: 0.061, Learning Rate: 1.0e-03\n",
      "Epoch: 185, It: 0, Loss Data: 1.010e+00, Loss Eqns: 5.567e+00, Loss Aux: 3.191e-01, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 186, It: 0, Loss Data: 9.229e-01, Loss Eqns: 5.041e+00, Loss Aux: 1.911e-01, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 187, It: 0, Loss Data: 8.596e-01, Loss Eqns: 6.678e+00, Loss Aux: 1.339e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 188, It: 0, Loss Data: 9.348e-01, Loss Eqns: 6.386e+00, Loss Aux: 1.141e-01, Time: 0.063, Learning Rate: 1.0e-03\n",
      "Epoch: 189, It: 0, Loss Data: 9.167e-01, Loss Eqns: 4.841e+00, Loss Aux: 2.008e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 190, It: 0, Loss Data: 9.632e-01, Loss Eqns: 9.404e+00, Loss Aux: 3.961e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 191, It: 0, Loss Data: 9.567e-01, Loss Eqns: 6.121e+00, Loss Aux: 2.238e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 192, It: 0, Loss Data: 1.008e+00, Loss Eqns: 7.324e+00, Loss Aux: 1.229e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 193, It: 0, Loss Data: 9.056e-01, Loss Eqns: 5.566e+00, Loss Aux: 1.824e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 194, It: 0, Loss Data: 9.173e-01, Loss Eqns: 8.895e+00, Loss Aux: 2.655e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 195, It: 0, Loss Data: 9.433e-01, Loss Eqns: 6.709e+00, Loss Aux: 1.217e-01, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 196, It: 0, Loss Data: 1.027e+00, Loss Eqns: 9.194e+00, Loss Aux: 1.277e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 197, It: 0, Loss Data: 1.001e+00, Loss Eqns: 5.920e+00, Loss Aux: 2.433e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 198, It: 0, Loss Data: 9.189e-01, Loss Eqns: 6.797e+00, Loss Aux: 3.112e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 199, It: 0, Loss Data: 1.045e+00, Loss Eqns: 8.042e+00, Loss Aux: 2.045e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 200, It: 0, Loss Data: 1.138e+00, Loss Eqns: 6.576e+00, Loss Aux: 8.648e-02, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 201, It: 0, Loss Data: 9.523e-01, Loss Eqns: 8.436e+00, Loss Aux: 1.110e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 202, It: 0, Loss Data: 1.038e+00, Loss Eqns: 5.797e+00, Loss Aux: 2.289e-01, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 203, It: 0, Loss Data: 9.621e-01, Loss Eqns: 6.809e+00, Loss Aux: 2.926e-01, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 204, It: 0, Loss Data: 9.720e-01, Loss Eqns: 5.138e+00, Loss Aux: 9.398e-02, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 205, It: 0, Loss Data: 9.450e-01, Loss Eqns: 5.311e+00, Loss Aux: 8.745e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 206, It: 0, Loss Data: 9.408e-01, Loss Eqns: 5.673e+00, Loss Aux: 1.930e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 207, It: 0, Loss Data: 9.755e-01, Loss Eqns: 4.129e+00, Loss Aux: 5.179e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 208, It: 0, Loss Data: 8.389e-01, Loss Eqns: 4.136e+00, Loss Aux: 5.178e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 209, It: 0, Loss Data: 8.656e-01, Loss Eqns: 5.654e+00, Loss Aux: 2.111e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 210, It: 0, Loss Data: 9.633e-01, Loss Eqns: 5.319e+00, Loss Aux: 1.489e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 211, It: 0, Loss Data: 1.009e+00, Loss Eqns: 4.342e+00, Loss Aux: 2.343e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 212, It: 0, Loss Data: 9.528e-01, Loss Eqns: 4.836e+00, Loss Aux: 4.546e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 213, It: 0, Loss Data: 9.913e-01, Loss Eqns: 4.072e+00, Loss Aux: 3.300e-01, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 214, It: 0, Loss Data: 1.018e+00, Loss Eqns: 5.103e+00, Loss Aux: 1.314e-01, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 215, It: 0, Loss Data: 9.774e-01, Loss Eqns: 4.804e+00, Loss Aux: 7.854e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 216, It: 0, Loss Data: 9.459e-01, Loss Eqns: 4.104e+00, Loss Aux: 1.266e-01, Time: 0.068, Learning Rate: 1.0e-03\n",
      "Epoch: 217, It: 0, Loss Data: 9.463e-01, Loss Eqns: 4.768e+00, Loss Aux: 2.901e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 218, It: 0, Loss Data: 9.808e-01, Loss Eqns: 3.985e+00, Loss Aux: 2.557e-01, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 219, It: 0, Loss Data: 8.863e-01, Loss Eqns: 5.384e+00, Loss Aux: 1.272e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 220, It: 0, Loss Data: 9.631e-01, Loss Eqns: 4.897e+00, Loss Aux: 1.319e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 221, It: 0, Loss Data: 9.249e-01, Loss Eqns: 4.586e+00, Loss Aux: 2.363e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 222, It: 0, Loss Data: 8.889e-01, Loss Eqns: 4.861e+00, Loss Aux: 3.895e-01, Time: 0.067, Learning Rate: 1.0e-03\n",
      "Epoch: 223, It: 0, Loss Data: 9.976e-01, Loss Eqns: 4.880e+00, Loss Aux: 3.159e-01, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 224, It: 0, Loss Data: 9.032e-01, Loss Eqns: 4.184e+00, Loss Aux: 1.369e-01, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 225, It: 0, Loss Data: 9.033e-01, Loss Eqns: 5.330e+00, Loss Aux: 1.156e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 226, It: 0, Loss Data: 9.064e-01, Loss Eqns: 4.109e+00, Loss Aux: 1.599e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 227, It: 0, Loss Data: 9.686e-01, Loss Eqns: 3.778e+00, Loss Aux: 2.377e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 228, It: 0, Loss Data: 9.735e-01, Loss Eqns: 4.070e+00, Loss Aux: 2.863e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 229, It: 0, Loss Data: 9.367e-01, Loss Eqns: 4.254e+00, Loss Aux: 1.569e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 230, It: 0, Loss Data: 1.052e+00, Loss Eqns: 4.857e+00, Loss Aux: 5.924e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 231, It: 0, Loss Data: 9.532e-01, Loss Eqns: 5.077e+00, Loss Aux: 8.642e-02, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 232, It: 0, Loss Data: 9.332e-01, Loss Eqns: 4.329e+00, Loss Aux: 2.514e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 233, It: 0, Loss Data: 9.124e-01, Loss Eqns: 4.252e+00, Loss Aux: 3.300e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 234, It: 0, Loss Data: 8.816e-01, Loss Eqns: 4.367e+00, Loss Aux: 2.039e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 235, It: 0, Loss Data: 9.468e-01, Loss Eqns: 4.164e+00, Loss Aux: 1.179e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 236, It: 0, Loss Data: 9.294e-01, Loss Eqns: 4.922e+00, Loss Aux: 1.731e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 237, It: 0, Loss Data: 9.108e-01, Loss Eqns: 3.982e+00, Loss Aux: 2.816e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 238, It: 0, Loss Data: 9.369e-01, Loss Eqns: 3.857e+00, Loss Aux: 2.622e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 239, It: 0, Loss Data: 9.761e-01, Loss Eqns: 4.239e+00, Loss Aux: 1.222e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 240, It: 0, Loss Data: 9.644e-01, Loss Eqns: 4.227e+00, Loss Aux: 6.069e-02, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 241, It: 0, Loss Data: 8.762e-01, Loss Eqns: 4.611e+00, Loss Aux: 1.322e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 242, It: 0, Loss Data: 9.279e-01, Loss Eqns: 5.177e+00, Loss Aux: 2.173e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 243, It: 0, Loss Data: 8.543e-01, Loss Eqns: 4.156e+00, Loss Aux: 1.286e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 244, It: 0, Loss Data: 9.238e-01, Loss Eqns: 4.641e+00, Loss Aux: 7.245e-02, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 245, It: 0, Loss Data: 8.830e-01, Loss Eqns: 4.734e+00, Loss Aux: 1.227e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 246, It: 0, Loss Data: 8.339e-01, Loss Eqns: 4.299e+00, Loss Aux: 2.774e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 247, It: 0, Loss Data: 9.444e-01, Loss Eqns: 4.634e+00, Loss Aux: 2.726e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 248, It: 0, Loss Data: 1.010e+00, Loss Eqns: 4.094e+00, Loss Aux: 1.243e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 249, It: 0, Loss Data: 9.498e-01, Loss Eqns: 5.042e+00, Loss Aux: 1.114e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 250, It: 0, Loss Data: 8.542e-01, Loss Eqns: 4.644e+00, Loss Aux: 1.721e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 251, It: 0, Loss Data: 9.268e-01, Loss Eqns: 4.866e+00, Loss Aux: 2.170e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 252, It: 0, Loss Data: 9.072e-01, Loss Eqns: 4.491e+00, Loss Aux: 1.427e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 253, It: 0, Loss Data: 9.662e-01, Loss Eqns: 4.194e+00, Loss Aux: 9.460e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 254, It: 0, Loss Data: 8.851e-01, Loss Eqns: 5.173e+00, Loss Aux: 1.612e-01, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 255, It: 0, Loss Data: 9.712e-01, Loss Eqns: 4.222e+00, Loss Aux: 1.551e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 256, It: 0, Loss Data: 9.899e-01, Loss Eqns: 4.075e+00, Loss Aux: 1.475e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 257, It: 0, Loss Data: 9.019e-01, Loss Eqns: 4.494e+00, Loss Aux: 1.692e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 258, It: 0, Loss Data: 8.224e-01, Loss Eqns: 4.221e+00, Loss Aux: 1.275e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 259, It: 0, Loss Data: 8.553e-01, Loss Eqns: 3.644e+00, Loss Aux: 1.554e-01, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 260, It: 0, Loss Data: 9.312e-01, Loss Eqns: 4.431e+00, Loss Aux: 1.359e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 261, It: 0, Loss Data: 8.695e-01, Loss Eqns: 4.482e+00, Loss Aux: 1.395e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 262, It: 0, Loss Data: 8.715e-01, Loss Eqns: 3.977e+00, Loss Aux: 1.495e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 263, It: 0, Loss Data: 8.978e-01, Loss Eqns: 4.625e+00, Loss Aux: 8.198e-02, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 264, It: 0, Loss Data: 8.787e-01, Loss Eqns: 4.165e+00, Loss Aux: 9.900e-02, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 265, It: 0, Loss Data: 8.828e-01, Loss Eqns: 4.056e+00, Loss Aux: 1.054e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 266, It: 0, Loss Data: 9.143e-01, Loss Eqns: 4.075e+00, Loss Aux: 1.822e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 267, It: 0, Loss Data: 9.260e-01, Loss Eqns: 4.576e+00, Loss Aux: 2.453e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 268, It: 0, Loss Data: 8.876e-01, Loss Eqns: 3.734e+00, Loss Aux: 1.272e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 269, It: 0, Loss Data: 8.941e-01, Loss Eqns: 4.107e+00, Loss Aux: 1.038e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 270, It: 0, Loss Data: 9.544e-01, Loss Eqns: 4.328e+00, Loss Aux: 1.308e-01, Time: 0.092, Learning Rate: 1.0e-03\n",
      "Epoch: 271, It: 0, Loss Data: 8.790e-01, Loss Eqns: 4.190e+00, Loss Aux: 1.026e-01, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 272, It: 0, Loss Data: 8.723e-01, Loss Eqns: 3.720e+00, Loss Aux: 2.070e-01, Time: 0.077, Learning Rate: 1.0e-03\n",
      "Epoch: 273, It: 0, Loss Data: 9.150e-01, Loss Eqns: 3.744e+00, Loss Aux: 2.375e-01, Time: 0.075, Learning Rate: 1.0e-03\n",
      "Epoch: 274, It: 0, Loss Data: 9.488e-01, Loss Eqns: 4.392e+00, Loss Aux: 1.128e-01, Time: 0.064, Learning Rate: 1.0e-03\n",
      "Epoch: 275, It: 0, Loss Data: 9.144e-01, Loss Eqns: 4.115e+00, Loss Aux: 9.301e-02, Time: 0.069, Learning Rate: 1.0e-03\n",
      "Epoch: 276, It: 0, Loss Data: 9.830e-01, Loss Eqns: 4.188e+00, Loss Aux: 2.467e-01, Time: 0.077, Learning Rate: 1.0e-03\n",
      "Epoch: 277, It: 0, Loss Data: 9.268e-01, Loss Eqns: 3.997e+00, Loss Aux: 2.292e-01, Time: 0.086, Learning Rate: 1.0e-03\n",
      "Epoch: 278, It: 0, Loss Data: 9.071e-01, Loss Eqns: 5.640e+00, Loss Aux: 1.254e-01, Time: 0.095, Learning Rate: 1.0e-03\n",
      "Epoch: 279, It: 0, Loss Data: 8.982e-01, Loss Eqns: 4.963e+00, Loss Aux: 1.144e-01, Time: 0.083, Learning Rate: 1.0e-03\n",
      "Epoch: 280, It: 0, Loss Data: 9.016e-01, Loss Eqns: 4.201e+00, Loss Aux: 2.263e-01, Time: 0.092, Learning Rate: 1.0e-03\n",
      "Epoch: 281, It: 0, Loss Data: 9.707e-01, Loss Eqns: 3.773e+00, Loss Aux: 1.538e-01, Time: 0.085, Learning Rate: 1.0e-03\n",
      "Epoch: 282, It: 0, Loss Data: 8.705e-01, Loss Eqns: 4.429e+00, Loss Aux: 6.002e-02, Time: 0.088, Learning Rate: 1.0e-03\n",
      "Epoch: 283, It: 0, Loss Data: 9.405e-01, Loss Eqns: 4.450e+00, Loss Aux: 4.336e-02, Time: 0.088, Learning Rate: 1.0e-03\n",
      "Epoch: 284, It: 0, Loss Data: 9.083e-01, Loss Eqns: 4.420e+00, Loss Aux: 1.186e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 285, It: 0, Loss Data: 8.433e-01, Loss Eqns: 4.515e+00, Loss Aux: 1.815e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 286, It: 0, Loss Data: 9.044e-01, Loss Eqns: 4.249e+00, Loss Aux: 1.304e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 287, It: 0, Loss Data: 9.600e-01, Loss Eqns: 4.034e+00, Loss Aux: 1.181e-01, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 288, It: 0, Loss Data: 8.763e-01, Loss Eqns: 3.891e+00, Loss Aux: 1.559e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 289, It: 0, Loss Data: 9.925e-01, Loss Eqns: 4.050e+00, Loss Aux: 2.639e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 290, It: 0, Loss Data: 1.015e+00, Loss Eqns: 4.309e+00, Loss Aux: 1.457e-01, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 291, It: 0, Loss Data: 8.857e-01, Loss Eqns: 4.712e+00, Loss Aux: 1.255e-01, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 292, It: 0, Loss Data: 8.836e-01, Loss Eqns: 3.990e+00, Loss Aux: 2.163e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 293, It: 0, Loss Data: 9.577e-01, Loss Eqns: 3.787e+00, Loss Aux: 1.048e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 294, It: 0, Loss Data: 8.871e-01, Loss Eqns: 3.778e+00, Loss Aux: 1.280e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 295, It: 0, Loss Data: 8.656e-01, Loss Eqns: 4.296e+00, Loss Aux: 2.478e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 296, It: 0, Loss Data: 8.553e-01, Loss Eqns: 3.893e+00, Loss Aux: 1.647e-01, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 297, It: 0, Loss Data: 8.049e-01, Loss Eqns: 4.224e+00, Loss Aux: 8.436e-02, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 298, It: 0, Loss Data: 9.174e-01, Loss Eqns: 4.195e+00, Loss Aux: 1.336e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 299, It: 0, Loss Data: 9.493e-01, Loss Eqns: 4.601e+00, Loss Aux: 1.455e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 300, It: 0, Loss Data: 8.782e-01, Loss Eqns: 4.017e+00, Loss Aux: 1.438e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 301, It: 0, Loss Data: 8.947e-01, Loss Eqns: 5.442e+00, Loss Aux: 1.131e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 302, It: 0, Loss Data: 9.000e-01, Loss Eqns: 4.674e+00, Loss Aux: 1.085e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 303, It: 0, Loss Data: 9.231e-01, Loss Eqns: 3.561e+00, Loss Aux: 2.086e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 304, It: 0, Loss Data: 9.542e-01, Loss Eqns: 5.508e+00, Loss Aux: 3.743e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 305, It: 0, Loss Data: 8.932e-01, Loss Eqns: 4.850e+00, Loss Aux: 7.802e-02, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 306, It: 0, Loss Data: 9.730e-01, Loss Eqns: 4.429e+00, Loss Aux: 1.199e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 307, It: 0, Loss Data: 8.856e-01, Loss Eqns: 4.238e+00, Loss Aux: 2.853e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 308, It: 0, Loss Data: 8.936e-01, Loss Eqns: 3.829e+00, Loss Aux: 3.170e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 309, It: 0, Loss Data: 9.072e-01, Loss Eqns: 4.562e+00, Loss Aux: 1.037e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 310, It: 0, Loss Data: 9.101e-01, Loss Eqns: 4.268e+00, Loss Aux: 1.042e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 311, It: 0, Loss Data: 8.903e-01, Loss Eqns: 5.235e+00, Loss Aux: 1.643e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 312, It: 0, Loss Data: 9.743e-01, Loss Eqns: 3.489e+00, Loss Aux: 1.427e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 313, It: 0, Loss Data: 8.802e-01, Loss Eqns: 4.208e+00, Loss Aux: 1.197e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 314, It: 0, Loss Data: 9.231e-01, Loss Eqns: 4.565e+00, Loss Aux: 9.474e-02, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 315, It: 0, Loss Data: 9.220e-01, Loss Eqns: 4.277e+00, Loss Aux: 7.897e-02, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 316, It: 0, Loss Data: 9.122e-01, Loss Eqns: 4.050e+00, Loss Aux: 1.241e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 317, It: 0, Loss Data: 8.984e-01, Loss Eqns: 4.549e+00, Loss Aux: 1.678e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 318, It: 0, Loss Data: 8.649e-01, Loss Eqns: 4.365e+00, Loss Aux: 1.621e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 319, It: 0, Loss Data: 8.930e-01, Loss Eqns: 4.089e+00, Loss Aux: 1.501e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 320, It: 0, Loss Data: 9.320e-01, Loss Eqns: 3.224e+00, Loss Aux: 3.206e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 321, It: 0, Loss Data: 8.531e-01, Loss Eqns: 3.852e+00, Loss Aux: 3.306e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 322, It: 0, Loss Data: 9.341e-01, Loss Eqns: 3.446e+00, Loss Aux: 1.292e-01, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 323, It: 0, Loss Data: 8.718e-01, Loss Eqns: 4.469e+00, Loss Aux: 1.889e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 324, It: 0, Loss Data: 9.551e-01, Loss Eqns: 3.601e+00, Loss Aux: 2.848e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 325, It: 0, Loss Data: 9.203e-01, Loss Eqns: 3.621e+00, Loss Aux: 2.850e-01, Time: 0.046, Learning Rate: 1.0e-03\n",
      "Epoch: 326, It: 0, Loss Data: 8.993e-01, Loss Eqns: 3.367e+00, Loss Aux: 1.111e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 327, It: 0, Loss Data: 9.425e-01, Loss Eqns: 4.016e+00, Loss Aux: 8.183e-02, Time: 0.047, Learning Rate: 1.0e-03\n",
      "Epoch: 328, It: 0, Loss Data: 8.565e-01, Loss Eqns: 3.798e+00, Loss Aux: 1.506e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 329, It: 0, Loss Data: 8.877e-01, Loss Eqns: 3.966e+00, Loss Aux: 2.109e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 330, It: 0, Loss Data: 8.670e-01, Loss Eqns: 4.330e+00, Loss Aux: 9.659e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 331, It: 0, Loss Data: 9.264e-01, Loss Eqns: 4.010e+00, Loss Aux: 8.088e-02, Time: 0.088, Learning Rate: 1.0e-03\n",
      "Epoch: 332, It: 0, Loss Data: 9.820e-01, Loss Eqns: 4.024e+00, Loss Aux: 1.201e-01, Time: 0.085, Learning Rate: 1.0e-03\n",
      "Epoch: 333, It: 0, Loss Data: 9.875e-01, Loss Eqns: 3.474e+00, Loss Aux: 2.355e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 334, It: 0, Loss Data: 8.905e-01, Loss Eqns: 4.120e+00, Loss Aux: 1.834e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 335, It: 0, Loss Data: 8.008e-01, Loss Eqns: 5.986e+00, Loss Aux: 1.406e-01, Time: 0.048, Learning Rate: 1.0e-03\n",
      "Epoch: 336, It: 0, Loss Data: 1.006e+00, Loss Eqns: 7.825e+00, Loss Aux: 4.595e-01, Time: 0.070, Learning Rate: 1.0e-03\n",
      "Epoch: 337, It: 0, Loss Data: 9.743e-01, Loss Eqns: 5.780e+00, Loss Aux: 3.729e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 338, It: 0, Loss Data: 8.932e-01, Loss Eqns: 5.297e+00, Loss Aux: 8.969e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 339, It: 0, Loss Data: 9.221e-01, Loss Eqns: 9.709e+00, Loss Aux: 7.863e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 340, It: 0, Loss Data: 9.136e-01, Loss Eqns: 5.540e+00, Loss Aux: 1.671e-01, Time: 0.082, Learning Rate: 1.0e-03\n",
      "Epoch: 341, It: 0, Loss Data: 1.034e+00, Loss Eqns: 8.415e+00, Loss Aux: 2.417e-01, Time: 0.064, Learning Rate: 1.0e-03\n",
      "Epoch: 342, It: 0, Loss Data: 8.922e-01, Loss Eqns: 4.640e+00, Loss Aux: 2.556e-01, Time: 0.087, Learning Rate: 1.0e-03\n",
      "Epoch: 343, It: 0, Loss Data: 9.075e-01, Loss Eqns: 1.158e+01, Loss Aux: 3.125e-01, Time: 0.101, Learning Rate: 1.0e-03\n",
      "Epoch: 344, It: 0, Loss Data: 9.542e-01, Loss Eqns: 5.829e+00, Loss Aux: 6.006e-02, Time: 0.091, Learning Rate: 1.0e-03\n",
      "Epoch: 345, It: 0, Loss Data: 9.578e-01, Loss Eqns: 9.185e+00, Loss Aux: 4.967e-02, Time: 0.081, Learning Rate: 1.0e-03\n",
      "Epoch: 346, It: 0, Loss Data: 9.946e-01, Loss Eqns: 8.252e+00, Loss Aux: 2.352e-01, Time: 0.082, Learning Rate: 1.0e-03\n",
      "Epoch: 347, It: 0, Loss Data: 1.048e+00, Loss Eqns: 6.589e+00, Loss Aux: 6.756e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 348, It: 0, Loss Data: 1.013e+00, Loss Eqns: 7.493e+00, Loss Aux: 6.484e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 349, It: 0, Loss Data: 9.368e-01, Loss Eqns: 6.255e+00, Loss Aux: 3.049e-01, Time: 0.086, Learning Rate: 1.0e-03\n",
      "Epoch: 350, It: 0, Loss Data: 9.266e-01, Loss Eqns: 5.957e+00, Loss Aux: 1.754e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 351, It: 0, Loss Data: 9.206e-01, Loss Eqns: 5.912e+00, Loss Aux: 1.832e-01, Time: 0.081, Learning Rate: 1.0e-03\n",
      "Epoch: 352, It: 0, Loss Data: 9.882e-01, Loss Eqns: 6.420e+00, Loss Aux: 1.458e-01, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 353, It: 0, Loss Data: 8.716e-01, Loss Eqns: 5.657e+00, Loss Aux: 2.505e-01, Time: 0.092, Learning Rate: 1.0e-03\n",
      "Epoch: 354, It: 0, Loss Data: 9.822e-01, Loss Eqns: 5.617e+00, Loss Aux: 2.215e-01, Time: 0.092, Learning Rate: 1.0e-03\n",
      "Epoch: 355, It: 0, Loss Data: 9.553e-01, Loss Eqns: 5.844e+00, Loss Aux: 8.747e-02, Time: 0.095, Learning Rate: 1.0e-03\n",
      "Epoch: 356, It: 0, Loss Data: 8.877e-01, Loss Eqns: 5.314e+00, Loss Aux: 6.287e-02, Time: 0.076, Learning Rate: 1.0e-03\n",
      "Epoch: 357, It: 0, Loss Data: 8.937e-01, Loss Eqns: 4.914e+00, Loss Aux: 9.219e-02, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 358, It: 0, Loss Data: 8.473e-01, Loss Eqns: 4.170e+00, Loss Aux: 1.752e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 359, It: 0, Loss Data: 8.983e-01, Loss Eqns: 4.145e+00, Loss Aux: 2.778e-01, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 360, It: 0, Loss Data: 9.194e-01, Loss Eqns: 4.822e+00, Loss Aux: 3.465e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 361, It: 0, Loss Data: 9.839e-01, Loss Eqns: 4.209e+00, Loss Aux: 3.434e-01, Time: 0.086, Learning Rate: 1.0e-03\n",
      "Epoch: 362, It: 0, Loss Data: 8.876e-01, Loss Eqns: 3.919e+00, Loss Aux: 2.973e-01, Time: 0.092, Learning Rate: 1.0e-03\n",
      "Epoch: 363, It: 0, Loss Data: 9.434e-01, Loss Eqns: 4.093e+00, Loss Aux: 2.513e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 364, It: 0, Loss Data: 8.773e-01, Loss Eqns: 4.082e+00, Loss Aux: 2.381e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 365, It: 0, Loss Data: 9.041e-01, Loss Eqns: 3.426e+00, Loss Aux: 2.196e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 366, It: 0, Loss Data: 9.330e-01, Loss Eqns: 3.696e+00, Loss Aux: 1.846e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 367, It: 0, Loss Data: 9.464e-01, Loss Eqns: 3.522e+00, Loss Aux: 1.197e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 368, It: 0, Loss Data: 9.065e-01, Loss Eqns: 3.669e+00, Loss Aux: 7.455e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 369, It: 0, Loss Data: 8.860e-01, Loss Eqns: 4.088e+00, Loss Aux: 6.089e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 370, It: 0, Loss Data: 8.451e-01, Loss Eqns: 4.242e+00, Loss Aux: 7.897e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 371, It: 0, Loss Data: 9.046e-01, Loss Eqns: 3.843e+00, Loss Aux: 1.153e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 372, It: 0, Loss Data: 9.135e-01, Loss Eqns: 4.486e+00, Loss Aux: 1.251e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 373, It: 0, Loss Data: 9.192e-01, Loss Eqns: 4.000e+00, Loss Aux: 1.122e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 374, It: 0, Loss Data: 8.728e-01, Loss Eqns: 4.116e+00, Loss Aux: 1.039e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 375, It: 0, Loss Data: 9.087e-01, Loss Eqns: 3.511e+00, Loss Aux: 1.527e-01, Time: 0.065, Learning Rate: 1.0e-03\n",
      "Epoch: 376, It: 0, Loss Data: 9.544e-01, Loss Eqns: 3.720e+00, Loss Aux: 1.478e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 377, It: 0, Loss Data: 9.731e-01, Loss Eqns: 4.095e+00, Loss Aux: 1.420e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 378, It: 0, Loss Data: 8.531e-01, Loss Eqns: 4.617e+00, Loss Aux: 1.334e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 379, It: 0, Loss Data: 7.996e-01, Loss Eqns: 4.023e+00, Loss Aux: 1.688e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 380, It: 0, Loss Data: 8.828e-01, Loss Eqns: 4.183e+00, Loss Aux: 2.415e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 381, It: 0, Loss Data: 8.920e-01, Loss Eqns: 3.498e+00, Loss Aux: 2.297e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 382, It: 0, Loss Data: 8.770e-01, Loss Eqns: 4.516e+00, Loss Aux: 1.968e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 383, It: 0, Loss Data: 9.155e-01, Loss Eqns: 4.857e+00, Loss Aux: 1.889e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 384, It: 0, Loss Data: 7.814e-01, Loss Eqns: 3.384e+00, Loss Aux: 1.695e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 385, It: 0, Loss Data: 8.145e-01, Loss Eqns: 4.082e+00, Loss Aux: 1.439e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 386, It: 0, Loss Data: 8.052e-01, Loss Eqns: 3.655e+00, Loss Aux: 1.461e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 387, It: 0, Loss Data: 7.913e-01, Loss Eqns: 4.016e+00, Loss Aux: 1.317e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 388, It: 0, Loss Data: 8.509e-01, Loss Eqns: 4.367e+00, Loss Aux: 1.156e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 389, It: 0, Loss Data: 8.927e-01, Loss Eqns: 4.262e+00, Loss Aux: 1.418e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 390, It: 0, Loss Data: 7.934e-01, Loss Eqns: 4.566e+00, Loss Aux: 1.721e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 391, It: 0, Loss Data: 9.127e-01, Loss Eqns: 4.269e+00, Loss Aux: 2.564e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 392, It: 0, Loss Data: 7.799e-01, Loss Eqns: 4.151e+00, Loss Aux: 1.989e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 393, It: 0, Loss Data: 7.962e-01, Loss Eqns: 4.330e+00, Loss Aux: 1.353e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 394, It: 0, Loss Data: 8.147e-01, Loss Eqns: 4.092e+00, Loss Aux: 9.471e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 395, It: 0, Loss Data: 7.972e-01, Loss Eqns: 3.521e+00, Loss Aux: 1.396e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 396, It: 0, Loss Data: 9.070e-01, Loss Eqns: 3.696e+00, Loss Aux: 1.884e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 397, It: 0, Loss Data: 8.997e-01, Loss Eqns: 3.982e+00, Loss Aux: 1.049e-01, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 398, It: 0, Loss Data: 8.144e-01, Loss Eqns: 3.699e+00, Loss Aux: 1.133e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 399, It: 0, Loss Data: 8.673e-01, Loss Eqns: 3.974e+00, Loss Aux: 1.671e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 400, It: 0, Loss Data: 8.957e-01, Loss Eqns: 3.799e+00, Loss Aux: 2.116e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 401, It: 0, Loss Data: 8.446e-01, Loss Eqns: 3.558e+00, Loss Aux: 1.968e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 402, It: 0, Loss Data: 9.045e-01, Loss Eqns: 3.497e+00, Loss Aux: 1.409e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 403, It: 0, Loss Data: 9.113e-01, Loss Eqns: 4.285e+00, Loss Aux: 1.195e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 404, It: 0, Loss Data: 7.760e-01, Loss Eqns: 4.071e+00, Loss Aux: 9.861e-02, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 405, It: 0, Loss Data: 8.465e-01, Loss Eqns: 3.864e+00, Loss Aux: 9.900e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 406, It: 0, Loss Data: 8.755e-01, Loss Eqns: 4.558e+00, Loss Aux: 1.272e-01, Time: 0.072, Learning Rate: 1.0e-03\n",
      "Epoch: 407, It: 0, Loss Data: 8.691e-01, Loss Eqns: 4.036e+00, Loss Aux: 1.425e-01, Time: 0.061, Learning Rate: 1.0e-03\n",
      "Epoch: 408, It: 0, Loss Data: 8.458e-01, Loss Eqns: 3.516e+00, Loss Aux: 1.117e-01, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 409, It: 0, Loss Data: 8.182e-01, Loss Eqns: 3.499e+00, Loss Aux: 1.142e-01, Time: 0.063, Learning Rate: 1.0e-03\n",
      "Epoch: 410, It: 0, Loss Data: 7.952e-01, Loss Eqns: 3.797e+00, Loss Aux: 2.062e-01, Time: 0.061, Learning Rate: 1.0e-03\n",
      "Epoch: 411, It: 0, Loss Data: 7.783e-01, Loss Eqns: 3.626e+00, Loss Aux: 2.073e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 412, It: 0, Loss Data: 8.345e-01, Loss Eqns: 3.738e+00, Loss Aux: 7.085e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 413, It: 0, Loss Data: 8.435e-01, Loss Eqns: 3.573e+00, Loss Aux: 7.044e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 414, It: 0, Loss Data: 7.863e-01, Loss Eqns: 3.597e+00, Loss Aux: 1.587e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 415, It: 0, Loss Data: 8.730e-01, Loss Eqns: 4.385e+00, Loss Aux: 2.622e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 416, It: 0, Loss Data: 9.587e-01, Loss Eqns: 3.941e+00, Loss Aux: 1.705e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 417, It: 0, Loss Data: 7.775e-01, Loss Eqns: 4.433e+00, Loss Aux: 1.181e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 418, It: 0, Loss Data: 7.921e-01, Loss Eqns: 3.998e+00, Loss Aux: 1.184e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 419, It: 0, Loss Data: 8.189e-01, Loss Eqns: 3.042e+00, Loss Aux: 2.006e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 420, It: 0, Loss Data: 8.485e-01, Loss Eqns: 3.212e+00, Loss Aux: 1.841e-01, Time: 0.078, Learning Rate: 1.0e-03\n",
      "Epoch: 421, It: 0, Loss Data: 8.251e-01, Loss Eqns: 3.638e+00, Loss Aux: 1.387e-01, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 422, It: 0, Loss Data: 7.846e-01, Loss Eqns: 4.003e+00, Loss Aux: 9.159e-02, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 423, It: 0, Loss Data: 8.149e-01, Loss Eqns: 3.672e+00, Loss Aux: 1.982e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 424, It: 0, Loss Data: 7.914e-01, Loss Eqns: 4.044e+00, Loss Aux: 1.914e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 425, It: 0, Loss Data: 7.916e-01, Loss Eqns: 3.919e+00, Loss Aux: 1.001e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 426, It: 0, Loss Data: 9.034e-01, Loss Eqns: 4.097e+00, Loss Aux: 1.147e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 427, It: 0, Loss Data: 7.944e-01, Loss Eqns: 3.445e+00, Loss Aux: 2.303e-01, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 428, It: 0, Loss Data: 8.040e-01, Loss Eqns: 3.762e+00, Loss Aux: 1.339e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 429, It: 0, Loss Data: 8.482e-01, Loss Eqns: 4.181e+00, Loss Aux: 5.860e-02, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 430, It: 0, Loss Data: 7.813e-01, Loss Eqns: 4.166e+00, Loss Aux: 7.732e-02, Time: 0.066, Learning Rate: 1.0e-03\n",
      "Epoch: 431, It: 0, Loss Data: 8.750e-01, Loss Eqns: 4.120e+00, Loss Aux: 2.084e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 432, It: 0, Loss Data: 7.878e-01, Loss Eqns: 3.588e+00, Loss Aux: 2.641e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 433, It: 0, Loss Data: 8.108e-01, Loss Eqns: 3.729e+00, Loss Aux: 1.623e-01, Time: 0.063, Learning Rate: 1.0e-03\n",
      "Epoch: 434, It: 0, Loss Data: 7.876e-01, Loss Eqns: 3.981e+00, Loss Aux: 1.321e-01, Time: 0.097, Learning Rate: 1.0e-03\n",
      "Epoch: 435, It: 0, Loss Data: 8.369e-01, Loss Eqns: 3.641e+00, Loss Aux: 1.799e-01, Time: 0.100, Learning Rate: 1.0e-03\n",
      "Epoch: 436, It: 0, Loss Data: 8.971e-01, Loss Eqns: 4.094e+00, Loss Aux: 2.546e-01, Time: 0.091, Learning Rate: 1.0e-03\n",
      "Epoch: 437, It: 0, Loss Data: 7.799e-01, Loss Eqns: 3.667e+00, Loss Aux: 7.896e-02, Time: 0.100, Learning Rate: 1.0e-03\n",
      "Epoch: 438, It: 0, Loss Data: 7.539e-01, Loss Eqns: 4.087e+00, Loss Aux: 4.274e-02, Time: 0.091, Learning Rate: 1.0e-03\n",
      "Epoch: 439, It: 0, Loss Data: 8.283e-01, Loss Eqns: 3.737e+00, Loss Aux: 8.703e-02, Time: 0.093, Learning Rate: 1.0e-03\n",
      "Epoch: 440, It: 0, Loss Data: 7.042e-01, Loss Eqns: 3.895e+00, Loss Aux: 1.719e-01, Time: 0.099, Learning Rate: 1.0e-03\n",
      "Epoch: 441, It: 0, Loss Data: 8.026e-01, Loss Eqns: 3.788e+00, Loss Aux: 1.214e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 442, It: 0, Loss Data: 8.096e-01, Loss Eqns: 3.735e+00, Loss Aux: 1.037e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 443, It: 0, Loss Data: 7.250e-01, Loss Eqns: 3.883e+00, Loss Aux: 1.154e-01, Time: 0.084, Learning Rate: 1.0e-03\n",
      "Epoch: 444, It: 0, Loss Data: 8.338e-01, Loss Eqns: 3.790e+00, Loss Aux: 9.476e-02, Time: 0.096, Learning Rate: 1.0e-03\n",
      "Epoch: 445, It: 0, Loss Data: 7.504e-01, Loss Eqns: 4.430e+00, Loss Aux: 1.623e-01, Time: 0.089, Learning Rate: 1.0e-03\n",
      "Epoch: 446, It: 0, Loss Data: 7.364e-01, Loss Eqns: 3.313e+00, Loss Aux: 1.438e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 447, It: 0, Loss Data: 7.451e-01, Loss Eqns: 3.998e+00, Loss Aux: 1.426e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 448, It: 0, Loss Data: 7.611e-01, Loss Eqns: 4.665e+00, Loss Aux: 1.750e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 449, It: 0, Loss Data: 8.155e-01, Loss Eqns: 3.799e+00, Loss Aux: 9.060e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 450, It: 0, Loss Data: 7.602e-01, Loss Eqns: 4.406e+00, Loss Aux: 7.381e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 451, It: 0, Loss Data: 7.471e-01, Loss Eqns: 4.146e+00, Loss Aux: 1.566e-01, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 452, It: 0, Loss Data: 7.338e-01, Loss Eqns: 4.059e+00, Loss Aux: 1.145e-01, Time: 0.078, Learning Rate: 1.0e-03\n",
      "Epoch: 453, It: 0, Loss Data: 7.889e-01, Loss Eqns: 3.688e+00, Loss Aux: 5.715e-02, Time: 0.105, Learning Rate: 1.0e-03\n",
      "Epoch: 454, It: 0, Loss Data: 6.794e-01, Loss Eqns: 3.908e+00, Loss Aux: 1.153e-01, Time: 0.094, Learning Rate: 1.0e-03\n",
      "Epoch: 455, It: 0, Loss Data: 7.635e-01, Loss Eqns: 4.034e+00, Loss Aux: 2.254e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 456, It: 0, Loss Data: 7.830e-01, Loss Eqns: 4.043e+00, Loss Aux: 9.813e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 457, It: 0, Loss Data: 7.432e-01, Loss Eqns: 4.306e+00, Loss Aux: 2.312e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 458, It: 0, Loss Data: 7.628e-01, Loss Eqns: 3.923e+00, Loss Aux: 2.976e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 459, It: 0, Loss Data: 7.041e-01, Loss Eqns: 4.251e+00, Loss Aux: 8.443e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 460, It: 0, Loss Data: 7.228e-01, Loss Eqns: 3.902e+00, Loss Aux: 7.384e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 461, It: 0, Loss Data: 8.091e-01, Loss Eqns: 3.815e+00, Loss Aux: 1.186e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 462, It: 0, Loss Data: 7.862e-01, Loss Eqns: 3.993e+00, Loss Aux: 9.408e-02, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 463, It: 0, Loss Data: 7.251e-01, Loss Eqns: 4.067e+00, Loss Aux: 1.068e-01, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 464, It: 0, Loss Data: 7.110e-01, Loss Eqns: 4.697e+00, Loss Aux: 1.288e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 465, It: 0, Loss Data: 7.779e-01, Loss Eqns: 4.452e+00, Loss Aux: 5.642e-02, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 466, It: 0, Loss Data: 7.529e-01, Loss Eqns: 4.284e+00, Loss Aux: 1.534e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 467, It: 0, Loss Data: 7.575e-01, Loss Eqns: 3.812e+00, Loss Aux: 1.205e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 468, It: 0, Loss Data: 6.653e-01, Loss Eqns: 4.042e+00, Loss Aux: 9.956e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 469, It: 0, Loss Data: 6.945e-01, Loss Eqns: 4.212e+00, Loss Aux: 1.493e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 470, It: 0, Loss Data: 7.704e-01, Loss Eqns: 4.810e+00, Loss Aux: 2.089e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 471, It: 0, Loss Data: 7.039e-01, Loss Eqns: 4.351e+00, Loss Aux: 2.680e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 472, It: 0, Loss Data: 6.737e-01, Loss Eqns: 4.168e+00, Loss Aux: 1.009e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 473, It: 0, Loss Data: 6.783e-01, Loss Eqns: 4.063e+00, Loss Aux: 1.251e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 474, It: 0, Loss Data: 6.796e-01, Loss Eqns: 4.262e+00, Loss Aux: 1.803e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 475, It: 0, Loss Data: 6.346e-01, Loss Eqns: 4.036e+00, Loss Aux: 5.817e-02, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 476, It: 0, Loss Data: 6.777e-01, Loss Eqns: 4.496e+00, Loss Aux: 7.813e-02, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 477, It: 0, Loss Data: 6.985e-01, Loss Eqns: 5.190e+00, Loss Aux: 1.031e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 478, It: 0, Loss Data: 8.002e-01, Loss Eqns: 4.621e+00, Loss Aux: 2.079e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 479, It: 0, Loss Data: 8.116e-01, Loss Eqns: 4.205e+00, Loss Aux: 2.730e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 480, It: 0, Loss Data: 7.860e-01, Loss Eqns: 4.035e+00, Loss Aux: 2.345e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 481, It: 0, Loss Data: 8.010e-01, Loss Eqns: 3.825e+00, Loss Aux: 4.472e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 482, It: 0, Loss Data: 6.660e-01, Loss Eqns: 3.724e+00, Loss Aux: 1.881e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 483, It: 0, Loss Data: 6.771e-01, Loss Eqns: 3.989e+00, Loss Aux: 1.060e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 484, It: 0, Loss Data: 6.784e-01, Loss Eqns: 3.893e+00, Loss Aux: 1.924e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 485, It: 0, Loss Data: 6.898e-01, Loss Eqns: 4.926e+00, Loss Aux: 3.055e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 486, It: 0, Loss Data: 1.137e+00, Loss Eqns: 1.542e+01, Loss Aux: 3.676e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 487, It: 0, Loss Data: 1.245e+00, Loss Eqns: 1.111e+01, Loss Aux: 1.316e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 488, It: 0, Loss Data: 1.062e+00, Loss Eqns: 1.251e+01, Loss Aux: 6.999e-02, Time: 0.065, Learning Rate: 1.0e-03\n",
      "Epoch: 489, It: 0, Loss Data: 9.900e-01, Loss Eqns: 1.004e+01, Loss Aux: 2.099e-01, Time: 0.097, Learning Rate: 1.0e-03\n",
      "Epoch: 490, It: 0, Loss Data: 9.256e-01, Loss Eqns: 1.085e+01, Loss Aux: 2.083e-01, Time: 0.089, Learning Rate: 1.0e-03\n",
      "Epoch: 491, It: 0, Loss Data: 9.739e-01, Loss Eqns: 9.172e+00, Loss Aux: 2.063e-01, Time: 0.085, Learning Rate: 1.0e-03\n",
      "Epoch: 492, It: 0, Loss Data: 1.094e+00, Loss Eqns: 8.278e+00, Loss Aux: 2.002e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 493, It: 0, Loss Data: 1.024e+00, Loss Eqns: 7.375e+00, Loss Aux: 3.859e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 494, It: 0, Loss Data: 1.020e+00, Loss Eqns: 8.165e+00, Loss Aux: 4.416e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 495, It: 0, Loss Data: 9.715e-01, Loss Eqns: 8.812e+00, Loss Aux: 2.248e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 496, It: 0, Loss Data: 9.570e-01, Loss Eqns: 6.846e+00, Loss Aux: 8.723e-02, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 497, It: 0, Loss Data: 8.739e-01, Loss Eqns: 7.901e+00, Loss Aux: 6.008e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 498, It: 0, Loss Data: 9.290e-01, Loss Eqns: 6.135e+00, Loss Aux: 1.693e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 499, It: 0, Loss Data: 8.986e-01, Loss Eqns: 6.473e+00, Loss Aux: 1.783e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 500, It: 0, Loss Data: 9.601e-01, Loss Eqns: 6.373e+00, Loss Aux: 1.435e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 501, It: 0, Loss Data: 9.755e-01, Loss Eqns: 6.307e+00, Loss Aux: 1.161e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 502, It: 0, Loss Data: 1.040e+00, Loss Eqns: 6.230e+00, Loss Aux: 2.063e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 503, It: 0, Loss Data: 9.597e-01, Loss Eqns: 6.416e+00, Loss Aux: 3.882e-01, Time: 0.083, Learning Rate: 1.0e-03\n",
      "Epoch: 504, It: 0, Loss Data: 9.306e-01, Loss Eqns: 5.927e+00, Loss Aux: 3.081e-01, Time: 0.093, Learning Rate: 1.0e-03\n",
      "Epoch: 505, It: 0, Loss Data: 1.030e+00, Loss Eqns: 5.806e+00, Loss Aux: 2.393e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 506, It: 0, Loss Data: 8.530e-01, Loss Eqns: 6.072e+00, Loss Aux: 1.593e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 507, It: 0, Loss Data: 9.119e-01, Loss Eqns: 5.272e+00, Loss Aux: 2.762e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 508, It: 0, Loss Data: 8.521e-01, Loss Eqns: 6.399e+00, Loss Aux: 2.342e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 509, It: 0, Loss Data: 8.905e-01, Loss Eqns: 5.931e+00, Loss Aux: 1.562e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 510, It: 0, Loss Data: 8.965e-01, Loss Eqns: 5.578e+00, Loss Aux: 1.454e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 511, It: 0, Loss Data: 8.054e-01, Loss Eqns: 5.303e+00, Loss Aux: 1.385e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 512, It: 0, Loss Data: 8.822e-01, Loss Eqns: 5.417e+00, Loss Aux: 1.707e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 513, It: 0, Loss Data: 8.577e-01, Loss Eqns: 5.417e+00, Loss Aux: 7.663e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 514, It: 0, Loss Data: 9.809e-01, Loss Eqns: 5.978e+00, Loss Aux: 6.607e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 515, It: 0, Loss Data: 9.572e-01, Loss Eqns: 4.575e+00, Loss Aux: 1.779e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 516, It: 0, Loss Data: 1.039e+00, Loss Eqns: 4.536e+00, Loss Aux: 3.445e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 517, It: 0, Loss Data: 9.208e-01, Loss Eqns: 5.235e+00, Loss Aux: 1.104e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 518, It: 0, Loss Data: 9.549e-01, Loss Eqns: 5.462e+00, Loss Aux: 5.527e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 519, It: 0, Loss Data: 9.543e-01, Loss Eqns: 4.732e+00, Loss Aux: 8.018e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 520, It: 0, Loss Data: 8.952e-01, Loss Eqns: 4.391e+00, Loss Aux: 3.648e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 521, It: 0, Loss Data: 9.257e-01, Loss Eqns: 4.806e+00, Loss Aux: 2.823e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 522, It: 0, Loss Data: 9.541e-01, Loss Eqns: 5.248e+00, Loss Aux: 6.796e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 523, It: 0, Loss Data: 8.896e-01, Loss Eqns: 4.915e+00, Loss Aux: 5.181e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 524, It: 0, Loss Data: 9.136e-01, Loss Eqns: 5.499e+00, Loss Aux: 1.420e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 525, It: 0, Loss Data: 9.285e-01, Loss Eqns: 4.700e+00, Loss Aux: 2.581e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 526, It: 0, Loss Data: 9.757e-01, Loss Eqns: 4.582e+00, Loss Aux: 1.135e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 527, It: 0, Loss Data: 8.811e-01, Loss Eqns: 4.769e+00, Loss Aux: 1.130e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 528, It: 0, Loss Data: 8.449e-01, Loss Eqns: 4.577e+00, Loss Aux: 1.075e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 529, It: 0, Loss Data: 8.926e-01, Loss Eqns: 4.446e+00, Loss Aux: 2.298e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 530, It: 0, Loss Data: 9.276e-01, Loss Eqns: 3.824e+00, Loss Aux: 2.346e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 531, It: 0, Loss Data: 8.509e-01, Loss Eqns: 4.384e+00, Loss Aux: 1.209e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 532, It: 0, Loss Data: 7.787e-01, Loss Eqns: 5.102e+00, Loss Aux: 8.178e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 533, It: 0, Loss Data: 9.196e-01, Loss Eqns: 4.107e+00, Loss Aux: 1.104e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 534, It: 0, Loss Data: 8.956e-01, Loss Eqns: 4.526e+00, Loss Aux: 1.490e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 535, It: 0, Loss Data: 9.460e-01, Loss Eqns: 4.804e+00, Loss Aux: 1.027e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 536, It: 0, Loss Data: 8.152e-01, Loss Eqns: 5.105e+00, Loss Aux: 7.515e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 537, It: 0, Loss Data: 8.517e-01, Loss Eqns: 4.478e+00, Loss Aux: 7.824e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 538, It: 0, Loss Data: 8.727e-01, Loss Eqns: 3.843e+00, Loss Aux: 1.192e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 539, It: 0, Loss Data: 8.457e-01, Loss Eqns: 3.808e+00, Loss Aux: 1.494e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 540, It: 0, Loss Data: 8.446e-01, Loss Eqns: 4.013e+00, Loss Aux: 1.144e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 541, It: 0, Loss Data: 8.687e-01, Loss Eqns: 3.611e+00, Loss Aux: 6.970e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 542, It: 0, Loss Data: 8.216e-01, Loss Eqns: 3.508e+00, Loss Aux: 6.964e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 543, It: 0, Loss Data: 7.846e-01, Loss Eqns: 4.387e+00, Loss Aux: 1.387e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 544, It: 0, Loss Data: 8.192e-01, Loss Eqns: 3.594e+00, Loss Aux: 1.702e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 545, It: 0, Loss Data: 8.256e-01, Loss Eqns: 4.500e+00, Loss Aux: 1.180e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 546, It: 0, Loss Data: 8.985e-01, Loss Eqns: 4.237e+00, Loss Aux: 8.278e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 547, It: 0, Loss Data: 8.502e-01, Loss Eqns: 4.871e+00, Loss Aux: 7.902e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 548, It: 0, Loss Data: 8.668e-01, Loss Eqns: 4.280e+00, Loss Aux: 1.006e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 549, It: 0, Loss Data: 8.370e-01, Loss Eqns: 4.186e+00, Loss Aux: 1.172e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 550, It: 0, Loss Data: 8.784e-01, Loss Eqns: 3.814e+00, Loss Aux: 8.595e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 551, It: 0, Loss Data: 7.757e-01, Loss Eqns: 3.914e+00, Loss Aux: 8.555e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 552, It: 0, Loss Data: 7.940e-01, Loss Eqns: 4.311e+00, Loss Aux: 9.462e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 553, It: 0, Loss Data: 9.219e-01, Loss Eqns: 4.322e+00, Loss Aux: 1.186e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 554, It: 0, Loss Data: 7.194e-01, Loss Eqns: 4.295e+00, Loss Aux: 1.172e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 555, It: 0, Loss Data: 8.020e-01, Loss Eqns: 4.024e+00, Loss Aux: 8.593e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 556, It: 0, Loss Data: 8.270e-01, Loss Eqns: 4.415e+00, Loss Aux: 7.068e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 557, It: 0, Loss Data: 8.760e-01, Loss Eqns: 4.474e+00, Loss Aux: 9.133e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 558, It: 0, Loss Data: 7.578e-01, Loss Eqns: 4.636e+00, Loss Aux: 1.204e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 559, It: 0, Loss Data: 8.031e-01, Loss Eqns: 4.016e+00, Loss Aux: 9.353e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 560, It: 0, Loss Data: 6.890e-01, Loss Eqns: 3.901e+00, Loss Aux: 8.286e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 561, It: 0, Loss Data: 7.569e-01, Loss Eqns: 4.113e+00, Loss Aux: 8.021e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 562, It: 0, Loss Data: 7.324e-01, Loss Eqns: 3.950e+00, Loss Aux: 8.594e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 563, It: 0, Loss Data: 7.474e-01, Loss Eqns: 4.430e+00, Loss Aux: 9.066e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 564, It: 0, Loss Data: 8.371e-01, Loss Eqns: 4.540e+00, Loss Aux: 7.387e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 565, It: 0, Loss Data: 7.887e-01, Loss Eqns: 4.177e+00, Loss Aux: 5.777e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 566, It: 0, Loss Data: 7.698e-01, Loss Eqns: 4.546e+00, Loss Aux: 1.053e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 567, It: 0, Loss Data: 7.482e-01, Loss Eqns: 3.904e+00, Loss Aux: 1.186e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 568, It: 0, Loss Data: 7.728e-01, Loss Eqns: 4.371e+00, Loss Aux: 1.032e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 569, It: 0, Loss Data: 7.621e-01, Loss Eqns: 4.366e+00, Loss Aux: 1.257e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 570, It: 0, Loss Data: 7.634e-01, Loss Eqns: 3.817e+00, Loss Aux: 1.210e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 571, It: 0, Loss Data: 7.539e-01, Loss Eqns: 4.293e+00, Loss Aux: 8.430e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 572, It: 0, Loss Data: 7.597e-01, Loss Eqns: 4.734e+00, Loss Aux: 4.438e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 573, It: 0, Loss Data: 6.644e-01, Loss Eqns: 4.368e+00, Loss Aux: 5.489e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 574, It: 0, Loss Data: 7.665e-01, Loss Eqns: 4.312e+00, Loss Aux: 1.096e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 575, It: 0, Loss Data: 8.382e-01, Loss Eqns: 3.933e+00, Loss Aux: 9.848e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 576, It: 0, Loss Data: 7.515e-01, Loss Eqns: 3.943e+00, Loss Aux: 7.736e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 577, It: 0, Loss Data: 7.616e-01, Loss Eqns: 4.396e+00, Loss Aux: 9.167e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 578, It: 0, Loss Data: 7.627e-01, Loss Eqns: 3.876e+00, Loss Aux: 1.108e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 579, It: 0, Loss Data: 7.506e-01, Loss Eqns: 4.665e+00, Loss Aux: 9.971e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 580, It: 0, Loss Data: 7.821e-01, Loss Eqns: 3.731e+00, Loss Aux: 8.238e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 581, It: 0, Loss Data: 7.721e-01, Loss Eqns: 3.897e+00, Loss Aux: 8.177e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 582, It: 0, Loss Data: 6.833e-01, Loss Eqns: 4.382e+00, Loss Aux: 1.486e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 583, It: 0, Loss Data: 7.306e-01, Loss Eqns: 4.643e+00, Loss Aux: 1.087e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 584, It: 0, Loss Data: 7.264e-01, Loss Eqns: 5.256e+00, Loss Aux: 8.942e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 585, It: 0, Loss Data: 8.023e-01, Loss Eqns: 5.706e+00, Loss Aux: 7.723e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 586, It: 0, Loss Data: 7.910e-01, Loss Eqns: 5.884e+00, Loss Aux: 9.372e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 587, It: 0, Loss Data: 7.741e-01, Loss Eqns: 4.312e+00, Loss Aux: 6.324e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 588, It: 0, Loss Data: 7.826e-01, Loss Eqns: 4.405e+00, Loss Aux: 4.269e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 589, It: 0, Loss Data: 7.727e-01, Loss Eqns: 3.984e+00, Loss Aux: 5.575e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 590, It: 0, Loss Data: 8.512e-01, Loss Eqns: 4.532e+00, Loss Aux: 6.317e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 591, It: 0, Loss Data: 7.340e-01, Loss Eqns: 5.577e+00, Loss Aux: 1.532e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 592, It: 0, Loss Data: 7.569e-01, Loss Eqns: 4.769e+00, Loss Aux: 1.688e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 593, It: 0, Loss Data: 6.920e-01, Loss Eqns: 5.141e+00, Loss Aux: 1.043e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 594, It: 0, Loss Data: 8.128e-01, Loss Eqns: 4.726e+00, Loss Aux: 7.821e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 595, It: 0, Loss Data: 7.868e-01, Loss Eqns: 3.929e+00, Loss Aux: 1.843e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 596, It: 0, Loss Data: 7.434e-01, Loss Eqns: 3.914e+00, Loss Aux: 7.671e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 597, It: 0, Loss Data: 8.152e-01, Loss Eqns: 4.459e+00, Loss Aux: 6.669e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 598, It: 0, Loss Data: 7.871e-01, Loss Eqns: 5.055e+00, Loss Aux: 1.268e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 599, It: 0, Loss Data: 7.455e-01, Loss Eqns: 3.162e+00, Loss Aux: 1.030e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 600, It: 0, Loss Data: 7.256e-01, Loss Eqns: 4.086e+00, Loss Aux: 7.686e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 601, It: 0, Loss Data: 7.803e-01, Loss Eqns: 4.113e+00, Loss Aux: 5.985e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 602, It: 0, Loss Data: 7.167e-01, Loss Eqns: 3.216e+00, Loss Aux: 7.518e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 603, It: 0, Loss Data: 7.751e-01, Loss Eqns: 4.182e+00, Loss Aux: 9.137e-02, Time: 0.101, Learning Rate: 1.0e-03\n",
      "Epoch: 604, It: 0, Loss Data: 6.615e-01, Loss Eqns: 3.965e+00, Loss Aux: 1.306e-01, Time: 0.101, Learning Rate: 1.0e-03\n",
      "Epoch: 605, It: 0, Loss Data: 7.473e-01, Loss Eqns: 3.461e+00, Loss Aux: 1.446e-01, Time: 0.065, Learning Rate: 1.0e-03\n",
      "Epoch: 606, It: 0, Loss Data: 7.820e-01, Loss Eqns: 3.089e+00, Loss Aux: 1.549e-01, Time: 0.098, Learning Rate: 1.0e-03\n",
      "Epoch: 607, It: 0, Loss Data: 7.065e-01, Loss Eqns: 3.766e+00, Loss Aux: 1.306e-01, Time: 0.096, Learning Rate: 1.0e-03\n",
      "Epoch: 608, It: 0, Loss Data: 7.115e-01, Loss Eqns: 3.723e+00, Loss Aux: 6.784e-02, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 609, It: 0, Loss Data: 7.462e-01, Loss Eqns: 4.479e+00, Loss Aux: 4.687e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 610, It: 0, Loss Data: 7.374e-01, Loss Eqns: 3.320e+00, Loss Aux: 5.755e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 611, It: 0, Loss Data: 7.391e-01, Loss Eqns: 3.957e+00, Loss Aux: 7.195e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 612, It: 0, Loss Data: 7.027e-01, Loss Eqns: 5.073e+00, Loss Aux: 1.414e-01, Time: 0.090, Learning Rate: 1.0e-03\n",
      "Epoch: 613, It: 0, Loss Data: 7.598e-01, Loss Eqns: 4.731e+00, Loss Aux: 1.216e-01, Time: 0.081, Learning Rate: 1.0e-03\n",
      "Epoch: 614, It: 0, Loss Data: 6.674e-01, Loss Eqns: 3.998e+00, Loss Aux: 1.424e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 615, It: 0, Loss Data: 7.889e-01, Loss Eqns: 4.369e+00, Loss Aux: 1.054e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 616, It: 0, Loss Data: 6.923e-01, Loss Eqns: 4.637e+00, Loss Aux: 8.160e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 617, It: 0, Loss Data: 7.549e-01, Loss Eqns: 3.553e+00, Loss Aux: 1.112e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 618, It: 0, Loss Data: 7.575e-01, Loss Eqns: 6.072e+00, Loss Aux: 1.303e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 619, It: 0, Loss Data: 9.798e-01, Loss Eqns: 4.824e+00, Loss Aux: 1.113e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 620, It: 0, Loss Data: 7.018e-01, Loss Eqns: 5.618e+00, Loss Aux: 1.582e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 621, It: 0, Loss Data: 7.241e-01, Loss Eqns: 4.327e+00, Loss Aux: 1.108e-01, Time: 0.100, Learning Rate: 1.0e-03\n",
      "Epoch: 622, It: 0, Loss Data: 7.072e-01, Loss Eqns: 4.170e+00, Loss Aux: 9.025e-02, Time: 0.076, Learning Rate: 1.0e-03\n",
      "Epoch: 623, It: 0, Loss Data: 7.285e-01, Loss Eqns: 3.857e+00, Loss Aux: 4.985e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 624, It: 0, Loss Data: 7.136e-01, Loss Eqns: 4.680e+00, Loss Aux: 9.896e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 625, It: 0, Loss Data: 6.936e-01, Loss Eqns: 3.372e+00, Loss Aux: 1.213e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 626, It: 0, Loss Data: 7.575e-01, Loss Eqns: 4.391e+00, Loss Aux: 5.682e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 627, It: 0, Loss Data: 7.449e-01, Loss Eqns: 4.041e+00, Loss Aux: 9.711e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 628, It: 0, Loss Data: 6.809e-01, Loss Eqns: 4.931e+00, Loss Aux: 1.527e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 629, It: 0, Loss Data: 7.012e-01, Loss Eqns: 3.643e+00, Loss Aux: 1.383e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 630, It: 0, Loss Data: 6.835e-01, Loss Eqns: 3.663e+00, Loss Aux: 1.188e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 631, It: 0, Loss Data: 6.559e-01, Loss Eqns: 3.792e+00, Loss Aux: 1.158e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 632, It: 0, Loss Data: 7.184e-01, Loss Eqns: 4.264e+00, Loss Aux: 6.017e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 633, It: 0, Loss Data: 6.648e-01, Loss Eqns: 3.617e+00, Loss Aux: 7.180e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 634, It: 0, Loss Data: 7.339e-01, Loss Eqns: 3.874e+00, Loss Aux: 6.383e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 635, It: 0, Loss Data: 6.269e-01, Loss Eqns: 3.715e+00, Loss Aux: 6.486e-02, Time: 0.073, Learning Rate: 1.0e-03\n",
      "Epoch: 636, It: 0, Loss Data: 5.906e-01, Loss Eqns: 3.391e+00, Loss Aux: 7.020e-02, Time: 0.089, Learning Rate: 1.0e-03\n",
      "Epoch: 637, It: 0, Loss Data: 6.538e-01, Loss Eqns: 3.947e+00, Loss Aux: 8.161e-02, Time: 0.063, Learning Rate: 1.0e-03\n",
      "Epoch: 638, It: 0, Loss Data: 6.719e-01, Loss Eqns: 4.081e+00, Loss Aux: 9.600e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 639, It: 0, Loss Data: 6.873e-01, Loss Eqns: 4.095e+00, Loss Aux: 8.076e-02, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 640, It: 0, Loss Data: 6.499e-01, Loss Eqns: 4.452e+00, Loss Aux: 8.958e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 641, It: 0, Loss Data: 6.825e-01, Loss Eqns: 3.720e+00, Loss Aux: 1.853e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 642, It: 0, Loss Data: 6.222e-01, Loss Eqns: 3.692e+00, Loss Aux: 1.349e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 643, It: 0, Loss Data: 6.845e-01, Loss Eqns: 3.869e+00, Loss Aux: 1.086e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 644, It: 0, Loss Data: 7.033e-01, Loss Eqns: 3.366e+00, Loss Aux: 8.712e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 645, It: 0, Loss Data: 6.343e-01, Loss Eqns: 4.692e+00, Loss Aux: 1.513e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 646, It: 0, Loss Data: 6.042e-01, Loss Eqns: 3.735e+00, Loss Aux: 6.605e-02, Time: 0.069, Learning Rate: 1.0e-03\n",
      "Epoch: 647, It: 0, Loss Data: 6.919e-01, Loss Eqns: 3.930e+00, Loss Aux: 5.918e-02, Time: 0.066, Learning Rate: 1.0e-03\n",
      "Epoch: 648, It: 0, Loss Data: 6.700e-01, Loss Eqns: 4.262e+00, Loss Aux: 6.055e-02, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 649, It: 0, Loss Data: 8.423e-01, Loss Eqns: 4.718e+00, Loss Aux: 8.060e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 650, It: 0, Loss Data: 7.401e-01, Loss Eqns: 4.137e+00, Loss Aux: 1.314e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 651, It: 0, Loss Data: 7.106e-01, Loss Eqns: 3.946e+00, Loss Aux: 1.306e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 652, It: 0, Loss Data: 1.037e+00, Loss Eqns: 6.950e+00, Loss Aux: 1.052e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 653, It: 0, Loss Data: 1.012e+00, Loss Eqns: 8.334e+00, Loss Aux: 2.050e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 654, It: 0, Loss Data: 9.078e-01, Loss Eqns: 8.448e+00, Loss Aux: 2.086e-01, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 655, It: 0, Loss Data: 8.499e-01, Loss Eqns: 6.236e+00, Loss Aux: 1.074e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 656, It: 0, Loss Data: 8.424e-01, Loss Eqns: 4.364e+00, Loss Aux: 1.104e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 657, It: 0, Loss Data: 8.591e-01, Loss Eqns: 6.089e+00, Loss Aux: 1.969e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 658, It: 0, Loss Data: 8.115e-01, Loss Eqns: 4.023e+00, Loss Aux: 9.117e-02, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 659, It: 0, Loss Data: 8.522e-01, Loss Eqns: 4.892e+00, Loss Aux: 5.019e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 660, It: 0, Loss Data: 8.508e-01, Loss Eqns: 5.594e+00, Loss Aux: 4.597e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 661, It: 0, Loss Data: 9.233e-01, Loss Eqns: 6.799e+00, Loss Aux: 1.352e-01, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 662, It: 0, Loss Data: 7.962e-01, Loss Eqns: 6.842e+00, Loss Aux: 1.676e-01, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 663, It: 0, Loss Data: 7.619e-01, Loss Eqns: 4.564e+00, Loss Aux: 8.743e-02, Time: 0.066, Learning Rate: 1.0e-03\n",
      "Epoch: 664, It: 0, Loss Data: 8.512e-01, Loss Eqns: 5.251e+00, Loss Aux: 7.158e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 665, It: 0, Loss Data: 7.408e-01, Loss Eqns: 4.118e+00, Loss Aux: 1.381e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 666, It: 0, Loss Data: 8.307e-01, Loss Eqns: 5.266e+00, Loss Aux: 1.391e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 667, It: 0, Loss Data: 7.683e-01, Loss Eqns: 3.926e+00, Loss Aux: 6.559e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 668, It: 0, Loss Data: 8.214e-01, Loss Eqns: 4.611e+00, Loss Aux: 7.435e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 669, It: 0, Loss Data: 8.796e-01, Loss Eqns: 4.785e+00, Loss Aux: 7.339e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 670, It: 0, Loss Data: 8.016e-01, Loss Eqns: 3.936e+00, Loss Aux: 1.133e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 671, It: 0, Loss Data: 8.065e-01, Loss Eqns: 4.388e+00, Loss Aux: 8.981e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 672, It: 0, Loss Data: 7.353e-01, Loss Eqns: 4.512e+00, Loss Aux: 1.050e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 673, It: 0, Loss Data: 7.985e-01, Loss Eqns: 3.695e+00, Loss Aux: 1.452e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 674, It: 0, Loss Data: 7.651e-01, Loss Eqns: 3.948e+00, Loss Aux: 1.471e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 675, It: 0, Loss Data: 7.942e-01, Loss Eqns: 4.412e+00, Loss Aux: 1.476e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 676, It: 0, Loss Data: 7.574e-01, Loss Eqns: 3.847e+00, Loss Aux: 8.400e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 677, It: 0, Loss Data: 7.102e-01, Loss Eqns: 3.542e+00, Loss Aux: 1.264e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 678, It: 0, Loss Data: 7.465e-01, Loss Eqns: 4.136e+00, Loss Aux: 1.590e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 679, It: 0, Loss Data: 6.585e-01, Loss Eqns: 3.841e+00, Loss Aux: 8.071e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 680, It: 0, Loss Data: 7.437e-01, Loss Eqns: 5.123e+00, Loss Aux: 4.232e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 681, It: 0, Loss Data: 7.324e-01, Loss Eqns: 4.100e+00, Loss Aux: 9.364e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 682, It: 0, Loss Data: 7.617e-01, Loss Eqns: 3.858e+00, Loss Aux: 1.744e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 683, It: 0, Loss Data: 7.330e-01, Loss Eqns: 3.641e+00, Loss Aux: 1.422e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 684, It: 0, Loss Data: 7.012e-01, Loss Eqns: 3.338e+00, Loss Aux: 9.626e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 685, It: 0, Loss Data: 7.135e-01, Loss Eqns: 3.690e+00, Loss Aux: 9.635e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 686, It: 0, Loss Data: 7.249e-01, Loss Eqns: 4.291e+00, Loss Aux: 1.651e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 687, It: 0, Loss Data: 6.956e-01, Loss Eqns: 4.070e+00, Loss Aux: 5.749e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 688, It: 0, Loss Data: 6.457e-01, Loss Eqns: 4.144e+00, Loss Aux: 8.842e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 689, It: 0, Loss Data: 6.668e-01, Loss Eqns: 4.149e+00, Loss Aux: 1.657e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 690, It: 0, Loss Data: 6.799e-01, Loss Eqns: 3.994e+00, Loss Aux: 1.531e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 691, It: 0, Loss Data: 6.898e-01, Loss Eqns: 3.714e+00, Loss Aux: 9.540e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 692, It: 0, Loss Data: 6.487e-01, Loss Eqns: 4.803e+00, Loss Aux: 6.023e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 693, It: 0, Loss Data: 7.869e-01, Loss Eqns: 4.696e+00, Loss Aux: 5.241e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 694, It: 0, Loss Data: 6.438e-01, Loss Eqns: 4.294e+00, Loss Aux: 1.132e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 695, It: 0, Loss Data: 7.324e-01, Loss Eqns: 3.720e+00, Loss Aux: 9.410e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 696, It: 0, Loss Data: 7.241e-01, Loss Eqns: 3.763e+00, Loss Aux: 8.389e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 697, It: 0, Loss Data: 6.717e-01, Loss Eqns: 4.078e+00, Loss Aux: 7.761e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 698, It: 0, Loss Data: 7.194e-01, Loss Eqns: 3.828e+00, Loss Aux: 1.022e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 699, It: 0, Loss Data: 6.891e-01, Loss Eqns: 3.696e+00, Loss Aux: 9.381e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 700, It: 0, Loss Data: 7.093e-01, Loss Eqns: 4.229e+00, Loss Aux: 8.352e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 701, It: 0, Loss Data: 6.257e-01, Loss Eqns: 3.799e+00, Loss Aux: 8.466e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 702, It: 0, Loss Data: 6.502e-01, Loss Eqns: 3.918e+00, Loss Aux: 9.213e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 703, It: 0, Loss Data: 6.063e-01, Loss Eqns: 4.155e+00, Loss Aux: 1.004e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 704, It: 0, Loss Data: 6.083e-01, Loss Eqns: 3.486e+00, Loss Aux: 7.611e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 705, It: 0, Loss Data: 6.201e-01, Loss Eqns: 5.120e+00, Loss Aux: 9.353e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 706, It: 0, Loss Data: 6.708e-01, Loss Eqns: 3.490e+00, Loss Aux: 7.060e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 707, It: 0, Loss Data: 6.672e-01, Loss Eqns: 3.063e+00, Loss Aux: 6.172e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 708, It: 0, Loss Data: 6.821e-01, Loss Eqns: 4.191e+00, Loss Aux: 5.931e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 709, It: 0, Loss Data: 6.635e-01, Loss Eqns: 3.834e+00, Loss Aux: 6.979e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 710, It: 0, Loss Data: 6.684e-01, Loss Eqns: 3.516e+00, Loss Aux: 1.164e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 711, It: 0, Loss Data: 6.710e-01, Loss Eqns: 4.737e+00, Loss Aux: 1.464e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 712, It: 0, Loss Data: 7.058e-01, Loss Eqns: 4.867e+00, Loss Aux: 6.674e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 713, It: 0, Loss Data: 6.360e-01, Loss Eqns: 4.055e+00, Loss Aux: 1.087e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 714, It: 0, Loss Data: 6.156e-01, Loss Eqns: 4.052e+00, Loss Aux: 1.044e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 715, It: 0, Loss Data: 7.194e-01, Loss Eqns: 4.736e+00, Loss Aux: 7.354e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 716, It: 0, Loss Data: 6.499e-01, Loss Eqns: 3.221e+00, Loss Aux: 9.709e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 717, It: 0, Loss Data: 6.546e-01, Loss Eqns: 4.170e+00, Loss Aux: 9.975e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 718, It: 0, Loss Data: 6.603e-01, Loss Eqns: 3.736e+00, Loss Aux: 8.377e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 719, It: 0, Loss Data: 5.997e-01, Loss Eqns: 3.854e+00, Loss Aux: 1.110e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 720, It: 0, Loss Data: 5.766e-01, Loss Eqns: 3.906e+00, Loss Aux: 8.430e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 721, It: 0, Loss Data: 6.170e-01, Loss Eqns: 4.661e+00, Loss Aux: 4.966e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 722, It: 0, Loss Data: 5.286e-01, Loss Eqns: 3.973e+00, Loss Aux: 8.233e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 723, It: 0, Loss Data: 5.560e-01, Loss Eqns: 3.950e+00, Loss Aux: 1.145e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 724, It: 0, Loss Data: 5.537e-01, Loss Eqns: 3.648e+00, Loss Aux: 1.365e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 725, It: 0, Loss Data: 6.119e-01, Loss Eqns: 4.000e+00, Loss Aux: 9.066e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 726, It: 0, Loss Data: 5.977e-01, Loss Eqns: 3.631e+00, Loss Aux: 9.681e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 727, It: 0, Loss Data: 5.416e-01, Loss Eqns: 3.741e+00, Loss Aux: 1.050e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 728, It: 0, Loss Data: 5.627e-01, Loss Eqns: 3.783e+00, Loss Aux: 6.077e-02, Time: 0.061, Learning Rate: 1.0e-03\n",
      "Epoch: 729, It: 0, Loss Data: 5.408e-01, Loss Eqns: 3.859e+00, Loss Aux: 7.539e-02, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 730, It: 0, Loss Data: 5.905e-01, Loss Eqns: 3.767e+00, Loss Aux: 1.294e-01, Time: 0.066, Learning Rate: 1.0e-03\n",
      "Epoch: 731, It: 0, Loss Data: 5.534e-01, Loss Eqns: 4.049e+00, Loss Aux: 1.019e-01, Time: 0.081, Learning Rate: 1.0e-03\n",
      "Epoch: 732, It: 0, Loss Data: 5.839e-01, Loss Eqns: 4.530e+00, Loss Aux: 6.212e-02, Time: 0.070, Learning Rate: 1.0e-03\n",
      "Epoch: 733, It: 0, Loss Data: 5.839e-01, Loss Eqns: 4.333e+00, Loss Aux: 3.717e-02, Time: 0.095, Learning Rate: 1.0e-03\n",
      "Epoch: 734, It: 0, Loss Data: 5.994e-01, Loss Eqns: 4.160e+00, Loss Aux: 9.285e-02, Time: 0.104, Learning Rate: 1.0e-03\n",
      "Epoch: 735, It: 0, Loss Data: 6.264e-01, Loss Eqns: 3.959e+00, Loss Aux: 1.341e-01, Time: 0.104, Learning Rate: 1.0e-03\n",
      "Epoch: 736, It: 0, Loss Data: 7.280e-01, Loss Eqns: 4.434e+00, Loss Aux: 1.031e-01, Time: 0.099, Learning Rate: 1.0e-03\n",
      "Epoch: 737, It: 0, Loss Data: 6.151e-01, Loss Eqns: 6.077e+00, Loss Aux: 1.035e-01, Time: 0.099, Learning Rate: 1.0e-03\n",
      "Epoch: 738, It: 0, Loss Data: 6.476e-01, Loss Eqns: 4.486e+00, Loss Aux: 6.527e-02, Time: 0.090, Learning Rate: 1.0e-03\n",
      "Epoch: 739, It: 0, Loss Data: 6.643e-01, Loss Eqns: 4.198e+00, Loss Aux: 1.069e-01, Time: 0.099, Learning Rate: 1.0e-03\n",
      "Epoch: 740, It: 0, Loss Data: 6.001e-01, Loss Eqns: 4.388e+00, Loss Aux: 9.959e-02, Time: 0.096, Learning Rate: 1.0e-03\n",
      "Epoch: 741, It: 0, Loss Data: 6.232e-01, Loss Eqns: 4.776e+00, Loss Aux: 1.286e-01, Time: 0.104, Learning Rate: 1.0e-03\n",
      "Epoch: 742, It: 0, Loss Data: 4.896e-01, Loss Eqns: 4.312e+00, Loss Aux: 1.425e-01, Time: 0.099, Learning Rate: 1.0e-03\n",
      "Epoch: 743, It: 0, Loss Data: 5.954e-01, Loss Eqns: 4.564e+00, Loss Aux: 2.118e-01, Time: 0.091, Learning Rate: 1.0e-03\n",
      "Epoch: 744, It: 0, Loss Data: 8.743e-01, Loss Eqns: 6.068e+00, Loss Aux: 6.937e-02, Time: 0.099, Learning Rate: 1.0e-03\n",
      "Epoch: 745, It: 0, Loss Data: 5.996e-01, Loss Eqns: 5.295e+00, Loss Aux: 8.591e-02, Time: 0.099, Learning Rate: 1.0e-03\n",
      "Epoch: 746, It: 0, Loss Data: 5.748e-01, Loss Eqns: 4.046e+00, Loss Aux: 1.566e-01, Time: 0.101, Learning Rate: 1.0e-03\n",
      "Epoch: 747, It: 0, Loss Data: 7.170e-01, Loss Eqns: 6.584e+00, Loss Aux: 8.971e-02, Time: 0.098, Learning Rate: 1.0e-03\n",
      "Epoch: 748, It: 0, Loss Data: 6.801e-01, Loss Eqns: 4.910e+00, Loss Aux: 1.442e-01, Time: 0.092, Learning Rate: 1.0e-03\n",
      "Epoch: 749, It: 0, Loss Data: 5.919e-01, Loss Eqns: 4.558e+00, Loss Aux: 2.323e-01, Time: 0.093, Learning Rate: 1.0e-03\n",
      "Epoch: 750, It: 0, Loss Data: 6.968e-01, Loss Eqns: 6.758e+00, Loss Aux: 1.093e-01, Time: 0.096, Learning Rate: 1.0e-03\n",
      "Epoch: 751, It: 0, Loss Data: 7.574e-01, Loss Eqns: 5.681e+00, Loss Aux: 7.791e-02, Time: 0.073, Learning Rate: 1.0e-03\n",
      "Epoch: 752, It: 0, Loss Data: 7.706e-01, Loss Eqns: 5.607e+00, Loss Aux: 7.534e-02, Time: 0.077, Learning Rate: 1.0e-03\n",
      "Epoch: 753, It: 0, Loss Data: 6.662e-01, Loss Eqns: 4.406e+00, Loss Aux: 3.789e-01, Time: 0.068, Learning Rate: 1.0e-03\n",
      "Epoch: 754, It: 0, Loss Data: 7.618e-01, Loss Eqns: 6.146e+00, Loss Aux: 2.952e-01, Time: 0.068, Learning Rate: 1.0e-03\n",
      "Epoch: 755, It: 0, Loss Data: 7.063e-01, Loss Eqns: 4.394e+00, Loss Aux: 1.177e-01, Time: 0.067, Learning Rate: 1.0e-03\n",
      "Epoch: 756, It: 0, Loss Data: 7.044e-01, Loss Eqns: 5.036e+00, Loss Aux: 1.357e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 757, It: 0, Loss Data: 6.726e-01, Loss Eqns: 4.225e+00, Loss Aux: 1.913e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 758, It: 0, Loss Data: 6.310e-01, Loss Eqns: 3.773e+00, Loss Aux: 3.330e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 759, It: 0, Loss Data: 6.443e-01, Loss Eqns: 4.322e+00, Loss Aux: 2.218e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 760, It: 0, Loss Data: 6.147e-01, Loss Eqns: 4.537e+00, Loss Aux: 7.108e-02, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 761, It: 0, Loss Data: 5.924e-01, Loss Eqns: 4.590e+00, Loss Aux: 8.830e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 762, It: 0, Loss Data: 5.725e-01, Loss Eqns: 4.338e+00, Loss Aux: 1.507e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 763, It: 0, Loss Data: 5.903e-01, Loss Eqns: 4.765e+00, Loss Aux: 1.683e-01, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 764, It: 0, Loss Data: 6.091e-01, Loss Eqns: 4.009e+00, Loss Aux: 8.792e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 765, It: 0, Loss Data: 5.903e-01, Loss Eqns: 3.852e+00, Loss Aux: 8.009e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 766, It: 0, Loss Data: 5.994e-01, Loss Eqns: 4.639e+00, Loss Aux: 1.103e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 767, It: 0, Loss Data: 5.593e-01, Loss Eqns: 4.150e+00, Loss Aux: 9.283e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 768, It: 0, Loss Data: 5.962e-01, Loss Eqns: 3.681e+00, Loss Aux: 9.248e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 769, It: 0, Loss Data: 5.132e-01, Loss Eqns: 4.293e+00, Loss Aux: 2.302e-01, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 770, It: 0, Loss Data: 5.348e-01, Loss Eqns: 3.793e+00, Loss Aux: 2.615e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 771, It: 0, Loss Data: 5.486e-01, Loss Eqns: 4.129e+00, Loss Aux: 9.820e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 772, It: 0, Loss Data: 5.482e-01, Loss Eqns: 3.867e+00, Loss Aux: 5.497e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 773, It: 0, Loss Data: 4.910e-01, Loss Eqns: 4.720e+00, Loss Aux: 8.813e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 774, It: 0, Loss Data: 6.164e-01, Loss Eqns: 5.264e+00, Loss Aux: 9.965e-02, Time: 0.074, Learning Rate: 1.0e-03\n",
      "Epoch: 775, It: 0, Loss Data: 9.665e-01, Loss Eqns: 6.454e+00, Loss Aux: 1.759e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 776, It: 0, Loss Data: 7.799e-01, Loss Eqns: 6.942e+00, Loss Aux: 1.020e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 777, It: 0, Loss Data: 8.217e-01, Loss Eqns: 7.744e+00, Loss Aux: 3.280e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 778, It: 0, Loss Data: 7.924e-01, Loss Eqns: 5.994e+00, Loss Aux: 2.383e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 779, It: 0, Loss Data: 5.855e-01, Loss Eqns: 4.412e+00, Loss Aux: 5.250e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 780, It: 0, Loss Data: 7.862e-01, Loss Eqns: 5.727e+00, Loss Aux: 6.488e-02, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 781, It: 0, Loss Data: 7.229e-01, Loss Eqns: 5.497e+00, Loss Aux: 8.009e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 782, It: 0, Loss Data: 7.641e-01, Loss Eqns: 5.657e+00, Loss Aux: 1.697e-01, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 783, It: 0, Loss Data: 6.166e-01, Loss Eqns: 6.748e+00, Loss Aux: 2.927e-01, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 784, It: 0, Loss Data: 5.714e-01, Loss Eqns: 8.020e+00, Loss Aux: 2.714e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 785, It: 0, Loss Data: 5.865e-01, Loss Eqns: 6.481e+00, Loss Aux: 9.521e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 786, It: 0, Loss Data: 5.775e-01, Loss Eqns: 5.631e+00, Loss Aux: 7.626e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 787, It: 0, Loss Data: 5.309e-01, Loss Eqns: 6.419e+00, Loss Aux: 8.029e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 788, It: 0, Loss Data: 5.140e-01, Loss Eqns: 6.547e+00, Loss Aux: 7.090e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 789, It: 0, Loss Data: 5.098e-01, Loss Eqns: 5.268e+00, Loss Aux: 7.889e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 790, It: 0, Loss Data: 4.638e-01, Loss Eqns: 4.719e+00, Loss Aux: 1.652e-01, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 791, It: 0, Loss Data: 4.544e-01, Loss Eqns: 6.995e+00, Loss Aux: 2.385e-01, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 792, It: 0, Loss Data: 4.396e-01, Loss Eqns: 5.285e+00, Loss Aux: 2.015e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 793, It: 0, Loss Data: 4.701e-01, Loss Eqns: 4.381e+00, Loss Aux: 1.048e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 794, It: 0, Loss Data: 4.160e-01, Loss Eqns: 4.636e+00, Loss Aux: 6.554e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 795, It: 0, Loss Data: 4.361e-01, Loss Eqns: 5.870e+00, Loss Aux: 4.468e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 796, It: 0, Loss Data: 3.915e-01, Loss Eqns: 4.845e+00, Loss Aux: 7.538e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 797, It: 0, Loss Data: 3.997e-01, Loss Eqns: 4.784e+00, Loss Aux: 1.819e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 798, It: 0, Loss Data: 4.147e-01, Loss Eqns: 4.858e+00, Loss Aux: 2.300e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 799, It: 0, Loss Data: 4.076e-01, Loss Eqns: 5.626e+00, Loss Aux: 2.170e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 800, It: 0, Loss Data: 6.575e-01, Loss Eqns: 4.410e+00, Loss Aux: 2.152e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 801, It: 0, Loss Data: 5.547e-01, Loss Eqns: 4.539e+00, Loss Aux: 7.202e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 802, It: 0, Loss Data: 5.005e-01, Loss Eqns: 4.651e+00, Loss Aux: 1.298e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 803, It: 0, Loss Data: 4.148e-01, Loss Eqns: 4.203e+00, Loss Aux: 1.260e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 804, It: 0, Loss Data: 5.014e-01, Loss Eqns: 4.488e+00, Loss Aux: 4.839e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 805, It: 0, Loss Data: 4.214e-01, Loss Eqns: 5.090e+00, Loss Aux: 5.348e-02, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 806, It: 0, Loss Data: 3.740e-01, Loss Eqns: 4.497e+00, Loss Aux: 1.198e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 807, It: 0, Loss Data: 4.588e-01, Loss Eqns: 4.233e+00, Loss Aux: 2.181e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 808, It: 0, Loss Data: 3.876e-01, Loss Eqns: 4.117e+00, Loss Aux: 2.265e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 809, It: 0, Loss Data: 6.012e-01, Loss Eqns: 5.668e+00, Loss Aux: 1.519e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 810, It: 0, Loss Data: 3.921e-01, Loss Eqns: 3.932e+00, Loss Aux: 1.410e-01, Time: 0.084, Learning Rate: 1.0e-03\n",
      "Epoch: 811, It: 0, Loss Data: 4.866e-01, Loss Eqns: 5.627e+00, Loss Aux: 1.457e-01, Time: 0.096, Learning Rate: 1.0e-03\n",
      "Epoch: 812, It: 0, Loss Data: 5.163e-01, Loss Eqns: 4.209e+00, Loss Aux: 5.448e-02, Time: 0.094, Learning Rate: 1.0e-03\n",
      "Epoch: 813, It: 0, Loss Data: 6.245e-01, Loss Eqns: 4.579e+00, Loss Aux: 6.360e-02, Time: 0.082, Learning Rate: 1.0e-03\n",
      "Epoch: 814, It: 0, Loss Data: 3.998e-01, Loss Eqns: 4.104e+00, Loss Aux: 1.153e-01, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 815, It: 0, Loss Data: 4.643e-01, Loss Eqns: 4.632e+00, Loss Aux: 8.348e-02, Time: 0.071, Learning Rate: 1.0e-03\n",
      "Epoch: 816, It: 0, Loss Data: 5.017e-01, Loss Eqns: 5.144e+00, Loss Aux: 1.694e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 817, It: 0, Loss Data: 4.921e-01, Loss Eqns: 4.561e+00, Loss Aux: 1.020e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 818, It: 0, Loss Data: 6.051e-01, Loss Eqns: 4.702e+00, Loss Aux: 2.623e-01, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 819, It: 0, Loss Data: 4.089e-01, Loss Eqns: 4.510e+00, Loss Aux: 2.496e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 820, It: 0, Loss Data: 4.941e-01, Loss Eqns: 4.205e+00, Loss Aux: 9.432e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 821, It: 0, Loss Data: 4.762e-01, Loss Eqns: 4.021e+00, Loss Aux: 4.937e-02, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 822, It: 0, Loss Data: 4.520e-01, Loss Eqns: 4.186e+00, Loss Aux: 1.103e-01, Time: 0.064, Learning Rate: 1.0e-03\n",
      "Epoch: 823, It: 0, Loss Data: 4.462e-01, Loss Eqns: 4.218e+00, Loss Aux: 1.998e-01, Time: 0.067, Learning Rate: 1.0e-03\n",
      "Epoch: 824, It: 0, Loss Data: 4.644e-01, Loss Eqns: 4.416e+00, Loss Aux: 1.830e-01, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 825, It: 0, Loss Data: 4.138e-01, Loss Eqns: 4.202e+00, Loss Aux: 1.072e-01, Time: 0.066, Learning Rate: 1.0e-03\n",
      "Epoch: 826, It: 0, Loss Data: 4.201e-01, Loss Eqns: 4.548e+00, Loss Aux: 8.434e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 827, It: 0, Loss Data: 4.055e-01, Loss Eqns: 4.375e+00, Loss Aux: 5.338e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 828, It: 0, Loss Data: 4.418e-01, Loss Eqns: 4.777e+00, Loss Aux: 4.557e-02, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 829, It: 0, Loss Data: 3.852e-01, Loss Eqns: 4.664e+00, Loss Aux: 1.557e-01, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 830, It: 0, Loss Data: 4.521e-01, Loss Eqns: 3.707e+00, Loss Aux: 2.559e-01, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 831, It: 0, Loss Data: 3.670e-01, Loss Eqns: 3.838e+00, Loss Aux: 1.041e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 832, It: 0, Loss Data: 4.530e-01, Loss Eqns: 4.341e+00, Loss Aux: 6.534e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 833, It: 0, Loss Data: 3.651e-01, Loss Eqns: 4.670e+00, Loss Aux: 6.437e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 834, It: 0, Loss Data: 3.761e-01, Loss Eqns: 4.367e+00, Loss Aux: 1.261e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 835, It: 0, Loss Data: 3.331e-01, Loss Eqns: 3.851e+00, Loss Aux: 1.480e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 836, It: 0, Loss Data: 4.190e-01, Loss Eqns: 4.346e+00, Loss Aux: 1.403e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 837, It: 0, Loss Data: 3.462e-01, Loss Eqns: 3.992e+00, Loss Aux: 1.358e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 838, It: 0, Loss Data: 3.148e-01, Loss Eqns: 4.336e+00, Loss Aux: 1.182e-01, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 839, It: 0, Loss Data: 3.018e-01, Loss Eqns: 4.782e+00, Loss Aux: 5.631e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 840, It: 0, Loss Data: 3.174e-01, Loss Eqns: 4.461e+00, Loss Aux: 4.451e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 841, It: 0, Loss Data: 3.023e-01, Loss Eqns: 4.849e+00, Loss Aux: 6.594e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 842, It: 0, Loss Data: 2.839e-01, Loss Eqns: 4.538e+00, Loss Aux: 8.344e-02, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 843, It: 0, Loss Data: 3.743e-01, Loss Eqns: 5.090e+00, Loss Aux: 9.306e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 844, It: 0, Loss Data: 2.806e-01, Loss Eqns: 4.456e+00, Loss Aux: 1.066e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 845, It: 0, Loss Data: 3.439e-01, Loss Eqns: 4.815e+00, Loss Aux: 7.610e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 846, It: 0, Loss Data: 3.253e-01, Loss Eqns: 4.592e+00, Loss Aux: 6.747e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 847, It: 0, Loss Data: 3.063e-01, Loss Eqns: 4.283e+00, Loss Aux: 4.428e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 848, It: 0, Loss Data: 3.189e-01, Loss Eqns: 4.732e+00, Loss Aux: 1.324e-01, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 849, It: 0, Loss Data: 2.937e-01, Loss Eqns: 4.195e+00, Loss Aux: 1.024e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 850, It: 0, Loss Data: 3.070e-01, Loss Eqns: 4.501e+00, Loss Aux: 8.627e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 851, It: 0, Loss Data: 2.490e-01, Loss Eqns: 4.932e+00, Loss Aux: 1.050e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 852, It: 0, Loss Data: 3.110e-01, Loss Eqns: 4.581e+00, Loss Aux: 8.720e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 853, It: 0, Loss Data: 2.619e-01, Loss Eqns: 4.669e+00, Loss Aux: 8.867e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 854, It: 0, Loss Data: 3.029e-01, Loss Eqns: 5.100e+00, Loss Aux: 1.046e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 855, It: 0, Loss Data: 2.596e-01, Loss Eqns: 4.187e+00, Loss Aux: 8.819e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 856, It: 0, Loss Data: 2.450e-01, Loss Eqns: 4.642e+00, Loss Aux: 6.529e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 857, It: 0, Loss Data: 2.686e-01, Loss Eqns: 4.118e+00, Loss Aux: 5.645e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 858, It: 0, Loss Data: 2.667e-01, Loss Eqns: 4.771e+00, Loss Aux: 4.306e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 859, It: 0, Loss Data: 2.689e-01, Loss Eqns: 4.420e+00, Loss Aux: 3.992e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 860, It: 0, Loss Data: 2.405e-01, Loss Eqns: 5.152e+00, Loss Aux: 8.074e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 861, It: 0, Loss Data: 2.612e-01, Loss Eqns: 4.579e+00, Loss Aux: 1.189e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 862, It: 0, Loss Data: 2.732e-01, Loss Eqns: 4.645e+00, Loss Aux: 6.553e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 863, It: 0, Loss Data: 2.336e-01, Loss Eqns: 4.685e+00, Loss Aux: 6.822e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 864, It: 0, Loss Data: 2.793e-01, Loss Eqns: 4.326e+00, Loss Aux: 7.843e-02, Time: 0.063, Learning Rate: 1.0e-03\n",
      "Epoch: 865, It: 0, Loss Data: 2.155e-01, Loss Eqns: 4.246e+00, Loss Aux: 7.147e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 866, It: 0, Loss Data: 2.809e-01, Loss Eqns: 4.238e+00, Loss Aux: 7.172e-02, Time: 0.068, Learning Rate: 1.0e-03\n",
      "Epoch: 867, It: 0, Loss Data: 2.650e-01, Loss Eqns: 4.657e+00, Loss Aux: 1.217e-01, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 868, It: 0, Loss Data: 2.219e-01, Loss Eqns: 4.505e+00, Loss Aux: 6.962e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 869, It: 0, Loss Data: 2.303e-01, Loss Eqns: 4.689e+00, Loss Aux: 6.324e-02, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 870, It: 0, Loss Data: 2.676e-01, Loss Eqns: 5.057e+00, Loss Aux: 6.955e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 871, It: 0, Loss Data: 2.911e-01, Loss Eqns: 4.989e+00, Loss Aux: 5.281e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 872, It: 0, Loss Data: 2.369e-01, Loss Eqns: 4.238e+00, Loss Aux: 4.315e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 873, It: 0, Loss Data: 2.999e-01, Loss Eqns: 4.965e+00, Loss Aux: 1.276e-01, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 874, It: 0, Loss Data: 2.585e-01, Loss Eqns: 4.783e+00, Loss Aux: 4.909e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 875, It: 0, Loss Data: 2.298e-01, Loss Eqns: 4.474e+00, Loss Aux: 5.556e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 876, It: 0, Loss Data: 2.897e-01, Loss Eqns: 5.104e+00, Loss Aux: 7.327e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 877, It: 0, Loss Data: 3.009e-01, Loss Eqns: 4.538e+00, Loss Aux: 6.738e-02, Time: 0.064, Learning Rate: 1.0e-03\n",
      "Epoch: 878, It: 0, Loss Data: 2.269e-01, Loss Eqns: 4.145e+00, Loss Aux: 9.231e-02, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 879, It: 0, Loss Data: 3.387e-01, Loss Eqns: 5.109e+00, Loss Aux: 1.364e-01, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 880, It: 0, Loss Data: 2.591e-01, Loss Eqns: 4.582e+00, Loss Aux: 9.490e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 881, It: 0, Loss Data: 2.203e-01, Loss Eqns: 4.825e+00, Loss Aux: 6.967e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 882, It: 0, Loss Data: 3.442e-01, Loss Eqns: 5.183e+00, Loss Aux: 1.131e-01, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 883, It: 0, Loss Data: 2.534e-01, Loss Eqns: 4.703e+00, Loss Aux: 5.718e-02, Time: 0.065, Learning Rate: 1.0e-03\n",
      "Epoch: 884, It: 0, Loss Data: 2.678e-01, Loss Eqns: 4.160e+00, Loss Aux: 6.451e-02, Time: 0.077, Learning Rate: 1.0e-03\n",
      "Epoch: 885, It: 0, Loss Data: 3.269e-01, Loss Eqns: 5.078e+00, Loss Aux: 1.283e-01, Time: 0.074, Learning Rate: 1.0e-03\n",
      "Epoch: 886, It: 0, Loss Data: 2.161e-01, Loss Eqns: 4.714e+00, Loss Aux: 6.597e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 887, It: 0, Loss Data: 2.306e-01, Loss Eqns: 4.508e+00, Loss Aux: 7.688e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 888, It: 0, Loss Data: 4.471e-01, Loss Eqns: 5.873e+00, Loss Aux: 8.394e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 889, It: 0, Loss Data: 3.411e-01, Loss Eqns: 5.233e+00, Loss Aux: 6.422e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 890, It: 0, Loss Data: 2.416e-01, Loss Eqns: 5.047e+00, Loss Aux: 6.503e-02, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 891, It: 0, Loss Data: 2.797e-01, Loss Eqns: 4.757e+00, Loss Aux: 1.133e-01, Time: 0.103, Learning Rate: 1.0e-03\n",
      "Epoch: 892, It: 0, Loss Data: 2.387e-01, Loss Eqns: 4.357e+00, Loss Aux: 5.223e-02, Time: 0.097, Learning Rate: 1.0e-03\n",
      "Epoch: 893, It: 0, Loss Data: 2.765e-01, Loss Eqns: 4.395e+00, Loss Aux: 4.517e-02, Time: 0.098, Learning Rate: 1.0e-03\n",
      "Epoch: 894, It: 0, Loss Data: 2.273e-01, Loss Eqns: 4.744e+00, Loss Aux: 5.865e-02, Time: 0.098, Learning Rate: 1.0e-03\n",
      "Epoch: 895, It: 0, Loss Data: 2.439e-01, Loss Eqns: 4.616e+00, Loss Aux: 6.857e-02, Time: 0.079, Learning Rate: 1.0e-03\n",
      "Epoch: 896, It: 0, Loss Data: 2.662e-01, Loss Eqns: 4.254e+00, Loss Aux: 8.036e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 897, It: 0, Loss Data: 2.656e-01, Loss Eqns: 4.916e+00, Loss Aux: 5.212e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 898, It: 0, Loss Data: 2.254e-01, Loss Eqns: 4.479e+00, Loss Aux: 6.294e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 899, It: 0, Loss Data: 2.470e-01, Loss Eqns: 4.403e+00, Loss Aux: 6.978e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 900, It: 0, Loss Data: 2.436e-01, Loss Eqns: 4.398e+00, Loss Aux: 6.542e-02, Time: 0.102, Learning Rate: 1.0e-03\n",
      "Epoch: 901, It: 0, Loss Data: 2.417e-01, Loss Eqns: 4.410e+00, Loss Aux: 4.267e-02, Time: 0.092, Learning Rate: 1.0e-03\n",
      "Epoch: 902, It: 0, Loss Data: 2.333e-01, Loss Eqns: 4.168e+00, Loss Aux: 1.016e-01, Time: 0.070, Learning Rate: 1.0e-03\n",
      "Epoch: 903, It: 0, Loss Data: 2.156e-01, Loss Eqns: 5.005e+00, Loss Aux: 7.473e-02, Time: 0.073, Learning Rate: 1.0e-03\n",
      "Epoch: 904, It: 0, Loss Data: 2.021e-01, Loss Eqns: 5.128e+00, Loss Aux: 3.827e-02, Time: 0.085, Learning Rate: 1.0e-03\n",
      "Epoch: 905, It: 0, Loss Data: 2.451e-01, Loss Eqns: 4.746e+00, Loss Aux: 4.638e-02, Time: 0.086, Learning Rate: 1.0e-03\n",
      "Epoch: 906, It: 0, Loss Data: 2.054e-01, Loss Eqns: 4.671e+00, Loss Aux: 5.379e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 907, It: 0, Loss Data: 2.544e-01, Loss Eqns: 4.638e+00, Loss Aux: 6.219e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 908, It: 0, Loss Data: 2.333e-01, Loss Eqns: 4.350e+00, Loss Aux: 7.051e-02, Time: 0.100, Learning Rate: 1.0e-03\n",
      "Epoch: 909, It: 0, Loss Data: 2.190e-01, Loss Eqns: 4.662e+00, Loss Aux: 7.061e-02, Time: 0.089, Learning Rate: 1.0e-03\n",
      "Epoch: 910, It: 0, Loss Data: 2.202e-01, Loss Eqns: 5.003e+00, Loss Aux: 7.043e-02, Time: 0.090, Learning Rate: 1.0e-03\n",
      "Epoch: 911, It: 0, Loss Data: 2.097e-01, Loss Eqns: 4.421e+00, Loss Aux: 5.081e-02, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 912, It: 0, Loss Data: 2.480e-01, Loss Eqns: 4.568e+00, Loss Aux: 4.829e-02, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 913, It: 0, Loss Data: 2.024e-01, Loss Eqns: 4.381e+00, Loss Aux: 5.844e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 914, It: 0, Loss Data: 2.082e-01, Loss Eqns: 4.855e+00, Loss Aux: 6.336e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 915, It: 0, Loss Data: 1.969e-01, Loss Eqns: 4.303e+00, Loss Aux: 7.703e-02, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 916, It: 0, Loss Data: 1.854e-01, Loss Eqns: 4.227e+00, Loss Aux: 5.744e-02, Time: 0.067, Learning Rate: 1.0e-03\n",
      "Epoch: 917, It: 0, Loss Data: 1.921e-01, Loss Eqns: 4.639e+00, Loss Aux: 6.304e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 918, It: 0, Loss Data: 1.790e-01, Loss Eqns: 4.668e+00, Loss Aux: 5.747e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 919, It: 0, Loss Data: 1.520e-01, Loss Eqns: 4.581e+00, Loss Aux: 4.113e-02, Time: 0.063, Learning Rate: 1.0e-03\n",
      "Epoch: 920, It: 0, Loss Data: 1.928e-01, Loss Eqns: 4.682e+00, Loss Aux: 3.779e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 921, It: 0, Loss Data: 1.930e-01, Loss Eqns: 4.522e+00, Loss Aux: 7.914e-02, Time: 0.061, Learning Rate: 1.0e-03\n",
      "Epoch: 922, It: 0, Loss Data: 1.788e-01, Loss Eqns: 4.547e+00, Loss Aux: 5.013e-02, Time: 0.072, Learning Rate: 1.0e-03\n",
      "Epoch: 923, It: 0, Loss Data: 2.054e-01, Loss Eqns: 4.548e+00, Loss Aux: 4.577e-02, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 924, It: 0, Loss Data: 2.006e-01, Loss Eqns: 4.336e+00, Loss Aux: 4.425e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 925, It: 0, Loss Data: 1.917e-01, Loss Eqns: 4.717e+00, Loss Aux: 6.814e-02, Time: 0.063, Learning Rate: 1.0e-03\n",
      "Epoch: 926, It: 0, Loss Data: 1.649e-01, Loss Eqns: 4.625e+00, Loss Aux: 7.416e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 927, It: 0, Loss Data: 1.819e-01, Loss Eqns: 4.381e+00, Loss Aux: 7.590e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 928, It: 0, Loss Data: 1.809e-01, Loss Eqns: 4.823e+00, Loss Aux: 6.676e-02, Time: 0.065, Learning Rate: 1.0e-03\n",
      "Epoch: 929, It: 0, Loss Data: 1.697e-01, Loss Eqns: 4.415e+00, Loss Aux: 6.834e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 930, It: 0, Loss Data: 1.900e-01, Loss Eqns: 4.481e+00, Loss Aux: 4.993e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 931, It: 0, Loss Data: 1.951e-01, Loss Eqns: 4.607e+00, Loss Aux: 6.763e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 932, It: 0, Loss Data: 2.023e-01, Loss Eqns: 4.664e+00, Loss Aux: 5.251e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 933, It: 0, Loss Data: 2.399e-01, Loss Eqns: 5.131e+00, Loss Aux: 5.533e-02, Time: 0.088, Learning Rate: 1.0e-03\n",
      "Epoch: 934, It: 0, Loss Data: 2.002e-01, Loss Eqns: 4.322e+00, Loss Aux: 5.041e-02, Time: 0.063, Learning Rate: 1.0e-03\n",
      "Epoch: 935, It: 0, Loss Data: 2.244e-01, Loss Eqns: 4.731e+00, Loss Aux: 8.647e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 936, It: 0, Loss Data: 2.248e-01, Loss Eqns: 3.995e+00, Loss Aux: 4.012e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 937, It: 0, Loss Data: 2.041e-01, Loss Eqns: 4.373e+00, Loss Aux: 5.969e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 938, It: 0, Loss Data: 2.137e-01, Loss Eqns: 4.690e+00, Loss Aux: 5.786e-02, Time: 0.074, Learning Rate: 1.0e-03\n",
      "Epoch: 939, It: 0, Loss Data: 2.366e-01, Loss Eqns: 4.487e+00, Loss Aux: 5.778e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 940, It: 0, Loss Data: 1.855e-01, Loss Eqns: 4.042e+00, Loss Aux: 4.909e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 941, It: 0, Loss Data: 2.210e-01, Loss Eqns: 4.859e+00, Loss Aux: 9.075e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 942, It: 0, Loss Data: 2.303e-01, Loss Eqns: 4.609e+00, Loss Aux: 4.661e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 943, It: 0, Loss Data: 2.321e-01, Loss Eqns: 4.550e+00, Loss Aux: 4.789e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 944, It: 0, Loss Data: 1.991e-01, Loss Eqns: 4.756e+00, Loss Aux: 5.374e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 945, It: 0, Loss Data: 2.254e-01, Loss Eqns: 4.995e+00, Loss Aux: 5.643e-02, Time: 0.049, Learning Rate: 1.0e-03\n",
      "Epoch: 946, It: 0, Loss Data: 1.887e-01, Loss Eqns: 5.115e+00, Loss Aux: 6.733e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 947, It: 0, Loss Data: 2.093e-01, Loss Eqns: 4.581e+00, Loss Aux: 5.735e-02, Time: 0.128, Learning Rate: 1.0e-03\n",
      "Epoch: 948, It: 0, Loss Data: 2.455e-01, Loss Eqns: 4.945e+00, Loss Aux: 4.503e-02, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 949, It: 0, Loss Data: 2.131e-01, Loss Eqns: 4.381e+00, Loss Aux: 6.413e-02, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 950, It: 0, Loss Data: 2.254e-01, Loss Eqns: 4.416e+00, Loss Aux: 4.476e-02, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 951, It: 0, Loss Data: 2.116e-01, Loss Eqns: 4.875e+00, Loss Aux: 3.620e-02, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 952, It: 0, Loss Data: 2.077e-01, Loss Eqns: 4.640e+00, Loss Aux: 6.350e-02, Time: 0.061, Learning Rate: 1.0e-03\n",
      "Epoch: 953, It: 0, Loss Data: 6.401e-01, Loss Eqns: 9.236e+00, Loss Aux: 7.667e-02, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 954, It: 0, Loss Data: 1.825e+00, Loss Eqns: nan, Loss Aux: 2.192e-01, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 955, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 956, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 957, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 958, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 959, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 960, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 961, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 962, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 963, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 964, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 965, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 966, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 967, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 968, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 969, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 970, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 971, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 972, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 973, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 974, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 975, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.061, Learning Rate: 1.0e-03\n",
      "Epoch: 976, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 977, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 978, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 979, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 980, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 981, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 982, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 983, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 984, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 985, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 986, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 987, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 988, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 989, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 990, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 991, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 992, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 993, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 994, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 995, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 996, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 997, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 998, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 999, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1000, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1001, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 1002, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 1003, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1004, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1005, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.070, Learning Rate: 1.0e-03\n",
      "Epoch: 1006, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1007, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1008, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1009, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1010, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1011, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 1012, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 1013, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1014, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1015, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1016, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1017, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1018, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1019, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.061, Learning Rate: 1.0e-03\n",
      "Epoch: 1020, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.061, Learning Rate: 1.0e-03\n",
      "Epoch: 1021, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1022, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.063, Learning Rate: 1.0e-03\n",
      "Epoch: 1023, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1024, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1025, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1026, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1027, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 1028, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1029, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.081, Learning Rate: 1.0e-03\n",
      "Epoch: 1030, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1031, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1032, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1033, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1034, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1035, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1036, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1037, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1038, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 1039, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1040, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 1041, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 1042, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.066, Learning Rate: 1.0e-03\n",
      "Epoch: 1043, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1044, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1045, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 1046, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.067, Learning Rate: 1.0e-03\n",
      "Epoch: 1047, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.128, Learning Rate: 1.0e-03\n",
      "Epoch: 1048, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.066, Learning Rate: 1.0e-03\n",
      "Epoch: 1049, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1050, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1051, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1052, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 1053, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.055, Learning Rate: 1.0e-03\n",
      "Epoch: 1054, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 1055, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1056, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 1057, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 1058, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1059, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1060, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1061, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1062, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1063, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1064, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1065, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 1066, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1067, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 1068, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1069, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.063, Learning Rate: 1.0e-03\n",
      "Epoch: 1070, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 1071, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1072, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1073, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1074, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1075, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 1076, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1077, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.068, Learning Rate: 1.0e-03\n",
      "Epoch: 1078, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1079, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1080, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 1081, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1082, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1083, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1084, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1085, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 1086, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1087, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1088, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1089, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1090, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1091, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1092, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1093, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 1094, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1095, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.066, Learning Rate: 1.0e-03\n",
      "Epoch: 1096, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 1097, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1098, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 1099, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 1100, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1101, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 1102, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.082, Learning Rate: 1.0e-03\n",
      "Epoch: 1103, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.068, Learning Rate: 1.0e-03\n",
      "Epoch: 1104, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1105, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 1106, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.064, Learning Rate: 1.0e-03\n",
      "Epoch: 1107, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 1108, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.064, Learning Rate: 1.0e-03\n",
      "Epoch: 1109, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.068, Learning Rate: 1.0e-03\n",
      "Epoch: 1110, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.061, Learning Rate: 1.0e-03\n",
      "Epoch: 1111, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.063, Learning Rate: 1.0e-03\n",
      "Epoch: 1112, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.051, Learning Rate: 1.0e-03\n",
      "Epoch: 1113, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1114, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1115, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.064, Learning Rate: 1.0e-03\n",
      "Epoch: 1116, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 1117, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1118, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 1119, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1120, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1121, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.077, Learning Rate: 1.0e-03\n",
      "Epoch: 1122, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1123, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.082, Learning Rate: 1.0e-03\n",
      "Epoch: 1124, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.112, Learning Rate: 1.0e-03\n",
      "Epoch: 1125, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.074, Learning Rate: 1.0e-03\n",
      "Epoch: 1126, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 1127, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 1128, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1129, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.067, Learning Rate: 1.0e-03\n",
      "Epoch: 1130, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 1131, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1132, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.071, Learning Rate: 1.0e-03\n",
      "Epoch: 1133, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1134, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 1135, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 1136, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.063, Learning Rate: 1.0e-03\n",
      "Epoch: 1137, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1138, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 1139, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1140, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1141, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 1142, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1143, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.066, Learning Rate: 1.0e-03\n",
      "Epoch: 1144, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.061, Learning Rate: 1.0e-03\n",
      "Epoch: 1145, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1146, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1147, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 1148, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1149, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.056, Learning Rate: 1.0e-03\n",
      "Epoch: 1150, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.067, Learning Rate: 1.0e-03\n",
      "Epoch: 1151, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1152, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.066, Learning Rate: 1.0e-03\n",
      "Epoch: 1153, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.059, Learning Rate: 1.0e-03\n",
      "Epoch: 1154, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1155, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1156, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.060, Learning Rate: 1.0e-03\n",
      "Epoch: 1157, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1158, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1159, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.062, Learning Rate: 1.0e-03\n",
      "Epoch: 1160, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1161, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1162, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.058, Learning Rate: 1.0e-03\n",
      "Epoch: 1163, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1164, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.050, Learning Rate: 1.0e-03\n",
      "Epoch: 1165, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.066, Learning Rate: 1.0e-03\n",
      "Epoch: 1166, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.052, Learning Rate: 1.0e-03\n",
      "Epoch: 1167, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.057, Learning Rate: 1.0e-03\n",
      "Epoch: 1168, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.068, Learning Rate: 1.0e-03\n",
      "Epoch: 1169, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n",
      "Epoch: 1170, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.054, Learning Rate: 1.0e-03\n",
      "Epoch: 1171, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.069, Learning Rate: 1.0e-03\n",
      "Epoch: 1172, It: 0, Loss Data: nan, Loss Eqns: nan, Loss Aux: nan, Time: 0.053, Learning Rate: 1.0e-03\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    start_time = time.time()\n",
    "    layers = [1] + 7*[7*40] + [7]\n",
    "    \n",
    "    # function that returns dx/dt\n",
    "    def f(x, t): # x is 7 x 1\n",
    "        J0 = 2.5\n",
    "        k1 = 100.0\n",
    "        k2 = 6.0\n",
    "        k3 = 16.0\n",
    "        k4 = 100.0\n",
    "        k5 = 1.28\n",
    "        k6 = 12.0\n",
    "        k = 1.8\n",
    "        kappa = 13.0\n",
    "        q = 4.0\n",
    "        K1 = 0.52\n",
    "        psi = 0.1\n",
    "        N = 1.0\n",
    "        A = 4.0\n",
    "        \n",
    "        f1 = J0 - (k1*x[0]*x[5])/(1+(x[5]/K1)**q)\n",
    "        f2 = 2*(k1*x[0]*x[5])/(1+(x[5]/K1)**q) - k2*x[1]*(N-x[4]) - k6*x[1]*x[4]\n",
    "        f3 = k2*x[1]*(N-x[4]) - k3*x[2]*(A-x[5])\n",
    "        f4 = k3*x[2]*(A-x[5]) - k4*x[3]*x[4] - kappa*(x[3]-x[6])\n",
    "        f5 = k2*x[1]*(N-x[4]) - k4*x[3]*x[4] - k6*x[1]*x[4]\n",
    "        f6 = -2*(k1*x[0]*x[5])/(1+(x[5]/K1)**q) + 2*k3*x[2]*(A-x[5]) - k5*x[5]\n",
    "        f7 = psi*kappa*(x[3]-x[6]) - k*x[6]\n",
    "        \n",
    "        f = np.array([f1, f2, f3, f4, f5, f6, f7])\n",
    "        return f\n",
    "\n",
    "    def addNoise(S, noise):\n",
    "        std = noise*S.std(0)\n",
    "        S[1:,:] += np.random.normal(0.0, std, (S.shape[0]-1, S.shape[1]))\n",
    "        return S\n",
    "        \n",
    "    # time points\n",
    "    t_star = np.arange(0, 10, 0.005)\n",
    "    N = t_star.shape[0]\n",
    "    N_eqns = N\n",
    "    N_data = N // 4\n",
    "    \n",
    "    S1 = np.random.uniform(0.15, 1.60, 1)\n",
    "    S2 = np.random.uniform(0.19, 2.16, 1)\n",
    "    S3 = np.random.uniform(0.04, 0.20, 1)\n",
    "    S4 = np.random.uniform(0.10, 0.35, 1)\n",
    "    S5 = np.random.uniform(0.08, 0.30, 1)\n",
    "    S6 = np.random.uniform(0.14, 2.67, 1)\n",
    "    S7 = np.random.uniform(0.05, 0.10, 1)\n",
    "    \n",
    "    # initial condition\n",
    "#    x0 = np.array([S1, S2, S3, S4, S5, S6, S7]).flatten()\n",
    "    x0 = np.array([0.50144272, 1.95478666, 0.19788759, 0.14769148, 0.16059078,\n",
    "                   0.16127341, 0.06404702]).flatten()\n",
    "    \n",
    "    # solve ODE\n",
    "    S_star = odeint(f, x0, t_star)\n",
    "    \n",
    "    noise = 0.1\n",
    "    t_train = t_star[:,None]\n",
    "    S_train = addNoise(S_star, noise)\n",
    "\n",
    "    N0 = 0\n",
    "    N1 = N - 1 \n",
    "    idx_data = np.concatenate([np.array([N0]),\n",
    "                               np.random.choice(np.arange(1, N-1), size=N_data, replace=False),\n",
    "                               np.array([N-1]),\n",
    "                               np.array([N1])])\n",
    "    idx_eqns = np.concatenate([np.array([N0]),\n",
    "                               np.random.choice(np.arange(1, N-1), size=N_eqns-2, replace=False),\n",
    "                               np.array([N-1])])\n",
    "\n",
    "    model = HiddenPathways(t_train[idx_data],\n",
    "                           S_train[idx_data,:],\n",
    "                           t_train[idx_eqns],\n",
    "                           layers)\n",
    "\n",
    "    model.train(num_epochs=20000, batch_size=N_eqns, learning_rate=1e-3)\n",
    "    model.train(num_epochs=40000, batch_size=N_eqns, learning_rate=1e-4)\n",
    "    model.train(num_epochs=20000, batch_size=N_eqns, learning_rate=1e-5)\n",
    "    \n",
    "    S_pred, S_pred_std = model.predict(t_star[:,None])\n",
    "    \n",
    "    ####### Plotting ##################\n",
    "    \n",
    "    fig, ax = newfig(3.0, 0.3)\n",
    "    gs0 = gridspec.GridSpec(1, 2)\n",
    "    gs0.update(top=0.95, bottom=0.1, left=0.1, right=0.95, hspace=0.5, wspace=0.3)\n",
    "    ax = plt.subplot(gs0[0:1, 0:1])\n",
    "    ax.plot(t_star,S_star[:,4],'C1',linewidth=2,label='input data')\n",
    "    ax.scatter(t_star[idx_data],S_star[idx_data,4],marker='o',s=50,label='sampled input')\n",
    "    ax.set_xlabel('$t\\ (min)$', fontsize=18)\n",
    "    ax.set_ylabel('$S_5\\ (mM)$', fontsize=18)\n",
    "    ax.legend(fontsize='large')\n",
    "    \n",
    "    ax = plt.subplot(gs0[0:1, 1:2])\n",
    "    ax.plot(t_star,S_star[:,5],'C1',linewidth=2)\n",
    "    ax.scatter(t_star[idx_data],S_star[idx_data,5],marker='o',s=50)\n",
    "    ax.set_xlabel('$t\\ (min)$', fontsize=18)\n",
    "    ax.set_ylabel('$S_6\\ (mM)$', fontsize=18)\n",
    "\n",
    "    ####################################\n",
    "\n",
    "    fig, ax = newfig(3.0, 0.4)\n",
    "    gs1 = gridspec.GridSpec(1, 3)\n",
    "    gs1.update(top=0.95, bottom=0.15, left=0.1, right=0.95, hspace=0.3, wspace=0.3)\n",
    "    ax = plt.subplot(gs1[0:1, 0:1])\n",
    "    ax.plot(t_star,S_star[:,0],'C1',linewidth=2,label='exact')\n",
    "    ax.plot(t_star,S_pred[:,0],'g-.',linewidth=3,label='learned')\n",
    "    ax.set_xlabel('$t\\ (min)$', fontsize=18)\n",
    "    ax.set_ylabel('$S_1\\ (mM)$', fontsize=18)\n",
    "    ax.legend(fontsize='large')\n",
    "    \n",
    "    ax = plt.subplot(gs1[0:1, 1:2])\n",
    "    ax.plot(t_star,S_star[:,1],'C1',linewidth=2)\n",
    "    ax.plot(t_star,S_pred[:,1],'g-.',linewidth=3)\n",
    "    ax.set_xlabel('$t\\ (min)$', fontsize=18)\n",
    "    ax.set_ylabel('$S_2\\ (mM)$', fontsize=18)\n",
    "\n",
    "    ax = plt.subplot(gs1[0:1, 2:3])\n",
    "    ax.plot(t_star,S_star[:,2],'C1',linewidth=2)\n",
    "    ax.plot(t_star,S_pred[:,2],'g-.',linewidth=3)\n",
    "    ax.set_xlabel('$t\\ (min)$', fontsize=18)\n",
    "    ax.set_ylabel('$S_3\\ (mM)$', fontsize=18)\n",
    "    \n",
    "    fig, ax = newfig(3.5, 0.4)\n",
    "    gs2 = gridspec.GridSpec(1, 3)\n",
    "    gs2.update(top=0.95, bottom=0.15, left=0.1, right=0.95, hspace=0.3, wspace=0.3)\n",
    "    ax = plt.subplot(gs2[0:1, 0:1])\n",
    "    ax.plot(t_star,S_star[:,3],'C1',linewidth=2)\n",
    "    ax.plot(t_star,S_pred[:,3],'g-.',linewidth=3)\n",
    "    ax.set_xlabel('$t\\ (min)$', fontsize=18)\n",
    "    ax.set_ylabel('$S_4\\ (mM)$', fontsize=18)\n",
    "\n",
    "    ax = plt.subplot(gs2[0:1, 1:2])\n",
    "    ax.scatter(t_star[idx_data],S_star[idx_data,4],marker='o',c='C1',s=30)\n",
    "    ax.plot(t_star,S_pred[:,4],'g-.',linewidth=3)\n",
    "    ax.set_xlabel('$t\\ (min)$', fontsize=18)\n",
    "    ax.set_ylabel('$S_5\\ (mM)$', fontsize=18)\n",
    "    \n",
    "    ax = plt.subplot(gs2[0:1, 2:3])\n",
    "    ax.scatter(t_star[idx_data],S_star[idx_data,5],marker='o',c='C1',s=30)\n",
    "    ax.plot(t_star,S_pred[:,5],'g--',linewidth=3)\n",
    "    ax.set_xlabel('$t\\ (min)$', fontsize=18)\n",
    "    ax.set_ylabel('$S_6\\ (mM)$', fontsize=18)\n",
    "\n",
    "    fig, ax = newfig(1, 1.5)\n",
    "    gs3 = gridspec.GridSpec(1, 1)\n",
    "    gs3.update(top=0.95, bottom=0.15, left=0.15, right=0.95, hspace=0.3, wspace=0.3)\n",
    "    ax = plt.subplot(gs3[0:1, 0:1])\n",
    "    ax.plot(t_star,S_star[:,6],'C1',linewidth=2)\n",
    "    ax.plot(t_star,S_pred[:,6],'g-.',linewidth=3)\n",
    "    ax.set_xlabel('$t\\ (min)$', fontsize=18)\n",
    "    ax.set_ylabel('$S_7\\ (mM)$', fontsize=18)\n",
    "\n",
    "    print('J0 = %.6f' % ( model.sess.run(model.J0) ) )\n",
    "    print('k1 = %.6f' % ( model.sess.run(model.k1) ) )\n",
    "    print('k2 = %.6f' % ( model.sess.run(model.k2) ) )\n",
    "    print('k3 = %.6f' % ( model.sess.run(model.k3) ) )\n",
    "    print('k4 = %.6f' % ( model.sess.run(model.k4) ) )\n",
    "    print('k5 = %.6f' % ( model.sess.run(model.k5) ) )\n",
    "    print('k6 = %.6f' % ( model.sess.run(model.k6) ) )\n",
    "    print('k = %.6f' % ( model.sess.run(model.k) ) )\n",
    "    print('kappa = %.6f' % ( model.sess.run(model.kappa) ) )\n",
    "    print('q = %.6f' % ( model.sess.run(model.q) ) )\n",
    "    print('K1 = %.6f' % ( model.sess.run(model.K1) ) )\n",
    "    print('psi = %.6f' % ( model.sess.run(model.psi) ) )\n",
    "    print('N = %.6f' % ( model.sess.run(model.N) ) )\n",
    "    print('A = %.6f' % ( model.sess.run(model.A) ) )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Tiempo de ejecucion: %i minutos.\" % (elapsed_time/60))\n",
    "    # savefig('./figures/Glycolytic', crop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
